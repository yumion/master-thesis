
\chapter{結果}\label{chap_result}
%loss,accのグラフは全て．
%前処理を行った場合は，その画像の一例も載せる
%モデルの画像(appendixの方がいいかもしれないです)
%各sectionで異なる場合は各sectionで載せましょう
%なるべくpdfで保存しましょう

\section{古典的な画像処理手法による識別精度評価}
%楕円検出の塗った画像
%円検出の画像
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{fig/circle_detection}
	\caption{Detection of circle.}
	\label{fig:circle_detection}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{fig/eclips_detection}
	\caption{Detection of ellipse}
	\label{fig:ellipse_detection}
\end{figure}

\fig{circle_detection}が円検出を利用して正常の構造を検出した結果である．緑色の円の部分が円検出された領域を示している．真円にフィッティングしているため綺麗な円構造のみの検出となり，正常でも検出できていない部分が多くなっている．全体的に正常領域で円検出が多く行われ，腫瘍領域では円検出結果が少なくなる結果となったが，腫瘍部分も円検出されている部分が一部ある．

\fig{ellipse_detection}は楕円検出を行った結果である．円検出よりも正常領域の検出が増えたことが分かる．楕円検出がされた領域の多いところほど正常の領域で，楕円検出が少ない領域が腫瘍であるという傾向を捉えることができた．古典的な画像処理の利点は，画像検出の理由が明確に説明できることである．このような円検出や楕円検出であれば，円形度を算出することができるので，深層学習を用いた結果よりも透明性がある．しかしながら，明確な輪郭で無ければ検出できない場合が多く検出精度に限界があることから，これでは腫瘍の見落とし防止に利用することができない．そこで古典的な画像処理の性能を上回る，深層学習を利用するが必要であると分かる．


\section{教師あり学習による識別精度評価}
教師あり学習では，まず2次元画像で，元画像のまま学習した場合と，擬似HE変換した画像を学習する場合，さらに事前学習を行った場合で比較を行った．

\begin{figure}[H]
	\centering
	\begin{minipage}{0.5\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/chapter4/2dcnn_preprocessing}
		\caption{ROC curve for 2DCNN with different preprocessing}
		\label{fig:2dcnnpreprocessing}
	\end{minipage}
	\makeatletter
	\def\@captype{table}
	\makeatother
	\begin{minipage}{0.4\columnwidth}
		\centering
		\caption{AUC of ROC curve for 2DCNN with different preprocessing}
		\label{tab:2DCNNpreprocessing_AUC}
		\begin{tabular}{cc}\toprule
			& AUC \\ \midrule
			HE: False, pretrain: False &  \\ 
			HE: True, pretrain: False &  \\ 
			HE: True, pretrain: True &  \\ \bottomrule
		\end{tabular} 
	\end{minipage}
\end{figure}

\fig{2dcnnpreprocessing}は評価指数のROCである．  
前処理を何も行わない場合が緑線であり，擬似HE変換を行った場合が青線，擬似HE変換を行いかつ事前学習を行った場合が橙線である．
AUCを\tab {2DCNNpreprocessing_AUC}にまとめた．前処理を何も行わない場合が一番AUCが低い値となった．

\fig{2d_preprocess_matrix}に混同行列を示す．図の左側は画像枚数であり，右側はその割合を示している．
\fig{no_preprocess}のように，前処理なしでは腫瘍は87\%の精度で検出できているが，正常は40\%の精度でしか検出できていなかったことが分かる．

腫瘍の見落としとなる偽陰性の割合が13\%と低く，偽陽性が高くなっていることが分かる．このようになった理由は，深層学習の学習方法に理由がある．今回学習に用いたデータで正常は，783枚であるのに対して，腫瘍の学習データの枚数は1566枚である．正常の画像よりも腫瘍の画像を2倍多く学習に利用することで，腫瘍の検出がより重要視されるモデルになったのである．これは，病変の見落としリスクを防止するためには，非常に良い結果となった．

腫瘍の見落とし率が低いことは良いが，正常を腫瘍と判断する割合が高すぎるのでこれを正しく正常画像を正常と判断できるようにしなくてはならない．

ここで前処理として擬似HE変換を行って学習した結果(\fig{he_preprocess})と，擬似HE変換した後に，事前学習も行った時の学習結果(\fig{pretrain_preprocess})から，前処理をすることで腫瘍の見落としリスクを低くしたまま，正常画像を正しく認識することができるようになった．

\begin{figure}[H]
	\centering
	
	\begin{minipage}[b]{\columnwidth}
		\centering
		\includegraphics[clip, width=0.45\linewidth]{fig/chapter4/count_pretrain_False_he_False}
		\includegraphics[clip, width=0.45\linewidth]{fig/chapter4/pretrain_False_he_False}
		\subcaption{pretrain: Flase, he transform: False}
		\label{fig:no_preprocess}
	\end{minipage}

	\begin{minipage}[b]{\columnwidth}
		\centering
		\includegraphics[clip, width=0.45\linewidth]{fig/chapter4/count_pretrain_False_he_True}
		\includegraphics[clip, width=0.45\linewidth]{fig/chapter4/pretrain_False_he_True}
		\subcaption{pretrain: Flase, he transform: True}
		\label{fig:he_preprocess}
	\end{minipage}

	\begin{minipage}[b]{\columnwidth}
		\centering
		\includegraphics[clip, width=0.45\linewidth]{fig/chapter4/count_pretrain_True_he_True}
		\includegraphics[clip, width=0.45\linewidth]{fig/chapter4/pretrain_True_he_True}
		\subcaption{pretrain: True, he transform: True}
		\label{fig:pretrain_preprocess}
	\end{minipage}
	
	\caption{Confusion Matrix for 2DCNN with different preprocessing}
	\label{fig:2d_preprocess_matrix}
	
\end{figure}


\subsection*{モデルの比較}
InceptionV3とXception, InceptionResnetV2を比較して，どのネットワークが認識精度が高いのかを実験した．

\begin{figure}[H]
	\centering
	\begin{minipage}{0.4\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/chapter4/2dcnn_model}
		\caption{Comparing deep learning models}
		\label{fig:2dcnnmodel}
	\end{minipage}
	\makeatletter
	\def\@captype{table}
	\makeatother
	\begin{minipage}{0.4\columnwidth}
		\centering
		\caption{AUC of ROC curve for different models}
		\label{tab:2DCNNcompare_AUC}
		\begin{tabular}{cc}\toprule
			& AUC \\ \midrule
			InceptionV3 &  \\ 
			Xception &  \\ 
			InceptionResNetV2 &  \\ \bottomrule
		\end{tabular} 
	\end{minipage}
\end{figure}

\tab{2DCNNcompare_AUC}に示す通り，先行研究でも利用されているように，今回の研究に用いた検体データでもInceptionV3がもっともAUCが大きくなるディープラーニングのモデルであることが分かった．XceptionやInceptionResNetV2など様々なディープラーニングのモデルが研究され利用されており，一般物体認識では，InceptionResNet，Xception, InceptionV3の順番に精度が高いことが報告されている．しかし今回のように適用するデータが変化すると，どのモデルがもっとも精度が高くなるかを事前に判断することが難しいことが分かった．

\subsection{3次元画像解析}
3DCNNによる3次元画像解析の結果を\fig{depthall}に示す．ここで，深さ方向の取り方は1通りではないため，大きく3つに分けてその差を比較してみた．

\begin{figure}[H]
	\centering
	\begin{minipage}{0.4\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/chapter4/3d/roc/depth_all.pdf}
		\caption{Dependence of depth}
		\label{fig:depthall}
	\end{minipage}
	\makeatletter
	\def\@captype{table}
	\makeatother
	\begin{minipage}{0.4\columnwidth}
		\centering
		\caption{AUC of ROC for }
		\label{tab:2DCNNpreprocessing_AUC}
		\begin{tabular}{cc}\toprule
			depth & AUC \\ \midrule
			10 &  \\ 
			50 &  \\ 
			100 &  \\ \bottomrule
		\end{tabular} 
	\end{minipage}
\end{figure}

\fig{depthall}に示したように，深さ方向に10, 50, 100と増やしていくと腫瘍と正常の認識精度を向上させることができた．これは，正常の場合だと腺菅構造が深さ方向に一定に変化するのに対して，腫瘍の場合は構造が乱れているので変化も不規則であるという性質など，3次元的な構造を捉えながら学習することができたため，認識精度が上がったと言える．

3DCNNは，学習する際の訓練パラメータが多すぎて，学習が収束しない点や，メモリが大きくてGPUに乗らないという問題があり，時系列解析で行われているLSTMやGRUまた，BidirectionalなGRUをモデルの比較として利用した結果を\fig{2dcnn+LSTM_roc}に示す．

\begin{figure}[H]
	\centering
	
	\begin{minipage}[b]{0.33\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/chapter4/3d/roc/depth_10.pdf}
		\subcaption{depth: 10これだけlegendの色が他と異なります．合わせましょう}
		\label{fig:}
	\end{minipage}
	\begin{minipage}[b]{0.33\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/chapter4/3d/roc/depth_50.pdf}
		\subcaption{depth: 50}
		\label{fig:}
	\end{minipage}
	\begin{minipage}[b]{0.33\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/chapter4/3d/roc/depth_100.pdf}
		\subcaption{depth: 100}
		\label{fig:}
	\end{minipage}
	
	\caption{ROC curve with different depth}
	\label{fig:2dcnn+LSTM_roc}
	
\end{figure}

\fig{2dcnn+LSTM_roc}より，LSTMやGRU, Bi-GRUといったモデルによる精度の差は，ほとんどなかった．GRUがもっとも訓練パラメータが少ないので学習時間や推論時間を早くすることができる．そのため今後はGRUを使って学習をすると効率が良いことが分かった．

\fig{gru_matrix}に2DCNNとGRUを組み合わせた手法における混合行列を示す．図の左側は画像枚数であり，右側はその割合を示している．
%何か少し説明解説が欲しいです

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{\columnwidth}
		\centering
		\includegraphics[clip, width=0.45\linewidth]{fig/chapter4/3d/confusion_matrix/count_confusion_matrix_False_10_gru}
		\includegraphics[clip, width=0.45\linewidth]{fig/chapter4/3d/confusion_matrix/normalized_confusion_matrix_False_10_gru}
		\subcaption{depth: 10}
		\label{fig:}
	\end{minipage}

	\begin{minipage}[b]{\columnwidth}
		\centering
		\includegraphics[clip, width=0.45\linewidth]{fig/chapter4/3d/confusion_matrix/count_confusion_matrix_False_50_gru}
		\includegraphics[clip, width=0.45\linewidth]{fig/chapter4/3d/confusion_matrix/normalized_confusion_matrix_False_50_gru}
		\subcaption{depth: 50}
		\label{fig:}
	\end{minipage}

	\begin{minipage}[b]{\columnwidth}
		\centering
		\includegraphics[clip, width=0.45\linewidth]{fig/chapter4/3d/confusion_matrix/count_confusion_matrix_False_100_gru}
		\includegraphics[clip, width=0.45\linewidth]{fig/chapter4/3d/confusion_matrix/normalized_confusion_matrix_False_100_gru}
		\subcaption{depth: 100}
		\label{fig:}
	\end{minipage}
	
	\caption{Confusion Matrix for 2DCNN and GRU}
	\label{fig:gru_matrix}
	
\end{figure}



\section{教師なし学習による識別精度評価}

\subsection{AutoEncoder}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{fig/chapter4/unet_ae}
	\caption{Generated images by AutoEncoder}
	\label{fig:unetae}
\end{figure}


\subsection{GAN}

\begin{figure}[H]
	\centering
	
	\begin{minipage}{0.24\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/generative_adversarial_nets/0000_0000}
		\subcaption{epochs = 0}
		\label{fig:}
	\end{minipage}
	\begin{minipage}{0.24\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/generative_adversarial_nets/0079_0000}
		\subcaption{epochs = 79}
		\label{fig:}
	\end{minipage}
	\begin{minipage}{0.24\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/generative_adversarial_nets/0641_0000}
		\subcaption{epochs = 641}
		\label{fig:}
	\end{minipage}
	\begin{minipage}{0.24\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/generative_adversarial_nets/0969_0000}
		\subcaption{epochs = 969}
		\label{fig:}
	\end{minipage}
	\begin{minipage}{0.24\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/generative_adversarial_nets/1213_0000}
		\subcaption{epochs = 1213}
		\label{fig:}
	\end{minipage}
	\begin{minipage}{0.24\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/generative_adversarial_nets/1619_0000}
		\subcaption{epochs = 1619}
		\label{fig:}
	\end{minipage}
	\begin{minipage}{0.24\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/generative_adversarial_nets/2004_0000}
		\subcaption{epochs = 2004}
		\label{fig:}
	\end{minipage}
	\begin{minipage}{0.24\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/generative_adversarial_nets/3208_0000}
		\subcaption{epochs = 3208}
		\label{fig:}
	\end{minipage}
	
	\caption{Transition of generated images by GAN}
	\label{fig:GANimage}
	
\end{figure}

\subsection{VAE}
擬似HE染色した画像と元のカラー画像のそれぞれに対してVAEを行い，潜在変数を2次元空間にプロットした結果を\fig {VAEplot}に示す．

\begin{figure}[H]
	\centering
	
	\begin{minipage}[b]{0.45\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/variational_auto_encoder/vae_colon_epoch_100_c13_he}
		\subcaption{HE like color at sample A}
		\label{fig:}
	\end{minipage}
	\begin{minipage}[b]{0.45\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/variational_auto_encoder/vae_colon_epoch_299_c13_rgb}
		\subcaption{original color at sample A}
		\label{fig:}
	\end{minipage}
	\begin{minipage}[b]{0.45\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/variational_auto_encoder/vae_colon_epoch_100_he_mix}
		\subcaption{HE like color at sample A and B}
		\label{fig:}
	\end{minipage}
	\begin{minipage}[b]{0.45\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/variational_auto_encoder/vae_colon_epoch_100_rgb_mix}
		\subcaption{original color at sample A and B}
		\label{fig:}
	\end{minipage}
	
	\caption{Latent space of 2D. Color ratio 1 is cancer, 0 is normal.}
	\label{fig:VAEplot}
		
\end{figure}

部分的には，正常と腫瘍で分布が異なるようになったが，明確な境界線を引くことが難しい．したがって，教師あり学習で境界を明確にし，教師なし学習でデータの構造を抽出することができれば，もっとも精度の高い認識モデルが作れる．

\begin{figure}[H]
	\centering
	
	\begin{minipage}[b]{0.45\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/chapter4/vae_latent_vectors}
		\subcaption{Encoded latent vectors. The blue points are normal, the red points are the cancer.}
		\label{fig:vaelatentvectors}
	\end{minipage}
	\hspace{5truemm}
	\begin{minipage}[b]{0.45\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/chapter4/vae_result_picture}
		\subcaption{The image on the lower left is a normal image and the image on the right is the image of the cancer.}
		\label{fig:vaeresultpicture}
	\end{minipage}

	\caption{Results of VAE decoder}
	\label{fig:resultsVAE}
	
\end{figure}


VAEで生成される画像が\fig{vaeresultpicture}である．これはプロットの分布を元に画像を再生成した画像であり，左下が正常の傾向の画像で，右側が腫瘍の傾向の画像に生成されていること分かる．画像は正常から腫瘍へと徐々に変化していき，構造的な特徴を学習できている．


\section{半教師あり学習による識別精度評価}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{fig/chapter4/accuracy_summary}
	\caption{Accuracy of semi-supervised learning model based on VAE}
	\label{fig:semiVAE_acc}
\end{figure}

\fig{semiVAE_acc}に半教師あり学習による識別精度の推移を示した．
教師あり学習では画像枚数が足りない場合においても，半教師あり学習は大量の教師ラベルのない画像に対しても特徴を学習するため，教師ラベルの予測精度を上げることができた．


\section{学習結果の可視化}
深層学習を用いて，腫瘍の確率が高い領域を認識するアルゴリズムを，病理医が見るために画像上に腫瘍の部分が見て分かるように可視化を行った．

まずはスライドウィンドウで画像を取得していき，各ウィンドウの画像に対して病変の確率を出して，その確率をヒートマップにした(\fig{cancer_detect_sampleB})．青い領域は正常であり，黄色や赤色になっている部分が腫瘍と深層学習により認識されている部分である．この領域を医師が見ることでスクリーニングになる．\fig{cancer_detect_sampleB}は人が見落としやすい場所に腫瘍があってもAIによって見落としを防止し，診断支援に繋がる可視化の1つの結果である．

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{fig/chapter4/visualization/sample_B}
	\caption{Visualize cancer detection. The blue area is normal and the yellow to red area is the cancer.}
	\label{fig:cancer_detect_sampleB}
\end{figure}

さらにGrad-CAMを利用することで，より詳細に深層学習の判断が可視化されるようになった．

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{fig/chapter4/large-grad-cam-step100-rm-black}
	\caption{Visualization with Grad-CAM}
	\label{fig:large-grad}
\end{figure}

\fig{large-grad}で赤色〜青色のヒートマップで塗られている部分が腫瘍であると認識された部分である．また何も塗られていない部分が正常と認識された部分である．このように可視化を行うことで腫瘍の可能性がある領域を一目で確認することができるようになった．