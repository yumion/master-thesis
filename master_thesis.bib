@article{FCN,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
eprint = {1411.4038},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Long, Shelhamer, Darrell - 2014 - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
month = {nov},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {http://arxiv.org/abs/1411.4038},
year = {2014}
}
@inproceedings{MaskRCNN,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.322},
eprint = {1703.06870},
isbn = {9781538610329},
issn = {15505499},
month = {dec},
pages = {2980--2988},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Mask R-CNN}},
volume = {2017-October},
year = {2017}
}
@article{Ulku2019,
abstract = {Semantic segmentation is the pixel-wise labelling of an image. Since the problem is defined at the pixel level, determining image class labels only is not acceptable, but localising them at the original image pixel resolution is necessary. Boosted by the extraordinary ability of convolutional neural networks (CNN) in creating semantic, high level and hierarchical image features; excessive numbers of deep learning-based 2D semantic segmentation approaches have been proposed within the last decade. In this survey, we mainly focus on the recent scientific developments in semantic segmentation, specifically on deep learning-based methods using 2D images. We started with an analysis of the public image sets and leaderboards for 2D semantic segmantation, with an overview of the techniques employed in performance evaluation. In examining the evolution of the field, we chronologically categorised the approaches into three main periods, namely pre-and early deep learning era, the fully convolutional era, and the post-FCN era. We technically analysed the solutions put forward in terms of solving the fundamental problems of the field, such as fine-grained localisation and scale invariance. Before drawing our conclusions, we present a table of methods from all mentioned eras, with a brief summary of each approach that explains their contribution to the field. We conclude the survey by discussing the current challenges of the field and to what extent they have been solved.},
archivePrefix = {arXiv},
arxivId = {1912.10230},
author = {Ulku, Irem and Akagunduz, Erdem},
eprint = {1912.10230},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Ulku, Akagunduz - 2019 - A Survey on Deep Learning-based Architectures for Semantic Segmentation on 2D images.pdf:pdf},
month = {dec},
title = {{A Survey on Deep Learning-based Architectures for Semantic Segmentation on 2D images}},
url = {http://arxiv.org/abs/1912.10230},
year = {2019}
}
@article{OpenAI2019,
abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/},
archivePrefix = {arXiv},
arxivId = {1910.07113},
author = {OpenAI and Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
eprint = {1910.07113},
month = {oct},
title = {{Solving Rubik's Cube with a Robot Hand}},
url = {http://arxiv.org/abs/1910.07113},
year = {2019}
}
@article{James2017,
abstract = {End-to-end control for robot manipulation and grasping is emerging as an attractive alternative to traditional pipelined approaches. However, end-to-end methods tend to either be slow to train, exhibit little or no generalisability, or lack the ability to accomplish long-horizon or multi-stage tasks. In this paper, we show how two simple techniques can lead to end-to-end (image to velocity) execution of a multi-stage task, which is analogous to a simple tidying routine, without having seen a single real image. This involves locating, reaching for, and grasping a cube, then locating a basket and dropping the cube inside. To achieve this, robot trajectories are computed in a simulator, to collect a series of control velocities which accomplish the task. Then, a CNN is trained to map observed images to velocities, using domain randomisation to enable generalisation to real world images. Results show that we are able to successfully accomplish the task in the real world with the ability to generalise to novel environments, including those with dynamic lighting conditions, distractor objects, and moving objects, including the basket itself. We believe our approach to be simple, highly scalable, and capable of learning long-horizon tasks that have until now not been shown with the state-of-the-art in end-to-end robot control.},
archivePrefix = {arXiv},
arxivId = {1707.02267},
author = {James, Stephen and Davison, Andrew J. and Johns, Edward},
eprint = {1707.02267},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/James, Davison, Johns - 2017 - Transferring End-to-End Visuomotor Control from Simulation to Real World for a Multi-Stage Task.pdf:pdf},
month = {jul},
title = {{Transferring End-to-End Visuomotor Control from Simulation to Real World for a Multi-Stage Task}},
url = {http://arxiv.org/abs/1707.02267},
year = {2017}
}
@article{Rodola,
author = {Rodol, Emanuele and Harada, Tatsuya and Kanezaki, Asako and A, Emanuele Rodol},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Rodol et al. - Unknown - グラフマッチング学習を用いたrgb-D画像からの物体検出.pdf:pdf},
keywords = {3d shape,gradient descent method,graph matching,keypoint matching,optimization},
number = {2},
title = {{グラフマッチング学習を用いたrgb-D画像からの物体検出}},
volume = {1}
}
@article{Xue2019a,
abstract = {Cervical intraepithelial neoplasia (CIN) grade of histopathology images is a crucial indicator in cervical biopsy results. Accurate CIN grading of epithelium regions helps pathologists with precancerous lesion diagnosis and treatment planning. Although an automated CIN grading system has been desired, supervised training of such a system would require a large amount of expert annotations, which are expensive and time-consuming to collect. In this paper, we investigate the CIN grade classification problem on segmented epithelium patches. We propose to use conditional Generative Adversarial Networks (cGANs) to expand the limited training dataset, by synthesizing realistic cervical histopathology images. While the synthetic images are visually appealing, they are not guaranteed to contain meaningful features for data augmentation. To tackle this issue, we propose a synthetic-image filtering mechanism based on the divergence in feature space between generated images and class centroids in order to control the feature quality of selected synthetic images for data augmentation. Our models are evaluated on a cervical histopathology image dataset with a limited number of patch-level CIN grade annotations. Extensive experimental results show a significant improvement of classification accuracy from 66.3{\%} to 71.7{\%} using the same ResNet18 baseline classifier after leveraging our cGAN generated images with feature-based filtering, which demonstrates the effectiveness of our models.},
archivePrefix = {arXiv},
arxivId = {1907.10655},
author = {Xue, Yuan and Zhou, Qianying and Ye, Jiarong and Long, L. Rodney and Antani, Sameer and Cornwell, Carl and Xue, Zhiyun and Huang, Xiaolei},
eprint = {1907.10655},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Synthetic Augmentation and Feature-based Filtering for Improved Cervical Histopathology Image Classification{\_}Xue et al.pdf:pdf},
pages = {1--9},
title = {{Synthetic Augmentation and Feature-based Filtering for Improved Cervical Histopathology Image Classification}},
url = {http://arxiv.org/abs/1907.10655},
year = {2019}
}
@article{Gaier2019b,
abstract = {Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/},
archivePrefix = {arXiv},
arxivId = {1906.04358},
author = {Gaier, Adam and Ha, David},
eprint = {1906.04358},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Gaier, Ha - 2019 - Weight Agnostic Neural Networks.pdf:pdf},
pages = {1--16},
title = {{Weight Agnostic Neural Networks}},
url = {http://arxiv.org/abs/1906.04358},
year = {2019}
}
@article{Kim2019a,
abstract = {We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based methods which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters.},
archivePrefix = {arXiv},
arxivId = {1907.10830},
author = {Kim, Junho and Kim, Minjae and Kang, Hyeonwoo and Lee, Kwanghee},
eprint = {1907.10830},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - 2019 - U-GAT-IT Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image.pdf:pdf},
title = {{U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation}},
url = {http://arxiv.org/abs/1907.10830},
year = {2019}
}
@article{Chiu2019a,
abstract = {Predicting the future is an important aspect for decision-making in robotics or autonomous driving systems, which heavily rely upon visual scene understanding. While prior work attempts to predict future video pixels, anticipate activities or forecast future scene semantic segments from segmentation of the preceding frames, methods that predict future semantic segmentation solely from the previous frame RGB data in a single end-to-end trainable model do not exist. In this paper, we propose a temporal encoder-decoder network architecture that encodes RGB frames from the past and decodes the future semantic segmentation. The network is coupled with a new knowledge distillation training framework specifically for the forecasting task. Our method, only seeing preceding video frames, implicitly models the scene segments while simultaneously accounting for the object dynamics to infer the future scene semantic segments. Our results on Cityscapes outperform the baseline and current state-of-the-art methods. Code is available at https://github.com/eddyhkchiu/segmenting{\_}the{\_}future/.},
archivePrefix = {arXiv},
arxivId = {1904.10666},
author = {Chiu, Hsu-kuang and Adeli, Ehsan and Niebles, Juan Carlos},
eprint = {1904.10666},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Segmenting the Future{\_}Chiu, Adeli, Niebles.pdf:pdf},
title = {{Segmenting the Future}},
url = {http://arxiv.org/abs/1904.10666},
year = {2019}
}
@article{Babin2019a,
abstract = {The stable and repeatable grasping of objects lying on a flat hard surface is addressed in this paper. A physical model of an object lying on a flat surface and its interaction with the environment and with a gripper is proposed. The important parameters governing the interaction are obtained. From this model, a grasping procedure is established and a robotic gripper is modified in order to grant the ability to pick up large thin objects lying on smooth hard surfaces. The procedure is implemented to demonstrate its repeatability on a chosen set of objects. It is shown that by sensing the force applied on the object and by taking advantage of the nature of the contact provided by the passive joint of the modified finger, a wide range of previously not directly graspable objects are made graspable via the application of a general approach. The experimental results reported clearly show the benefits of the simple force sensing implemented in the gripper as well as of the use of passive joints when interacting with very stiff environments. The proposed approach, while simple, yields a repeatable solution to a complex manipulation problem.},
author = {Babin, Vincent and St-Onge, David and Gosselin, Cl{\'{e}}ment},
doi = {10.1016/j.rcim.2018.06.002},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Stable and repeatable grasping of flat objects on hard surfaces using passive and epicyclic mechanisms{\_}Babin, St-Onge, Gosselin.pdf:pdf},
issn = {07365845},
journal = {Robotics and Computer-Integrated Manufacturing},
keywords = {Constrained grasping,Epicyclic mechanism,Gripper,Passive finger,Repeated grasping,Robot grasping},
number = {June 2018},
pages = {1--10},
publisher = {Elsevier Ltd},
title = {{Stable and repeatable grasping of flat objects on hard surfaces using passive and epicyclic mechanisms}},
url = {https://doi.org/10.1016/j.rcim.2018.06.002},
volume = {55},
year = {2019}
}
@article{Touvron2019a,
abstract = {Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the typical size of the objects seen by the classifier at train and test time. We experimentally validate that, for a target test resolution, using a lower train resolution offers better classification at test time. We then propose a simple yet effective and efficient strategy to optimize the classifier performance when the train and test resolutions differ. It involves only a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images. For instance, we obtain 77.1{\%} top-1 accuracy on ImageNet with a ResNet-50 trained on 128x128 images, and 79.8{\%} with one trained on 224x224 image. In addition, if we use extra training data we get 82.5{\%} with the ResNet-50 train with 224x224 images. Conversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4{\%} (top-5: 98.0{\%}) (single-crop). To the best of our knowledge this is the highest ImageNet single-crop, top-1 and top-5 accuracy to date.},
archivePrefix = {arXiv},
arxivId = {1906.06423},
author = {Touvron, Hugo and Vedaldi, Andrea and Douze, Matthijs and J{\'{e}}gou, Herv{\'{e}}},
eprint = {1906.06423},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Fixing the train-test resolution discrepancy{\_}Touvron et al.pdf:pdf},
title = {{Fixing the train-test resolution discrepancy}},
url = {http://arxiv.org/abs/1906.06423},
year = {2019}
}
@article{Vecerik2018,
abstract = {Insertion is a challenging haptic and visual control problem with significant practical value for manufacturing. Existing approaches in the model-based robotics community can be highly effective when task geometry is known, but are complex and cumbersome to implement, and must be tailored to each individual problem by a qualified engineer. Within the learning community there is a long history of insertion research, but existing approaches are typically either too sample-inefficient to run on real robots, or assume access to high-level object features, e.g. socket pose. In this paper we show that relatively minor modifications to an off-the-shelf Deep-RL algorithm (DDPG), combined with a small number of human demonstrations, allows the robot to quickly learn to solve these tasks efficiently and robustly. Our approach requires no modeling or simulation, no parameterized search or alignment behaviors, no vision system aside from raw images, and no reward shaping. We evaluate our approach on a narrow-clearance peg-insertion task and a deformable clip-insertion task, both of which include variability in the socket position. Our results show that these tasks can be solved reliably on the real robot in less than 10 minutes of interaction time, and that the resulting policies are robust to variance in the socket position and orientation.},
archivePrefix = {arXiv},
arxivId = {1810.01531},
author = {Vecerik, Mel and Sushkov, Oleg and Barker, David and Roth{\"{o}}rl, Thomas and Hester, Todd and Scholz, Jon},
eprint = {1810.01531},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning{\_}Vecerik et al.pdf:pdf},
title = {{A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1810.01531},
year = {2018}
}
@article{Cubuk2018a,
abstract = {Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5{\%} which is 0.4{\%} better than the previous record of 83.1{\%}. On CIFAR-10, we achieve an error rate of 1.5{\%}, which is 0.6{\%} better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.},
archivePrefix = {arXiv},
arxivId = {1805.09501},
author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
eprint = {1805.09501},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}AutoAugment Learning Augmentation Policies from Data{\_}Cubuk et al.pdf:pdf},
number = {Section 3},
title = {{AutoAugment: Learning Augmentation Policies from Data}},
url = {http://arxiv.org/abs/1805.09501},
year = {2018}
}
@article{Kanezaki2018a,
abstract = {We propose a Convolutional Neural Network (CNN)-based model "RotationNet," which takes multi-view images of an object as input and jointly estimates its pose and object category. Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset. RotationNet is designed to use only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available. Moreover, our pose alignment strategy enables one to obtain view-specific feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation. Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classification on 10- and 40-class ModelNet datasets. We also show that RotationNet, even trained without known poses, achieves the state-of-the-art performance on an object pose estimation dataset. The code is available on https://github.com/kanezaki/rotationnet},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.06208v4},
author = {Kanezaki, Asako and Matsushita, Yasuyuki and Nishida, Yoshifumi},
doi = {10.1109/CVPR.2018.00526},
eprint = {arXiv:1603.06208v4},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}RotationNet Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints{\_}Kanezaki, Matsushita, Nis.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {5010--5019},
title = {{RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints}},
year = {2018}
}
@article{Siam2018a,
abstract = {Video object segmentation is an essential task in robot manipulation to facilitate grasping and learning affordances. Incremental learning is important for robotics in unstructured environments, since the total number of objects and their variations can be intractable. Inspired by the children learning process, human robot interaction (HRI) can be utilized to teach robots about the world guided by humans similar to how children learn from a parent or a teacher. A human teacher can show potential objects of interest to the robot, which is able to self adapt to the teaching signal without providing manual segmentation labels. We propose a novel teacher-student learning paradigm to teach robots about their surrounding environment. A two-stream motion and appearance "teacher" network provides pseudo-labels to adapt an appearance "student" network. The student network is able to segment the newly learned objects in other scenes, whether they are static or in motion. We also introduce a carefully designed dataset that serves the proposed HRI setup, denoted as (I)nteractive (V)ideo (O)bject (S)egmentation. Our IVOS dataset contains teaching videos of different objects, and manipulation tasks. Unlike previous datasets, IVOS provides manipulation tasks sequences with segmentation annotation along with the waypoints for the robot trajectories. It also provides segmentation annotation for the different transformations such as translation, scale, planar rotation, and out-of-plane rotation. Our proposed adaptation method outperforms the state-of-the-art on DAVIS and FBMS with 6.8{\%} and 1.2{\%} in F-measure respectively. It improves over the baseline on IVOS dataset with 46.1{\%} and 25.9{\%} in mIoU.},
archivePrefix = {arXiv},
arxivId = {1810.07733},
author = {Siam, Mennatullah and Jiang, Chen and Lu, Steven and Petrich, Laura and Gamal, Mahmoud and Elhoseiny, Mohamed and Jagersand, Martin},
eprint = {1810.07733},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Video Object Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting{\_}Siam et al.pdf:pdf},
number = {I},
title = {{Video Object Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting}},
url = {http://arxiv.org/abs/1810.07733},
year = {2018}
}
@article{Rajeswaran2018a,
abstract = {Dexterous multi-fingered hands are extremely versatile and provide a generic way to perform a multitude of tasks in human-centric environments. However, effectively controlling them remains challenging due to their high dimensionality and large number of potential contacts. Deep reinforcement learning (DRL) provides a model-agnostic approach to control complex dynamical systems, but has not been shown to scale to high-dimensional dexterous manipulation. Furthermore, deployment of DRL on physical systems remains challenging due to sample inefficiency. Consequently, the success of DRL in robotics has thus far been limited to simpler manipulators and tasks. In this work, we show that model-free DRL can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand, and solve them from scratch in simulated experiments. Furthermore, with the use of a small number of human demonstrations, the sample complexity can be significantly reduced, which enables learning with sample sizes equivalent to a few hours of robot experience. The use of demonstrations result in policies that exhibit very natural movements and, surprisingly, are also substantially more robust.},
archivePrefix = {arXiv},
arxivId = {arXiv:1709.10087v2},
author = {Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
doi = {10.15607/rss.2018.xiv.049},
eprint = {arXiv:1709.10087v2},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations{\_}Rajeswaran et al.pdf:pdf},
title = {{Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations}},
year = {2018}
}
@article{Lahoud2017a,
abstract = {In this paper, we present a technique that places 3D bounding boxes around objects in an RGB-D scene. Our approach makes best use of the 2D information to quickly reduce the search space in 3D, benefiting from state-of-the-art 2D object detection techniques. We then use the 3D information to orient, place, and score bounding boxes around objects. We independently estimate the orienta-tion for every object, using previous techniques that utilize normal information. Object locations and sizes in 3D are learned using a multilayer perceptron (MLP). In the final step, we refine our detections based on object class relations within a scene. When compared to state-of-the-art detection methods that operate almost entirely in the sparse 3D do-main, extensive experiments on the well-known SUN RGB-D dataset [29] show that our proposed method is much faster (4.1s per image) in detecting 3D objects in RGB-D images and performs better (3 mAP higher) than the state-of-the-art method that is 4.7 times slower and comparably to the method that is two orders of magnitude slower. This work hints at the idea that 2D-driven object detection in 3D should be further explored, especially in cases where the 3D input is sparse.},
author = {Lahoud, Jean and Ghanem, Bernard},
doi = {10.1109/ICCV.2017.495},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}2D-Driven 3D Object Detection in RGB-D Images{\_}Lahoud, Ghanem.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {i},
pages = {4632--4640},
title = {{2D-Driven 3D Object Detection in RGB-D Images}},
volume = {2017-Octob},
year = {2017}
}
@article{Laia,
author = {Lai, Kevin and Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2011{\_}Sparse Distance Learning for Object Recognition Combining RGB and Depth Information{\_}Lai et al.pdf:pdf},
number = {1},
pages = {1--25},
title = {{Hybrid Si – SiC High Power Modules}}
}
@article{Song2017,
abstract = {Scene recognition with RGB images has been extensively studied and has reached very remarkable recognition levels, thanks to convolutional neural networks (CNN) and large scene datasets. In contrast, current RGB-D scene data is much more limited, so often leverages RGB large datasets, by transferring pretrained RGB CNN models and fine-tuning with the target RGB-D dataset. However, we show that this approach has the limitation of hardly reaching bottom layers, which is key to learn modality-specific features. In contrast, we focus on the bottom layers, and propose an alternative strategy to learn depth features combining local weakly supervised training from patches followed by global fine tuning with images. This strategy is capable of learning very discriminative depth-specific features with limited depth images, without resorting to Places-CNN. In addition we propose a modified CNN architecture to further match the complexity of the model and the amount of data available. For RGB-D scene recognition, depth and RGB features are combined by projecting them in a common space and further leaning a multilayer classifier, which is jointly optimized in an end-to-end network. Our framework achieves state-of-the-art accuracy on NYU2 and SUN RGB-D in both depth only and combined RGB-D data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1801.06797v1},
author = {Song, Xinhang and Herranz, Luis and Jiang, Shuqiang},
eprint = {arXiv:1801.06797v1},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Depth CNNs for RGB-D scene recognition Learning from scratch better than transferring from RGB-CNNs{\_}Song, Herranz, Jiang.pdf:pdf},
journal = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
pages = {4271--4277},
title = {{Depth CNNs for RGB-D scene recognition: Learning from scratch better than transferring from RGB-CNNs}},
year = {2017}
}
@article{Wang2015c,
abstract = {Most of the feature-learning methods for RGB-D objec- t recognition either learn features from color and depth modalities separately, or simply treat RGB-D as undiffer- entiated four-channel data, which cannot adequately ex- ploit the relationship between different modalities. Moti- vated by the intuition that different modalities should con- tain not only some modal-specific patterns but also some shared common patterns, we propose a multi-modal feature learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, and then connect them with our carefully designed multi- modal layers, which fuse color and depth information by enforcing a common part to be shared by features of dif- ferent modalities. In this way, we obtain features reflect- ing shared properties as well as modal-specific properties in different modalities. The information of the multi-modal learning frameworks is back-propagated to the early CNN layers. Experimental results show that our proposed multi- modal feature learning method outperforms state-of-the-art approaches on two widely used RGB-D object benchmark datasets. 1.},
author = {Wang, Anran and Cai, Jianfei and Lu, Jiwen and Cham, Tat Jen},
doi = {10.1109/ICCV.2015.134},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}MMSS Multi-modal sharable and specific feature learning for rgb-d object recognition{\_}Wang et al.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1125--1133},
publisher = {IEEE},
title = {{MMSS: Multi-modal sharable and specific feature learning for rgb-d object recognition}},
volume = {2015 Inter},
year = {2015}
}
@article{Gaier2019a,
abstract = {Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/},
archivePrefix = {arXiv},
arxivId = {1906.04358},
author = {Gaier, Adam and Ha, David},
eprint = {1906.04358},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Gaier, Ha - 2019 - Weight Agnostic Neural Networks.pdf:pdf},
month = {jun},
title = {{Weight Agnostic Neural Networks}},
url = {http://arxiv.org/abs/1906.04358},
year = {2019}
}
@article{Cubuk2018,
abstract = {Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5{\%} which is 0.4{\%} better than the previous record of 83.1{\%}. On CIFAR-10, we achieve an error rate of 1.5{\%}, which is 0.6{\%} better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.},
archivePrefix = {arXiv},
arxivId = {1805.09501},
author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
eprint = {1805.09501},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}AutoAugment Learning Augmentation Policies from Data{\_}Cubuk et al.pdf:pdf},
month = {may},
title = {{AutoAugment: Learning Augmentation Policies from Data}},
url = {http://arxiv.org/abs/1805.09501},
year = {2018}
}
@techreport{Kanezaki2018,
abstract = {We propose a Convolutional Neural Network (CNN)-based model "RotationNet," which takes multi-view images of an object as input and jointly estimates its pose and object category. Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset. RotationNet is designed to use only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available. Moreover, our pose alignment strategy enables one to obtain view-specific feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation. Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classification on 10- and 40-class ModelNet datasets. We also show that RotationNet, even trained without known poses, achieves the state-of-the-art performance on an object pose estimation dataset. The code is available on https://github.com/kanezaki/rotationnet},
archivePrefix = {arXiv},
arxivId = {1603.06208v4},
author = {Kanezaki, Asako and Matsushita, Yasuyuki and Nishida, Yoshifumi},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00526},
eprint = {1603.06208v4},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}RotationNet Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints{\_}Kanezaki, Matsushita, Nis.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
pages = {5010--5019},
title = {{RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints}},
url = {https://github.com/kanezaki/rotationnet},
year = {2018}
}
@techreport{Report2016,
abstract = {A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.},
archivePrefix = {arXiv},
arxivId = {1505.00880},
author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.114},
eprint = {1505.00880},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}Multi-view convolutional neural networks for 3D shape recognition{\_}Su et al.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
number = {02},
pages = {945--953},
pmid = {7410471},
title = {{Multi-view convolutional neural networks for 3D shape recognition}},
url = {http://vis-www.cs.umass.edu/mvcnn.},
volume = {2015 Inter},
year = {2015}
}
@article{Song2017a,
abstract = {Scene recognition with RGB images has been extensively studied and has reached very remarkable recognition levels, thanks to convolutional neural networks (CNN) and large scene datasets. In contrast, current RGB-D scene data is much more limited, so often leverages RGB large datasets, by transferring pretrained RGB CNN models and fine-tuning with the target RGB-D dataset. However, we show that this approach has the limitation of hardly reaching bottom layers, which is key to learn modality-specific features. In contrast, we focus on the bottom layers, and propose an alternative strategy to learn depth features combining local weakly supervised training from patches followed by global fine tuning with images. This strategy is capable of learning very discriminative depth-specific features with limited depth images, without resorting to Places-CNN. In addition we propose a modified CNN architecture to further match the complexity of the model and the amount of data available. For RGB-D scene recognition, depth and RGB features are combined by projecting them in a common space and further leaning a multilayer classifier, which is jointly optimized in an end-to-end network. Our framework achieves state-of-the-art accuracy on NYU2 and SUN RGB-D in both depth only and combined RGB-D data.},
archivePrefix = {arXiv},
arxivId = {1801.06797},
author = {Song, Xinhang and Herranz, Luis and Jiang, Shuqiang},
eprint = {1801.06797},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Depth CNNs for RGB-D scene recognition Learning from scratch better than transferring from RGB-CNNs{\_}Song, Herranz, Jiang.pdf:pdf},
journal = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
month = {jan},
pages = {4271--4277},
title = {{Depth CNNs for RGB-D scene recognition: Learning from scratch better than transferring from RGB-CNNs}},
url = {http://arxiv.org/abs/1801.06797},
year = {2017}
}
@article{Rodol,
author = {Rodol, Emanuele and Harada, Tatsuya and Kanezaki, Asako and A, Emanuele Rodol},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Rodol et al. - Unknown - グラフマッチング学習を用いたrgb-D画像からの物体検出.pdf:pdf},
keywords = {3d shape,gradient descent method,graph matching,keypoint matching,optimization},
number = {2},
title = {{グラフマッチング学習を用いたrgb-D画像からの物体検出}},
volume = {1}
}
@article{Wang2015b,
abstract = {Most of the feature-learning methods for RGB-D objec- t recognition either learn features from color and depth modalities separately, or simply treat RGB-D as undiffer- entiated four-channel data, which cannot adequately ex- ploit the relationship between different modalities. Moti- vated by the intuition that different modalities should con- tain not only some modal-specific patterns but also some shared common patterns, we propose a multi-modal feature learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, and then connect them with our carefully designed multi- modal layers, which fuse color and depth information by enforcing a common part to be shared by features of dif- ferent modalities. In this way, we obtain features reflect- ing shared properties as well as modal-specific properties in different modalities. The information of the multi-modal learning frameworks is back-propagated to the early CNN layers. Experimental results show that our proposed multi- modal feature learning method outperforms state-of-the-art approaches on two widely used RGB-D object benchmark datasets. 1.},
author = {Wang, Anran and Cai, Jianfei and Lu, Jiwen and Cham, Tat Jen},
doi = {10.1109/ICCV.2015.134},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}MMSS Multi-modal sharable and specific feature learning for rgb-d object recognition{\_}Wang et al.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1125--1133},
publisher = {IEEE},
title = {{MMSS: Multi-modal sharable and specific feature learning for rgb-d object recognition}},
volume = {2015 Inter},
year = {2015}
}
@article{Lai,
abstract = {In this work we address joint object category and instance recognition in the context of RGB-D (depth) cameras. Motivated by local distance learning, where a novel view of an object is compared to individual views of previously seen objects, we define a view-to-object distance where a novel view is compared simultaneously to all views of a previous object. This novel distance is based on a weighted combination of feature differences between views. We show, through jointly learning per-view weights, that this measure leads to superior classification performance on object category and instance recognition. More importantly, the proposed distance allows us to find a sparse solution via Group-Lasso regularization, where a small subset of representative views of an object is identified and used, with the rest discarded. This significantly reduces computational cost without compromising recognition accuracy. We evaluate the proposed technique, Instance Distance Learning (IDL), on the RGB-D Object Dataset, which consists of 300 object instances in 51 everyday categories and about 250,000 views of objects with both RGB color and depth. We empirically compare IDL to several alternative state-of-the-art approaches and also validate the use of visual and shape cues and their combination.},
author = {Lai, Kevin and Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2011{\_}Sparse Distance Learning for Object Recognition Combining RGB and Depth Information{\_}Lai et al.pdf:pdf},
journal = {IEEE International Conference on Robotucs and Automation},
number = {1},
pages = {1--25},
title = {{Sparse Distance Learning for Object Recognition Combining RGB and Depth Information}},
url = {https://rse-lab.cs.washington.edu/postscripts/sparse-distance-icra-11.pdf},
year = {2011}
}
@techreport{Lahoud2017,
abstract = {In this paper, we present a technique that places 3D bounding boxes around objects in an RGB-D scene. Our approach makes best use of the 2D information to quickly reduce the search space in 3D, benefiting from state-of-the-art 2D object detection techniques. We then use the 3D information to orient, place, and score bounding boxes around objects. We independently estimate the orienta-tion for every object, using previous techniques that utilize normal information. Object locations and sizes in 3D are learned using a multilayer perceptron (MLP). In the final step, we refine our detections based on object class relations within a scene. When compared to state-of-the-art detection methods that operate almost entirely in the sparse 3D do-main, extensive experiments on the well-known SUN RGB-D dataset [29] show that our proposed method is much faster (4.1s per image) in detecting 3D objects in RGB-D images and performs better (3 mAP higher) than the state-of-the-art method that is 4.7 times slower and comparably to the method that is two orders of magnitude slower. This work hints at the idea that 2D-driven object detection in 3D should be further explored, especially in cases where the 3D input is sparse.},
author = {Lahoud, Jean and Ghanem, Bernard},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.495},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}2D-Driven 3D Object Detection in RGB-D Images{\_}Lahoud, Ghanem.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
pages = {4632--4640},
title = {{2D-Driven 3D Object Detection in RGB-D Images}},
url = {https://ivul.kaust.edu.sa/Documents/Publications/2017/2D-Driven 3D Object Detection in RGB-D Images.pdf},
volume = {2017-Octob},
year = {2017}
}
@article{Rajeswaran2018,
abstract = {Dexterous multi-fingered hands are extremely versatile and provide a generic way to perform a multitude of tasks in human-centric environments. However, effectively controlling them remains challenging due to their high dimensionality and large number of potential contacts. Deep reinforcement learning (DRL) provides a model-agnostic approach to control complex dynamical systems, but has not been shown to scale to high-dimensional dexterous manipulation. Furthermore, deployment of DRL on physical systems remains challenging due to sample inefficiency. Consequently, the success of DRL in robotics has thus far been limited to simpler manipulators and tasks. In this work, we show that model-free DRL can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand, and solve them from scratch in simulated experiments. Furthermore, with the use of a small number of human demonstrations, the sample complexity can be significantly reduced, which enables learning with sample sizes equivalent to a few hours of robot experience. The use of demonstrations result in policies that exhibit very natural movements and, surprisingly, are also substantially more robust.},
archivePrefix = {arXiv},
arxivId = {1709.10087},
author = {Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
doi = {10.15607/rss.2018.xiv.049},
eprint = {1709.10087},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations{\_}Rajeswaran et al.pdf:pdf},
month = {sep},
title = {{Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations}},
url = {http://arxiv.org/abs/1709.10087},
year = {2018}
}
@article{Chiu2019,
abstract = {Predicting the future is an important aspect for decision-making in robotics or autonomous driving systems, which heavily rely upon visual scene understanding. While prior work attempts to predict future video pixels, anticipate activities or forecast future scene semantic segments from segmentation of the preceding frames, methods that predict future semantic segmentation solely from the previous frame RGB data in a single end-to-end trainable model do not exist. In this paper, we propose a temporal encoder-decoder network architecture that encodes RGB frames from the past and decodes the future semantic segmentation. The network is coupled with a new knowledge distillation training framework specifically for the forecasting task. Our method, only seeing preceding video frames, implicitly models the scene segments while simultaneously accounting for the object dynamics to infer the future scene semantic segments. Our results on Cityscapes outperform the baseline and current state-of-the-art methods. Code is available at https://github.com/eddyhkchiu/segmenting{\_}the{\_}future/.},
archivePrefix = {arXiv},
arxivId = {1904.10666},
author = {Chiu, Hsu-kuang and Adeli, Ehsan and Niebles, Juan Carlos},
eprint = {1904.10666},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Segmenting the Future{\_}Chiu, Adeli, Niebles.pdf:pdf},
month = {apr},
title = {{Segmenting the Future}},
url = {http://arxiv.org/abs/1904.10666},
year = {2019}
}
@article{Siam2018,
abstract = {Video object segmentation is an essential task in robot manipulation to facilitate grasping and learning affordances. Incremental learning is important for robotics in unstructured environments, since the total number of objects and their variations can be intractable. Inspired by the children learning process, human robot interaction (HRI) can be utilized to teach robots about the world guided by humans similar to how children learn from a parent or a teacher. A human teacher can show potential objects of interest to the robot, which is able to self adapt to the teaching signal without providing manual segmentation labels. We propose a novel teacher-student learning paradigm to teach robots about their surrounding environment. A two-stream motion and appearance "teacher" network provides pseudo-labels to adapt an appearance "student" network. The student network is able to segment the newly learned objects in other scenes, whether they are static or in motion. We also introduce a carefully designed dataset that serves the proposed HRI setup, denoted as (I)nteractive (V)ideo (O)bject (S)egmentation. Our IVOS dataset contains teaching videos of different objects, and manipulation tasks. Unlike previous datasets, IVOS provides manipulation tasks sequences with segmentation annotation along with the waypoints for the robot trajectories. It also provides segmentation annotation for the different transformations such as translation, scale, planar rotation, and out-of-plane rotation. Our proposed adaptation method outperforms the state-of-the-art on DAVIS and FBMS with 6.8{\%} and 1.2{\%} in F-measure respectively. It improves over the baseline on IVOS dataset with 46.1{\%} and 25.9{\%} in mIoU.},
archivePrefix = {arXiv},
arxivId = {1810.07733},
author = {Siam, Mennatullah and Jiang, Chen and Lu, Steven and Petrich, Laura and Gamal, Mahmoud and Elhoseiny, Mohamed and Jagersand, Martin},
eprint = {1810.07733},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Video Object Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting{\_}Siam et al.pdf:pdf},
month = {oct},
title = {{Video Object Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting}},
url = {http://arxiv.org/abs/1810.07733},
year = {2018}
}
@article{Vecerik2018a,
abstract = {Insertion is a challenging haptic and visual control problem with significant practical value for manufacturing. Existing approaches in the model-based robotics community can be highly effective when task geometry is known, but are complex and cumbersome to implement, and must be tailored to each individual problem by a qualified engineer. Within the learning community there is a long history of insertion research, but existing approaches are typically either too sample-inefficient to run on real robots, or assume access to high-level object features, e.g. socket pose. In this paper we show that relatively minor modifications to an off-the-shelf Deep-RL algorithm (DDPG), combined with a small number of human demonstrations, allows the robot to quickly learn to solve these tasks efficiently and robustly. Our approach requires no modeling or simulation, no parameterized search or alignment behaviors, no vision system aside from raw images, and no reward shaping. We evaluate our approach on a narrow-clearance peg-insertion task and a deformable clip-insertion task, both of which include variability in the socket position. Our results show that these tasks can be solved reliably on the real robot in less than 10 minutes of interaction time, and that the resulting policies are robust to variance in the socket position and orientation.},
archivePrefix = {arXiv},
arxivId = {1810.01531},
author = {Vecerik, Mel and Sushkov, Oleg and Barker, David and Roth{\"{o}}rl, Thomas and Hester, Todd and Scholz, Jon},
eprint = {1810.01531},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning{\_}Vecerik et al.pdf:pdf},
month = {oct},
title = {{A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1810.01531},
year = {2018}
}
@article{Touvron2019,
abstract = {Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the typical size of the objects seen by the classifier at train and test time. We experimentally validate that, for a target test resolution, using a lower train resolution offers better classification at test time. We then propose a simple yet effective and efficient strategy to optimize the classifier performance when the train and test resolutions differ. It involves only a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images. For instance, we obtain 77.1{\%} top-1 accuracy on ImageNet with a ResNet-50 trained on 128x128 images, and 79.8{\%} with one trained on 224x224 image. In addition, if we use extra training data we get 82.5{\%} with the ResNet-50 train with 224x224 images. Conversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4{\%} (top-5: 98.0{\%}) (single-crop). To the best of our knowledge this is the highest ImageNet single-crop, top-1 and top-5 accuracy to date.},
archivePrefix = {arXiv},
arxivId = {1906.06423},
author = {Touvron, Hugo and Vedaldi, Andrea and Douze, Matthijs and J{\'{e}}gou, Herv{\'{e}}},
eprint = {1906.06423},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Fixing the train-test resolution discrepancy{\_}Touvron et al.pdf:pdf},
month = {jun},
title = {{Fixing the train-test resolution discrepancy}},
url = {http://arxiv.org/abs/1906.06423},
year = {2019}
}
@inproceedings{VanHoof2016,
abstract = {{\textcopyright} 2016 IEEE. For many tasks, tactile or visual feedback is helpful or even crucial. However, designing controllers that take such high-dimensional feedback into account is non-trivial. Therefore, robots should be able to learn tactile skills through trial and error by using reinforcement learning algorithms. The input domain for such tasks, however, might include strongly correlated or non-relevant dimensions, making it hard to specify a suitable metric on such domains. Auto-encoders specialize in finding compact representations, where defining such a metric is likely to be easier. Therefore, we propose a reinforcement learning algorithm that can learn non-linear policies in continuous state spaces, which leverages representations learned using auto-encoders. We first evaluate this method on a simulated toytask with visual input. Then, we validate our approach on a real-robot tactile stabilization task.},
author = {{Van Hoof}, Herke and Chen, Nutan and Karl, Maximilian and {Van Der Smagt}, Patrick and Peters, Jan},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2016.7759578},
isbn = {9781509037629},
issn = {21530866},
pages = {3928--3934},
title = {{Stable reinforcement learning with autoencoders for tactile and visual data}},
volume = {2016-Novem},
year = {2016}
}
@article{George2019,
abstract = {We describe use of a bidirectional neuromyoelectric prosthetic hand that conveys biomimetic sensory feedback. Electromyographic recordings from residual arm muscles were decoded to provide independent and proportional control of a six-DOF prosthetic hand and wrist—the DEKA LUKE arm. Activation of contact sensors on the prosthesis resulted in intraneural microstimulation of residual sensory nerve fibers through chronically implanted Utah Slanted Electrode Arrays, thereby evoking tactile percepts on the phantom hand. With sensory feedback enabled, the participant exhibited greater precision in grip force and was better able to handle fragile objects. With active exploration, the participant was also able to distinguish between small and large objects and between soft and hard ones. When the sensory feedback was biomimetic—designed to mimic natural sensory signals—the participant was able to identify the objects significantly faster than with the use of traditional encoding algorithms that depended on only the present stimulus intensity. Thus, artificial touch can be sculpted by patterning the sensory feedback, and biologically inspired patterns elicit more interpretable and useful percepts.},
author = {George, J. A. and Kluger, D. T. and Davis, T. S. and Wendelken, S. M. and Okorokova, E. V. and He, Q. and Duncan, C. C. and Hutchinson, D. T. and Thumser, Z. C. and Beckler, D. T. and Marasco, P. D. and Bensmaia, S. J. and Clark, G. A.},
doi = {10.1126/scirobotics.aax2352},
issn = {2470-9476},
journal = {Science Robotics},
month = {jul},
number = {32},
pages = {eaax2352},
publisher = {Science Robotics},
title = {{Biomimetic sensory feedback through peripheral nerve stimulation improves dexterous use of a bionic hand}},
url = {http://robotics.sciencemag.org/lookup/doi/10.1126/scirobotics.aax2352},
volume = {4},
year = {2019}
}
@article{Kim2019,
abstract = {We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based methods which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters.},
archivePrefix = {arXiv},
arxivId = {1907.10830},
author = {Kim, Junho and Kim, Minjae and Kang, Hyeonwoo and Lee, Kwanghee},
eprint = {1907.10830},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - 2019 - U-GAT-IT Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image.pdf:pdf},
month = {jul},
title = {{U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation}},
url = {http://arxiv.org/abs/1907.10830},
year = {2019}
}
@article{Xue2019,
abstract = {Cervical intraepithelial neoplasia (CIN) grade of histopathology images is a crucial indicator in cervical biopsy results. Accurate CIN grading of epithelium regions helps pathologists with precancerous lesion diagnosis and treatment planning. Although an automated CIN grading system has been desired, supervised training of such a system would require a large amount of expert annotations, which are expensive and time-consuming to collect. In this paper, we investigate the CIN grade classification problem on segmented epithelium patches. We propose to use conditional Generative Adversarial Networks (cGANs) to expand the limited training dataset, by synthesizing realistic cervical histopathology images. While the synthetic images are visually appealing, they are not guaranteed to contain meaningful features for data augmentation. To tackle this issue, we propose a synthetic-image filtering mechanism based on the divergence in feature space between generated images and class centroids in order to control the feature quality of selected synthetic images for data augmentation. Our models are evaluated on a cervical histopathology image dataset with a limited number of patch-level CIN grade annotations. Extensive experimental results show a significant improvement of classification accuracy from 66.3{\%} to 71.7{\%} using the same ResNet18 baseline classifier after leveraging our cGAN generated images with feature-based filtering, which demonstrates the effectiveness of our models.},
archivePrefix = {arXiv},
arxivId = {1907.10655},
author = {Xue, Yuan and Zhou, Qianying and Ye, Jiarong and Long, L. Rodney and Antani, Sameer and Cornwell, Carl and Xue, Zhiyun and Huang, Xiaolei},
eprint = {1907.10655},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Synthetic Augmentation and Feature-based Filtering for Improved Cervical Histopathology Image Classification{\_}Xue et al.pdf:pdf},
month = {jul},
title = {{Synthetic Augmentation and Feature-based Filtering for Improved Cervical Histopathology Image Classification}},
url = {http://arxiv.org/abs/1907.10655},
year = {2019}
}
@article{Babin2019,
abstract = {The stable and repeatable grasping of objects lying on a flat hard surface is addressed in this paper. A physical model of an object lying on a flat surface and its interaction with the environment and with a gripper is proposed. The important parameters governing the interaction are obtained. From this model, a grasping procedure is established and a robotic gripper is modified in order to grant the ability to pick up large thin objects lying on smooth hard surfaces. The procedure is implemented to demonstrate its repeatability on a chosen set of objects. It is shown that by sensing the force applied on the object and by taking advantage of the nature of the contact provided by the passive joint of the modified finger, a wide range of previously not directly graspable objects are made graspable via the application of a general approach. The experimental results reported clearly show the benefits of the simple force sensing implemented in the gripper as well as of the use of passive joints when interacting with very stiff environments. The proposed approach, while simple, yields a repeatable solution to a complex manipulation problem.},
author = {Babin, Vincent and St-Onge, David and Gosselin, Cl{\'{e}}ment},
doi = {10.1016/j.rcim.2018.06.002},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Stable and repeatable grasping of flat objects on hard surfaces using passive and epicyclic mechanisms{\_}Babin, St-Onge, Gosselin.pdf:pdf},
issn = {07365845},
journal = {Robotics and Computer-Integrated Manufacturing},
keywords = {Constrained grasping,Epicyclic mechanism,Gripper,Passive finger,Repeated grasping,Robot grasping},
month = {feb},
pages = {1--10},
publisher = {Pergamon},
title = {{Stable and repeatable grasping of flat objects on hard surfaces using passive and epicyclic mechanisms}},
url = {https://www.sciencedirect.com/science/article/pii/S0736584518301297},
volume = {55},
year = {2019}
}
@article{Agostinelli2019,
abstract = {The Rubik's cube is a prototypical combinatorial puzzle that has a large state space with a single goal state. The goal state is unlikely to be accessed using sequences of randomly generated moves, posing unique challenges for machine learning. We solve the Rubik's cube with DeepCubeA, a deep reinforcement learning approach that learns how to solve increasingly difficult states in reverse from the goal state without any specific domain knowledge. DeepCubeA solves 100{\%} of all test configurations, finding a shortest path to the goal state 60.3{\%} of the time. DeepCubeA generalizes to other combinatorial puzzles and is able to solve the 15 puzzle, 24 puzzle, 35 puzzle, 48 puzzle, Lights Out and Sokoban, finding a shortest path in the majority of verifiable cases.},
author = {Agostinelli, Forest and McAleer, Stephen and Shmakov, Alexander and Baldi, Pierre},
doi = {10.1038/s42256-019-0070-z},
issn = {2522-5839},
journal = {Nature Machine Intelligence},
keywords = {Computational science,Computer science},
month = {jul},
pages = {1},
publisher = {Nature Publishing Group},
title = {{Solving the Rubik's cube with deep reinforcement learning and search}},
url = {http://www.nature.com/articles/s42256-019-0070-z},
year = {2019}
}
@article{Billard2019,
abstract = {{\textless}p{\textgreater}Dexterous manipulation is one of the primary goals in robotics. Robots with this capability could sort and package objects, chop vegetables, and fold clothes. As robots come to work side by side with humans, they must also become human-aware. Over the past decade, research has made strides toward these goals. Progress has come from advances in visual and haptic perception and in mechanics in the form of soft actuators that offer a natural compliance. Most notably, immense progress in machine learning has been leveraged to encapsulate models of uncertainty and to support improvements in adaptive and robust control. Open questions remain in terms of how to enable robots to deal with the most unpredictable agent of all, the human.{\textless}/p{\textgreater}},
author = {Billard, Aude and Kragic, Danica},
doi = {10.1126/science.aat8414},
file = {:Users/atsushi/Downloads/eaat8414.full.pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {6446},
pages = {eaat8414},
title = {{Trends and challenges in robot manipulation}},
url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aat8414},
volume = {364},
year = {2019}
}
@article{Brown2019,
abstract = {{\textless}p{\textgreater}In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold'em poker, the most popular form of poker played by humans.{\textless}/p{\textgreater}},
author = {Brown, Noam and Sandholm, Tuomas},
doi = {10.1126/science.aay2400},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Brown, Sandholm - 2019 - Superhuman AI for multiplayer poker.pdf:pdf},
issn = {0036-8075},
journal = {Science},
month = {jul},
pages = {eaay2400},
publisher = {American Association for the Advancement of Science},
title = {{Superhuman AI for multiplayer poker}},
url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aay2400},
year = {2019}
}
@article{Sharma2019,
abstract = {We introduce a computationally-efficient CNN micro-architecture Slim Module to design a lightweight deep neural network Slim-Net for face attribute prediction. Slim Modules are constructed by assembling depthwise separable convolutions with pointwise convolution to produce a computationally efficient module. The problem of facial attribute prediction is challenging because of the large variations in pose, background, illumination, and dataset imbalance. We stack these Slim Modules to devise a compact CNN which still maintains very high accuracy. Additionally, the neural network has a very low memory footprint which makes it suitable for mobile and embedded applications. Experiments on the CelebA dataset show that Slim-Net achieves an accuracy of 91.24{\%} with at least 25 times fewer parameters than comparably performing methods, which reduces the memory storage requirement of Slim-net by at least 87{\%}.},
archivePrefix = {arXiv},
arxivId = {1907.02157},
author = {Sharma, Ankit and Foroosh, Hassan},
eprint = {1907.02157},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Sharma, Foroosh - 2019 - Slim-CNN A Light-Weight CNN for Face Attribute Prediction.pdf:pdf},
month = {jul},
title = {{Slim-CNN: A Light-Weight CNN for Face Attribute Prediction}},
url = {http://arxiv.org/abs/1907.02157},
year = {2019}
}
@article{Qin2019,
abstract = {In this paper, we study the problem of 3D object detection from stereo images, in which the key challenge is how to effectively utilize stereo information. Different from previous methods using pixel-level depth maps, we propose employing 3D anchors to explicitly construct object-level correspondences between the regions of interest in stereo images, from which the deep neural network learns to detect and triangulate the targeted object in 3D space. We also introduce a cost-efficient channel reweighting strategy that enhances representational features and weakens noisy signals to facilitate the learning process. All of these are flexibly integrated into a solid baseline detector that uses monocular images. We demonstrate that both the monocular baseline and the stereo triangulation learning network outperform the prior state-of-the-arts in 3D object detection and localization on the challenging KITTI dataset.},
archivePrefix = {arXiv},
arxivId = {1906.01193},
author = {Qin, Zengyi and Wang, Jinglu and Lu, Yan},
eprint = {1906.01193},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Qin, Wang, Lu - 2019 - Triangulation Learning Network from Monocular to Stereo 3D Object Detection.pdf:pdf},
month = {jun},
title = {{Triangulation Learning Network: from Monocular to Stereo 3D Object Detection}},
url = {http://arxiv.org/abs/1906.01193},
year = {2019}
}
@techreport{Razavi2019,
abstract = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.},
archivePrefix = {arXiv},
arxivId = {1906.00446},
author = {Razavi, Ali and van den Oord, Aaron and Vinyals, Oriol},
eprint = {1906.00446},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Razavi, van den Oord, Vinyals DeepMind - Unknown - Generating Diverse High-Fidelity Images with VQ-VAE-2.pdf:pdf},
title = {{Generating Diverse High-Fidelity Images with VQ-VAE-2}},
url = {http://arxiv.org/abs/1906.00446},
year = {2019}
}
@techreport{Luketina2011,
abstract = {To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally , we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.},
author = {Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rockt, Tim},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Luketina et al. - Unknown - A Survey of Reinforcement Learning Informed by Natural Language.pdf:pdf},
title = {{A Survey of Reinforcement Learning Informed by Natural Language}},
url = {http://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/luketinaijcai19.pdf},
year = {2011}
}
@article{Lee2018c,
abstract = {Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. Results for simulated and real robot experiments are presented.},
archivePrefix = {arXiv},
arxivId = {1810.10191},
author = {Lee, Michelle A. and Zhu, Yuke and Srinivasan, Krishnan and Shah, Parth and Savarese, Silvio and Fei-Fei, Li and Garg, Animesh and Bohg, Jeannette},
eprint = {1810.10191},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Making Sense of Vision and Touch Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks{\_}Lee et al.pdf:pdf},
title = {{Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks}},
url = {http://arxiv.org/abs/1810.10191},
year = {2018}
}
@article{Takahashi2018,
abstract = {Estimation of tactile properties from vision, such as slipperiness or roughness, is important to effectively interact with the environment. These tactile properties help us decide which actions we should choose and how to perform them. E.g., we can drive slower if we see that we have bad traction or grasp tighter if an item looks slippery. We believe that this ability also helps robots to enhance their understanding of the environment, and thus enables them to tailor their actions to the situation at hand. We therefore propose a model to estimate the degree of tactile properties from visual perception alone (e.g., the level of slipperiness or roughness). Our method extends a encoder-decoder network, in which the latent variables are visual and tactile features. In contrast to previous works, our method does not require manual labeling, but only RGB images and the corresponding tactile sensor data. All our data is collected with a webcam and uSkin tactile sensor mounted on the end-effector of a Sawyer robot, which strokes the surfaces of 25 different materials. We show that our model generalizes to materials not included in the training data by evaluating the feature space, indicating that it has learned to associate important tactile properties with images.},
archivePrefix = {arXiv},
arxivId = {1803.03435},
author = {Takahashi, Kuniyuki and Tan, Jethro},
eprint = {1803.03435},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Deep Visuo-Tactile Learning Estimation of Tactile Properties from Images{\_}Takahashi, Tan.pdf:pdf},
pages = {1--7},
title = {{Deep Visuo-Tactile Learning: Estimation of Tactile Properties from Images}},
url = {http://arxiv.org/abs/1803.03435},
year = {2018}
}
@article{Araki2018,
abstract = {Accurate grasping of objects such as industrial parts and everyday necessities is an important task for industrial robots and living-support robots. Many methods have been proposed for grasp point detection for robots, some that utilize machine learning and some that do not. Recently, a grasp point detection method using a 2-stage deep neural network has been proposed. Although the 2-stage deep neural network could detect the grasping point of no-learned objects, the computation cost would be high. In this paper, we propose a method for detecting grasping points using one deep convolutional neural network (DCNN) introducing graspability. Simultaneous detection of grasping points and graspability in one neural network lessens calculation costs. Evaluation experiments confirmed that grasping points could be properly detected using graspability.},
author = {Araki, Ryosuke and Hasegawa, Takahiro and Yamauchi, Yuji and Yamashita, Takayoshi and Fujiyoshi, Hironobu and Domae, Yukiyasu and Kawanishi, Ryosuke and Seki, Makito},
doi = {10.7210/jrsj.36.559},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Araki et al. - 2018 - Graspabilityを導入したDCNNによる物体把持位置検出.pdf:pdf},
issn = {0289-1824},
journal = {Journal of the Robotics Society of Japan},
month = {nov},
number = {8},
pages = {559--566},
publisher = {The Robotics Society of Japan},
title = {{Graspabilityを導入したDCNNによる物体把持位置検出}},
volume = {36},
year = {2018}
}
@article{Garcia-Garcia2017,
abstract = {Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.},
archivePrefix = {arXiv},
arxivId = {1704.06857},
author = {Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Garcia-Rodriguez, Jose},
eprint = {1704.06857},
pages = {1--23},
title = {{A Review on Deep Learning Techniques Applied to Semantic Segmentation}},
url = {http://arxiv.org/abs/1704.06857},
year = {2017}
}
@article{Hatori2018,
abstract = {Comprehension of spoken natural language is an essential component for robots to communicate with human effectively. However, handling unconstrained spoken instructions is challenging due to (1) complex structures including a wide variety of expressions used in spoken language and (2) inherent ambiguity in interpretation of human instructions. In this paper, we propose the first comprehensive system that can handle unconstrained spoken language and is able to effectively resolve ambiguity in spoken instructions. Specifically, we integrate deep-learning-based object detection together with natural language processing technologies to handle unconstrained spoken instructions, and propose a method for robots to resolve instruction ambiguity through dialogue. Through our experiments on both a simulated environment as well as a physical industrial robot arm, we demonstrate the ability of our system to understand natural instructions from human operators effectively, and how higher success rates of the object picking task can be achieved through an interactive clarification process.},
author = {Hatori, Jun and Kikuchi, Yuta and Kobayashi, Sosuke and Takahashi, Kuniyuki and Tsuboi, Yuta and Unno, Yuya and Ko, Wilson and Tan, Jethro},
doi = {10.1109/ICRA.2018.8460699},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3774--3781},
publisher = {IEEE},
title = {{Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions}},
year = {2018}
}
@article{Sevastopolsky2018,
abstract = {In this work, we propose a special cascade network for image segmentation, which is based on the U-Net networks as building blocks and the idea of the iterative refinement. The model was mainly applied to achieve higher recognition quality for the task of finding borders of the optic disc and cup, which are relevant to the presence of glaucoma. Compared to a single U-Net and the state-of-the-art methods for the investigated tasks, very high segmentation quality has been achieved without a need for increasing the volume of datasets. Our experiments include comparison with the best-known methods on publicly available databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS, and evaluation on a private data set collected in collaboration with University of California San Francisco Medical School. The analysis of the architecture details is presented, and it is argued that the model can be employed for a broad scope of image segmentation problems of similar nature.},
archivePrefix = {arXiv},
arxivId = {1804.11294},
author = {Sevastopolsky, Artem and Drapak, Stepan and Kiselev, Konstantin and Snyder, Blake M. and Keenan, Jeremy D. and Georgievskaya, Anastasia},
eprint = {1804.11294},
title = {{Stack-U-Net: Refinement Network for Image Segmentation on the Example of Optic Disc and Cup}},
url = {http://arxiv.org/abs/1804.11294},
year = {2018}
}
@article{Zou2019,
abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today's object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century's time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
archivePrefix = {arXiv},
arxivId = {1905.05055},
author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
eprint = {1905.05055},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Zou et al. - 2019 - Object Detection in 20 Years A Survey.pdf:pdf},
month = {may},
title = {{Object Detection in 20 Years: A Survey}},
url = {http://arxiv.org/abs/1905.05055},
year = {2019}
}
@article{Liu2018b,
abstract = {Generic object detection, aiming at locating object instances from a large number of predefined categories in natural images, is one of the most fundamental and challenging problems in computer vision. Deep learning techniques have emerged in recent years as powerful methods for learning feature representations directly from data, and have led to remarkable breakthroughs in the field of generic object detection. Given this time of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought by deep learning techniques. More than 250 key contributions are included in this survey, covering many aspects of generic object detection research: leading detection frameworks and fundamental subproblems including object feature representation, object proposal generation, context information modeling and training strategies; evaluation issues, specifically benchmark datasets, evaluation metrics, and state of the art performance. We finish by identifying promising directions for future research.},
archivePrefix = {arXiv},
arxivId = {1809.02165},
author = {Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Liu, Xinwang and Pietik{\"{a}}inen, Matti},
eprint = {1809.02165},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2018 - Deep Learning for Generic Object Detection A Survey.pdf:pdf},
month = {sep},
title = {{Deep Learning for Generic Object Detection: A Survey}},
url = {http://arxiv.org/abs/1809.02165},
year = {2018}
}
@article{Wang2019,
abstract = {Convolutional neural networks (CNNs) have enabled the state-of-the-art performance in many computer vision tasks. However, little effort has been devoted to establishing convolution in non-linear space. Existing works mainly leverage on the activation layers, which can only provide point-wise non-linearity. To solve this problem, a new operation, kervolution (kernel convolution), is introduced to approximate complex behaviors of human perception systems leveraging on the kernel trick. It generalizes convolution, enhances the model capacity, and captures higher order interactions of features, via patch-wise kernel functions, but without introducing additional parameters. Extensive experiments show that kervolutional neural networks (KNN) achieve higher accuracy and faster convergence than baseline CNN.},
archivePrefix = {arXiv},
arxivId = {1904.03955},
author = {Wang, Chen and Yang, Jianfei and Xie, Lihua and Yuan, Junsong},
eprint = {1904.03955},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2019 - Kervolutional Neural Networks.pdf:pdf},
month = {apr},
title = {{Kervolutional Neural Networks}},
url = {http://arxiv.org/abs/1904.03955},
year = {2019}
}
@article{Chebotar2018,
abstract = {We consider the problem of transferring policies to the real world by training on a distribution of simulated scenarios. Rather than manually tuning the randomization of simulations, we adapt the simulation parameter distribution using a few real world roll-outs interleaved with policy training. In doing so, we are able to change the distribution of simulations to improve the policy transfer by matching the policy behavior in simulation and the real world. We show that policies trained with our method are able to reliably transfer to different robots in two real world tasks: swing-peg-in-hole and opening a cabinet drawer. The video of our experiments can be found at https://sites.google.com/view/simopt},
archivePrefix = {arXiv},
arxivId = {1810.05687},
author = {Chebotar, Yevgen and Handa, Ankur and Makoviychuk, Viktor and Macklin, Miles and Issac, Jan and Ratliff, Nathan and Fox, Dieter},
eprint = {1810.05687},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Chebotar et al. - 2018 - Closing the Sim-to-Real Loop Adapting Simulation Randomization with Real World Experience.pdf:pdf},
month = {oct},
title = {{Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience}},
url = {http://arxiv.org/abs/1810.05687},
year = {2018}
}
@article{Amini2018,
abstract = {Deep learning has revolutionized the ability to learn "end-to-end" autonomous vehicle control directly from raw sensory data. While there have been recent advances on extensions to handle forms of navigation instruction, these works are unable to capture the full distribution of possible actions that could be taken and to reason about localization of the robot within the environment. In this paper, we extend end-to-end driving networks with the ability to understand maps. We define a novel variational network capable of learning from raw camera data of the environment as well as higher level roadmaps to predict (1) a full probability distribution over the possible control commands; and (2) a deterministic control command capable of navigating on the route specified within the map. Additionally, we formulate how our model can be used to localize the robot according to correspondences between the map and the observed visual road topology, inspired by the rough localization that human drivers can perform. We evaluate our algorithms on real-world driving data, and reason about the robustness of the inferred steering commands under various types of rich driving scenarios. In addition, we evaluate our localization algorithm over a new set of roads and intersections which the model has never driven through and demonstrate rough localization in situations without any GPS prior.},
archivePrefix = {arXiv},
arxivId = {1811.10119},
author = {Amini, Alexander and Rosman, Guy and Karaman, Sertac and Rus, Daniela},
eprint = {1811.10119},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Amini et al. - 2018 - Variational End-to-End Navigation and Localization.pdf:pdf},
month = {nov},
title = {{Variational End-to-End Navigation and Localization}},
url = {http://arxiv.org/abs/1811.10119},
year = {2018}
}
@article{Takahashi2018,
abstract = {Estimation of tactile properties from vision, such as slipperiness or roughness, is important to effectively interact with the environment. These tactile properties help us decide which actions we should choose and how to perform them. E.g., we can drive slower if we see that we have bad traction or grasp tighter if an item looks slippery. We believe that this ability also helps robots to enhance their understanding of the environment, and thus enables them to tailor their actions to the situation at hand. We therefore propose a model to estimate the degree of tactile properties from visual perception alone (e.g., the level of slipperiness or roughness). Our method extends a encoder-decoder network, in which the latent variables are visual and tactile features. In contrast to previous works, our method does not require manual labeling, but only RGB images and the corresponding tactile sensor data. All our data is collected with a webcam and uSkin tactile sensor mounted on the end-effector of a Sawyer robot, which strokes the surfaces of 25 different materials. We show that our model generalizes to materials not included in the training data by evaluating the feature space, indicating that it has learned to associate important tactile properties with images.},
archivePrefix = {arXiv},
arxivId = {1803.03435},
author = {Takahashi, Kuniyuki and Tan, Jethro},
eprint = {1803.03435},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Takahashi, Tan - 2018 - Deep Visuo-Tactile Learning Estimation of Tactile Properties from Images.pdf:pdf},
month = {mar},
title = {{Deep Visuo-Tactile Learning: Estimation of Tactile Properties from Images}},
url = {http://arxiv.org/abs/1803.03435},
year = {2018}
}
@article{Lee2018b,
abstract = {Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. Results for simulated and real robot experiments are presented.},
archivePrefix = {arXiv},
arxivId = {1810.10191},
author = {Lee, Michelle A. and Zhu, Yuke and Srinivasan, Krishnan and Shah, Parth and Savarese, Silvio and Fei-Fei, Li and Garg, Animesh and Bohg, Jeannette},
eprint = {1810.10191},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2018 - Making Sense of Vision and Touch Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks.pdf:pdf},
month = {oct},
title = {{Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks}},
url = {http://arxiv.org/abs/1810.10191},
year = {2018}
}
@article{Watkins2005,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1007/bf00992698},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Watkins, Dayan - 2005 - Q-learning.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {may},
number = {3-4},
pages = {279--292},
publisher = {Kluwer Academic Publishers},
title = {{Q-learning}},
url = {http://link.springer.com/10.1007/BF00992698},
volume = {8},
year = {2005}
}
@article{Zhou2019,
abstract = {Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1{\%} AP at 142 FPS, 37.4{\%} AP at 52 FPS, and 45.1{\%} AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
archivePrefix = {arXiv},
arxivId = {1904.07850},
author = {Zhou, Xingyi and Wang, Dequan and Kr{\"{a}}henb{\"{u}}hl, Philipp},
eprint = {1904.07850},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, Wang, Kr{\"{a}}henb{\"{u}}hl - 2019 - Objects as Points.pdf:pdf},
month = {apr},
title = {{Objects as Points}},
url = {http://arxiv.org/abs/1904.07850},
year = {2019}
}
@techreport{Kawaharazuka2019,
abstract = {For dynamic manipulation of flexible objects, we propose an acquisition method of a flexible object motion equation model using a deep neural network and a control method to realize a target state by calculating an optimized time-series joint torque command. By using the proposed method, any physics model of a target object is not needed, and the object can be controlled as intended. We applied this method to manipulations of a rigid object, a flexible object with and without environmental contact, and a cloth, and verified its effectiveness.},
archivePrefix = {arXiv},
arxivId = {1901.10142},
author = {Kawaharazuka, Kento and Ogawa, Toru and Tamura, Juntaro and Nabeshima, Cota},
eprint = {1901.10142},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Kawaharazuka et al. - Unknown - Dynamic Manipulation of Flexible Objects with Torque Sequence Using a Deep Neural Network.pdf:pdf},
title = {{Dynamic Manipulation of Flexible Objects with Torque Sequence Using a Deep Neural Network}},
url = {http://arxiv.org/abs/1901.10142},
year = {2019}
}
@techreport{Arakawa2018,
abstract = {Exploration has been one of the greatest challenges in reinforcement learning (RL), which is a large obstacle in the application of RL to robotics. Even with state-of-the-art RL algorithms, building a well-learned agent often requires too many trials, mainly due to the difficulty of matching its actions with rewards in the distant future. A remedy for this is to train an agent with real-time feedback from a human observer who immediately gives rewards for some actions. This study tackles a series of challenges for introducing such a human-in-the-loop RL scheme. The first contribution of this work is our experiments with a precisely modeled human observer: binary, delay, stochasticity, unsustainability, and natural reaction. We also propose an RL method called DQN-TAMER, which efficiently uses both human feedback and distant rewards. We find that DQN-TAMER agents outperform their baselines in Maze and Taxi simulated environments. Furthermore, we demonstrate a real-world human-in-the-loop RL application where a camera automatically recognizes a user's facial expressions as feedback to the agent while the agent explores a maze.},
archivePrefix = {arXiv},
arxivId = {1810.11748},
author = {Arakawa, Riku and Kobayashi, Sosuke and Unno, Yuya and Tsuboi, Yuta and Maeda, Shin-ichi},
eprint = {1810.11748},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Arakawa et al. - Unknown - DQN-TAMER Human-in-the-Loop Reinforcement Learning with Intractable Feedback.pdf:pdf},
title = {{DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback}},
url = {http://arxiv.org/abs/1810.11748},
year = {2018}
}
@article{Horiuchi2017,
author = {Horiuchi, Yuuki and Makino, Yasutoshi and Shinoda, Hiroyuki},
doi = {10.1145/3132272.3135076},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2017{\_}Computational Foresight Forecasting Human Body Motion in Real-time for Reducing Delays in Interactive System{\_}Horiuchi, Makino, Shin.pdf:pdf},
isbn = {9781450346917},
journal = {Proceedings of the Interactive Surfaces and Spaces, {\{}ISS{\}} 2017, Brighton, United Kingdom, October 17 - 20, 2017},
pages = {312--317},
title = {{Computational Foresight: Forecasting Human Body Motion in Real-time for Reducing Delays in Interactive System}},
url = {https://doi.org/10.1145/3132272.3135076},
year = {2017}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Driessche, George Van Den and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Mastering the game of Go with deep neural networks and tree search.{\_}Silver et al.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Mastering the game of Go with deep neural networks and tree search.{\_}Silver et al.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
number = {7587},
pages = {484--9},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26819042},
volume = {529},
year = {2016}
}
@article{Mnih2013a,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
eprint = {1312.5602},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2013{\_}Playing Atari with Deep Reinforcement Learning{\_}Mnih et al.pdf:pdf},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Gu2017b,
abstract = {Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.},
author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey and {Venkatraman Narayanan}, Maxim Likhachev},
doi = {10.1109/ICRA.2017.7989385},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2017{\_}Deep Reinforcement Learning for Robotic Manipulation{\_}Gu et al.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates{\_}Gu et al.pdf:pdf;:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Venkatraman Narayanan - 2017 - Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates Shixiang.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Automatisierungstechnik,Computer Vision for Automa},
pages = {3389--3396},
title = {{Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates}},
url = {http://ieeexplore.ieee.org/servlet/opac?punumber=7960754},
year = {2017}
}
@article{Xiao2018,
abstract = {Figure 1. Our self-contained projection smartwatch (A) provides rectified graphics with touch input on the skin (B). We use a slide-to-unlock mechanism to reject inadvertent touches and provide a rapid projection calibration (C) before apps can be used (D). ABSTRACT Compact, worn computers with projected, on-skin touch interfaces have been a long-standing yet elusive goal, largely written off as science fiction. Such devices offer the potential to mitigate the significant human input/output bottleneck inherent in worn devices with small screens. In this work, we present the first, fully-functional and self-contained projection smartwatch implementation, containing the requisite compute, power, projection and touch-sensing capabilities. Our watch offers roughly 40 cm 2 of interactive surface area-more than five times that of a typical smartwatch display. We demonstrate continuous 2D finger tracking with interactive, rectified graphics, transforming the arm into a touchscreen. We discuss our hardware and software implementation, as well as evaluation results regarding touch accuracy and projection visibility.},
author = {Xiao, Robert and Cao, Teng and Guo, Ning and Zhuo, Jun and Zhang, Yang and Harrison, Chris},
doi = {10.1145/3173574.3173669},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}LumiWatch On-Arm Projected Graphics and Touch Input{\_}Xiao et al.pdf:pdf},
isbn = {9781450356206},
journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI '18},
keywords = {Author's kit,Conference Publications,Guides,depth sensing,instructions,on-body interaction,projection,smartwatch,time-of-flight,touch interaction},
pages = {1--11},
title = {{LumiWatch: On-Arm Projected Graphics and Touch Input}},
url = {https://doi.org/10.1145/3173574.3173669},
year = {2018}
}
@article{Lv2015a,
abstract = {There is an increasing interest in creating pervasive games based on emerging interaction technologies. In order to develop touch-less, interactive and augmented reality games on vision-based wearable device, a touch-less motion interaction technology is designed and evaluated in this work. Users interact with the augmented reality games with dynamic hands/feet gestures in front of the camera, which triggers the interaction event to interact with the virtual object in the scene. Three primitive augmented reality games with eleven dynamic gestures are developed based on the proposed touch-less interaction technology as proof. At last, a comparing evaluation is proposed to demonstrate the social acceptability and usability of the touch-less approach, running on a hybrid wearable framework or with Google Glass, as well as workload assessment, user's emotions and satisfaction.},
author = {Lv, Zhihan and Halawani, Alaa and Feng, Shengzhong and {Ur R{\'{e}}hman}, Shafiq and Li, Haibo},
doi = {10.1007/s00779-015-0844-1},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2015{\_}Touch-less interactive augmented reality game on vision-based wearable device{\_}Lv et al.pdf:pdf},
issn = {16174909},
journal = {Personal and Ubiquitous Computing},
keywords = {Augmented reality game,Hand free,Pervasive game,Smartphone game,Touch-less,Wearable device,game {\'{a}},game {\'{a}} augmented reality,hand free {\'{a}} pervasive,wearable device {\'{a}} smartphone},
number = {3},
pages = {551--567},
title = {{Touch-less interactive augmented reality game on vision-based wearable device}},
volume = {19},
year = {2015}
}
@article{Cvpr2019a,
abstract = {Weakly supervised object detection (WSOD) is a challenging task when provided with image category supervision but required to simultaneously learn object locations and object detectors. Many WSOD approaches adopt multiple instance learning (MIL) and have non-convex loss functions which are prone to get stuck into local minima (falsely localize object parts) while missing full object extent during training. In this paper, we introduce a continuation optimization method into MIL and thereby creating continuation multiple instance learning (C-MIL), with the intention of alleviating the non-convexity problem in a systematic way. We partition instances into spatially related and class related subsets, and approximate the original loss function with a series of smoothed loss functions defined within the subsets. Optimizing smoothed loss functions prevents the training procedure falling prematurely into local minima and facilitates the discovery of Stable Semantic Extremal Regions (SSERs) which indicate full object extent. On the PASCAL VOC 2007 and 2012 datasets, C-MIL improves the state-of-the-art of weakly supervised object detection and weakly supervised object localization with large margins.},
archivePrefix = {arXiv},
arxivId = {1904.05647},
author = {Wan, Fang and Liu, Chang and Ke, Wei and Ji, Xiangyang and Jiao, Jianbin and Ye, Qixiang},
eprint = {1904.05647},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}C-MIL Continuation Multiple Instance Learning for Weakly Supervised Object Detection{\_}Cvpr, Id(2).pdf:pdf},
month = {apr},
title = {{C-MIL : Continuation Multiple Instance Learning for Weakly Supervised Object Detection}},
url = {http://arxiv.org/abs/1904.05647},
year = {2019}
}
@article{Lee2009,
abstract = {This paper proposes an interactive e-learning system using pattern recognition and augmented reality. The goal of proposed system is to provide students with realistic audio-visual contents when they are leaning. The proposed e- learning system consists of image recognition, color and polka-dot pattern recognition, and augmented reality engine with audio-visual contents. When the web camera on a PC captures the current page of textbook, the e-Iearning system first identifies the images on the page, and augments some audio-visual contents on the monitor. For interactive learning, the proposed e-learning system exploits the color-band or polka-dot markers which are stuck to the end of a finger. The color-band and polka-dot marker act like the mouse cursor to indicate the position in the textbook image. Appropriate interactive audio-visual contents are augmented as the marker is located on the predefined image objects in the textbook. The proposed e-learning system was applied to the edu},
author = {{Sang Hwa}, Lee and Junyeong, Choi and Jong-Il, Park and Lee, Sang Hwa and Choi, Junyeong and Park, Jong Il and {Sang Hwa}, Lee and Junyeong, Choi and Jong-Il, Park},
doi = {10.1109/TCE.2009.5174470},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2009 - Interactive E-Learning System Using Pattern Recognition and Augmented Reality(3).pdf:pdf;:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2009 - Interactive E-Learning System Using Pattern Recognition and Augmented Reality(2).pdf:pdf},
issn = {00983063},
journal = {IEEE Transactions on Consumer Electronics},
keywords = {AUDIOVISUAL education,Augmented reality,COMPUTER network resources,E-learning system,INTERACTIVE multimedia,Interactive learning,PATTERN perception,Pattern recognition,STUDENTS -- Services for,WEB-based instruction,augmented reality,interactive learning,pattern recognition},
number = {2},
pages = {883--890},
title = {{Interactive E-Learning System Using Pattern Recognition and Augmented Reality.}},
url = {http://search-1ebscohost-1com-1ebsco.han.buw.uw.edu.pl/login.aspx?direct=true{\&}db=a9h{\&}AN=43924116{\&}lang=pl{\&}site=ehost-live},
volume = {55},
year = {2009}
}
@article{Mousavi2018c,
abstract = {In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This chapter reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.},
archivePrefix = {arXiv},
arxivId = {arXiv:1701.07274v6},
author = {Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {arXiv:1701.07274v6},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Deep Reinforcement Learning An Overview{\_}Mousavi, Schukat, Howley.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Deep Reinforcement Learning An Overview{\_}Mousavi, Schukat, Howley.pdf:pdf},
issn = {23673389},
journal = {Lecture Notes in Networks and Systems},
keywords = {Deep leaning,MDPs,Neural networks,Observable MDPs,Reinforcement learning},
pages = {426--440},
title = {{Deep Reinforcement Learning: An Overview}},
volume = {16},
year = {2018}
}
@article{Denil2016a,
abstract = {When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.},
archivePrefix = {arXiv},
arxivId = {1611.01843},
author = {Denil, Misha and Agrawal, Pulkit and Kulkarni, Tejas D and Erez, Tom and Battaglia, Peter and de Freitas, Nando},
eprint = {1611.01843},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Learning to Perform Physics Experiments via Deep Reinforcement Learning{\_}Denil et al.pdf:pdf},
pages = {1--15},
title = {{Learning to Perform Physics Experiments via Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.01843},
year = {2016}
}
@article{Chamzas2019a,
abstract = {Sampling-based planners are effective in many real-world applications such as robotics manipulation, navigation, and even protein modeling. However, it is often challenging to generate a collision-free path in environments where key areas are hard to sample. In the absence of any prior information, sampling-based planners are forced to explore uniformly or heuristically, which can lead to degraded performance. One way to improve performance is to use prior knowledge of environments to adapt the sampling strategy to the problem at hand. In this work, we decompose the workspace into local primitives, memorizing local experiences by these primitives in the form of local samplers, and store them in a database. We synthesize an efficient global sampler by retrieving local experiences relevant to the given situation. Our method transfers knowledge effectively between diverse environments that share local primitives and speeds up the performance dramatically. Our results show, in terms of solution time, an improvement of multiple orders of magnitude in two traditionally challenging high-dimensional problems compared to state-of-the-art approaches.},
archivePrefix = {arXiv},
arxivId = {1903.08693},
author = {Chamzas, Constantinos and Shrivastava, Anshumali and Kavraki, Lydia E.},
eprint = {1903.08693},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2019{\_}Using Local Experiences for Global Motion Planning{\_}Chamzas, Shrivastava, Kavraki.pdf:pdf},
month = {mar},
title = {{Using Local Experiences for Global Motion Planning}},
url = {http://arxiv.org/abs/1903.08693},
year = {2019}
}
@article{Hu2018,
abstract = {Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to {\~{}}100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.},
archivePrefix = {arXiv},
arxivId = {arXiv:1711.10370v2},
author = {Hu, Ronghang and Dollar, Piotr and He, Kaiming and Darrell, Trevor and Girshick, Ross},
doi = {10.1109/CVPR.2018.00445},
eprint = {arXiv:1711.10370v2},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Learning to Segment Every Thing{\_}Hu et al.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {4233--4241},
title = {{Learning to Segment Every Thing}},
year = {2018}
}
@article{Levine2015b,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
eprint = {1504.00702},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}End-to-End Training of Deep Visuomotor Policies{\_}Levine et al.pdf:pdf},
keywords = {neural networks,optimal control,reinforcement learning,vision},
pages = {1--40},
title = {{End-to-End Training of Deep Visuomotor Policies}},
url = {http://arxiv.org/abs/1504.00702},
volume = {17},
year = {2015}
}
@article{Finn2016b,
abstract = {A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a "visual imagination" of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.},
archivePrefix = {arXiv},
arxivId = {1605.07157},
author = {Finn, Chelsea and Goodfellow, Ian and Levine, Sergey},
eprint = {1605.07157},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Unsupervised Learning for Physical Interaction through Video Prediction{\_}Finn, Goodfellow, Levine.pdf:pdf},
title = {{Unsupervised Learning for Physical Interaction through Video Prediction}},
url = {http://arxiv.org/abs/1605.07157},
year = {2016}
}
@article{CanonBermudez2018,
abstract = {Electronic skins equipped with artificial receptors are able to extend our perception beyond the modalities that have naturally evolved. These synthetic receptors offer complimentary information on our surroundings and endow us with novel means of manipulating physical or even virtual objects. We realize highly compliant magnetosensitive skins with directional perception that enable magnetic cognition, body position tracking, and touchless object manipulation. Transfer printing of eight high-performance spin valve sensors arranged into two Wheatstone bridges onto 1.7-$\mu$m-thick polyimide foils ensures mechanical imperceptibility. This resembles a new class of interactive devices extracting information from the surroundings through magnetic tags. We demonstrate this concept in augmented reality systems with virtual knob-turning functions and the operation of virtual dialing pads, based on the interaction with magnetic fields. This technology will enable a cornucopia of applications from navigation, motion tracking in robotics, regenerative medicine, and sports and gaming to interaction in supplemented reality.},
author = {{Ca{\~{n}}{\'{o}}n Berm{\'{u}}dez}, Gilbert Santiago and Karnaushenko, Dmitriy Daniil Dmitriy D. and Karnaushenko, Dmitriy Daniil Dmitriy D. and Lebanov, Ana and Bischoff, Lothar and Kaltenbrunner, Martin and Fassbender, J{\"{u}}rgen and Schmidt, Oliver G. and Makarov, Denys and Santiago, Gilbert and Berm{\'{u}}dez, Ca{\~{n}}{\'{o}}n and Karnaushenko, Dmitriy Daniil Dmitriy D. and Karnaushenko, Dmitriy Daniil Dmitriy D. and Lebanov, Ana and Bischoff, Lothar and Kaltenbrunner, Martin and Fassbender, J{\"{u}}rgen and Schmidt, Oliver G. and Makarov, Denys},
doi = {10.1126/sciadv.aao2623},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Magnetosensitive e-skins with directional perception for augmented reality{\_}Ca{\~{n}}{\'{o}}n Berm{\'{u}}dez et al.pdf:pdf},
issn = {23752548},
journal = {Science Advances},
number = {1},
title = {{Magnetosensitive e-skins with directional perception for augmented reality}},
volume = {4},
year = {2018}
}
@article{Tao2019a,
abstract = {Current deep neural networks suffer from two problems; first, they are hard to interpret, and second, they suffer from overfitting. There have been many attempts to define interpretability in neural networks, but they typically lack causality or generality. A myriad of regularization techniques have been developed to prevent overfitting, and this has driven deep learning to become the hot topic it is today; however, while most regularization techniques are justified empirically and even intuitively, there is not much underlying theory. This paper argues that to extract the features used in neural networks to make decisions, it's important to look at the paths between clusters existing in the hidden spaces of neural networks. These features are of particular interest because they reflect the true decision making process of the neural network. This analysis is then furthered to present an ensemble algorithm for arbitrary neural networks which has guarantees for test accuracy. Finally, a discussion detailing the aforementioned guarantees is introduced and the implications to neural networks, including an intuitive explanation for all current regularization methods, are presented. The ensemble algorithm has generated state-of-the-art results for Wide-ResNet on CIFAR-10 and has improved test accuracy for all models it has been applied to.},
archivePrefix = {arXiv},
arxivId = {1904.05488},
author = {Tao, Sean},
eprint = {1904.05488},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Deep Neural Network Ensembles{\_}Tao.pdf:pdf},
month = {apr},
title = {{Deep Neural Network Ensembles}},
url = {http://arxiv.org/abs/1904.05488},
year = {2019}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P. and Silver, David and Com, Korayk Google and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Asynchronous Methods for Deep Reinforcement Learning{\_}Mnih et al.pdf:pdf},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@article{James2018a,
abstract = {Real world data, especially in the domain of robotics, is notoriously costly to collect. One way to circumvent this can be to leverage the power of simulation in order to produce large amounts of labelled data. However, training models on simulated images does not readily transfer to real-world ones. Using domain adaptation methods to cross this "reality gap" requires at best a large amount of unlabelled real-world data, whilst domain randomization alone can waste modeling power, rendering certain reinforcement learning (RL) methods unable to learn the task of interest. In this paper, we present Randomized-to-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no real-world data. Our method learns to translate randomized rendered images into their equivalent non-randomized, canonical versions. This in turn allows for real images to also be translated into canonical sim images. We demonstrate the effectiveness of this sim-to-real approach by training a vision-based closed-loop grasping reinforcement learning agent in simulation, and then transferring it to the real world to attain 70{\%} zero-shot grasp success on unseen objects, a result that almost doubles the success of learning the same task directly on domain randomization alone. Additionally, by joint finetuning in the real-world with only 5,000 real-world grasps, our method achieves 91{\%}, outperforming a state-of-the-art system trained with 580,000 real-world grasps, resulting in a reduction of real-world data by more than 99{\%}.},
archivePrefix = {arXiv},
arxivId = {1812.07252},
author = {James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos},
eprint = {1812.07252},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/James et al. - 2018 - Sim-to-Real via Sim-to-Sim Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Sim-to-Real via Sim-to-Sim Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks{\_}James et al.pdf:pdf},
month = {dec},
title = {{Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks}},
url = {http://arxiv.org/abs/1812.07252},
year = {2018}
}
@article{He2017a,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
doi = {10.1109/ICCV.2017.322},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2017 - Mask R-CNN.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Mask R-CNN{\_}He et al.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
month = {oct},
pages = {2980--2988},
publisher = {IEEE},
title = {{Mask R-CNN}},
url = {http://ieeexplore.ieee.org/document/8237584/},
volume = {2017-Octob},
year = {2017}
}
@article{Lee2014a,
abstract = {Ultrathin piezoelectric nanogenerator (NG) with a total thickness of ≈16 $\mu$m is fabricated as an active or self-powered sensor for monitoring local deformation on a human skin. The NG was based on an anodic aluminum oxide (AAO) as an insulating layer grown on a thin Al foil by anodization, on which a thin film made of aligned ZnO nanowire compacted arrays is grown by solution chemistry. The performance of the NG is characterized with the assistance of the finite element method (FEM) simulation. The extremely thin NG is attached on the surface of an eyelid, and its output voltage/current characterizes the motion of the eye ball underneath. Since there is no external power needed for the operation of the NG, this self-powered or active sensor can be effective in monitoring sleeping behavior, brain activities, and spirit status of a person as well as any biological associated skin deformation},
author = {Lee, Sangmin and Hinchet, Ronan and Lee, Yean and Yang, Ya and Lin, Zong-hong Hong and Ardila, Gustavo and Mont{\`{e}}s, Laurent and Mouis, Mireille and Wang, Zhong Lin},
doi = {10.1002/adfm.201301971},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2014{\_}Ultrathin nanogenerators as self-poweredactive skin sensors for tracking eye ball motion{\_}Lee et al.pdf:pdf},
issn = {1616301X},
journal = {Advanced Functional Materials},
keywords = {active sensors,eye ball motion,self-powered systems,ultrathin nanogenerators},
number = {8},
pages = {1163--1168},
title = {{Ultrathin nanogenerators as self-powered/active skin sensors for tracking eye ball motion}},
volume = {24},
year = {2014}
}
@article{Jang2018a,
abstract = {Well structured visual representations can make robot learning faster and can improve generalization. In this paper, we study how we can acquire effective object-centric representations for robotic manipulation tasks without human labeling by using autonomous robot interaction with the environment. Such representation learning methods can benefit from continuous refinement of the representation as the robot collects more experience, allowing them to scale effectively without human intervention. Our representation learning approach is based on object persistence: when a robot removes an object from a scene, the representation of that scene should change according to the features of the object that was removed. We formulate an arithmetic relationship between feature vectors from this observation, and use it to learn a representation of scenes and objects that can then be used to identify object instances, localize them in the scene, and perform goal-directed grasping tasks where the robot must retrieve commanded objects from a bin. The same grasping procedure can also be used to automatically collect training data for our method, by recording images of scenes, grasping and removing an object, and recording the outcome. Our experiments demonstrate that this self-supervised approach for tasked grasping substantially outperforms direct reinforcement learning from images and prior representation learning methods.},
archivePrefix = {arXiv},
arxivId = {1811.06964},
author = {Jang, Eric and Devin, Coline and Vanhoucke, Vincent and Levine, Sergey},
eprint = {1811.06964},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Jang et al. - 2018 - Grasp2Vec Learning Object Representations from Self-Supervised Grasping(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Grasp2Vec Learning Object Representations from Self-Supervised Grasping{\_}Jang et al.pdf:pdf},
keywords = {instance grasping,reinforcement learning,unsupervised learning},
number = {CoRL},
pages = {1--15},
title = {{Grasp2Vec: Learning Object Representations from Self-Supervised Grasping}},
url = {http://arxiv.org/abs/1811.06964},
year = {2018}
}
@article{Hausknecht2015,
abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting $\backslash$textit{\{}Deep Recurrent Q-Network{\}} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
archivePrefix = {arXiv},
arxivId = {1507.06527},
author = {Hausknecht, Matthew and Stone, Peter},
eprint = {1507.06527},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}Deep Recurrent Q-Learning for Partially Observable MDPs{\_}Hausknecht, Stone.pdf:pdf},
keywords = {Technical Report FS-15-06},
pages = {29--37},
title = {{Deep Recurrent Q-Learning for Partially Observable MDPs}},
url = {http://arxiv.org/abs/1507.06527},
year = {2015}
}
@article{Matas2018a,
abstract = {We have seen much recent progress in rigid object manipulation, but interaction with deformable objects has notably lagged behind. Due to the large configuration space of deformable objects, solutions using traditional modelling approaches require significant engineering work. Perhaps then, bypassing the need for explicit modelling and instead learning the control in an end-to-end manner serves as a better approach? Despite the growing interest in the use of end-to-end robot learning approaches, only a small amount of work has focused on their applicability to deformable object manipulation. Moreover, due to the large amount of data needed to learn these end-to-end solutions, an emerging trend is to learn control policies in simulation and then transfer them over to the real world. To-date, no work has explored whether it is possible to learn and transfer deformable object policies. We believe that if sim-to-real methods are to be employed further, then it should be possible to learn to interact with a wide variety of objects, and not only rigid objects. In this work, we use a combination of state-of-the-art deep reinforcement learning algorithms to solve the problem of manipulating deformable objects (specifically cloth). We evaluate our approach on three tasks --- folding a towel up to a mark, folding a face towel diagonally, and draping a piece of cloth over a hanger. Our agents are fully trained in simulation with domain randomisation, and then successfully deployed in the real world without having seen any real deformable objects.},
archivePrefix = {arXiv},
arxivId = {1806.07851},
author = {Matas, Jan and James, Stephen and Davison, Andrew J.},
eprint = {1806.07851},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Matas, James, Davison - 2018 - Sim-to-Real Reinforcement Learning for Deformable Object Manipulation.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Sim-to-Real Reinforcement Learning for Deformable Object Manipulation{\_}Matas, James, Davison.pdf:pdf},
keywords = {deformable objects,manipulation,reinforcement learning},
number = {CoRL},
title = {{Sim-to-Real Reinforcement Learning for Deformable Object Manipulation}},
url = {http://arxiv.org/abs/1806.07851},
year = {2018}
}
@article{Li2018,
abstract = {For the functional control of prosthetic hand, it is insufficient to obtain only the motion pattern information. As far as practicality is concerned, the control of the prosthetic hand force is indispensable. The application value of prosthetic hand will be greatly improved if the stable grip of prosthetic hand can be achieved. To address this problem, in this study, a bio-signal control method for grasping control of a prosthetic hand is proposed to improve patient's sense of using prosthetic hand and the thus improving the quality of life. A MYO gesture control armband is used to collect the surface electromyographic (sEMG) signals from the upper limb. The overlapping sliding window scheme are applied for data segmentation and the correlated features are extracted from each segmented data. Principal component analysis (PCA) methods are then deployed for dimension reduction. Deep neural network is used to generate sEMG-force regression model for force prediction at different levels. The predicted force values are input to a fuzzy controller for the grasping control of a prosthetic hand. A vibration feedback device is used to feed grasping force value back to patient's arm to improve patient's sense of using prosthetic hand and realize accurate grasping. To test the effectiveness of the scheme, 15 able-bodied subjects participated in the experiments. The classification results indicated that 8-channel sEMG applying all four time-domain features, with PCA reduction from 32 to 8 dimensions results in the highest classification accuracy. Based on the experimental results from 15 participants, the average recognition rate is over 95{\%}. On the other hand, from the statistical results of standard deviation, the between-subject variations ranges from 3.58 to 1.25{\%}, proving that the robustness and stability of the proposed approach. The method proposed hereto control grasping power through the patient's own sEMG signal, which achieves a high recognition rate to improve the success rate of grip and increases the sense of operation and also brings the gospel for upper extremity amputation patients.},
author = {Li, Chuanjiang and Ren, Jian and Huang, Huaiqi and Wang, Bin and Zhu, Yanfei and Hu, Huosheng},
doi = {10.1186/s12938-018-0539-8},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}PCA and deep learning based myoelectric grasping control of a prosthetic hand{\_}Li et al.pdf:pdf},
issn = {1475925X},
journal = {BioMedical Engineering Online},
keywords = {DNN,Fuzzy controller,Grasp control,PCA,Prosthetic hand,SEMG-force,Vibration feedback device},
number = {1},
pages = {1--18},
publisher = {BioMed Central},
title = {{PCA and deep learning based myoelectric grasping control of a prosthetic hand}},
url = {https://doi.org/10.1186/s12938-018-0539-8},
volume = {17},
year = {2018}
}
@article{Karambakhsh2019,
abstract = {Augmented reality is very useful in medical education because of the problem of having body organs in a regular classroom. In this paper, we propose to apply augmented reality to improve the way of teaching in medical schools and institutes. We propose a novel convolutional neural network (CNN) for gesture recognition, which recognizes the human's gestures as a certain instruction. We use augmented reality technology for anatomy learning, which simulates the scenarios where students can learn Anatomy with HoloLens instead of rare specimens. We have used the mesh reconstruction to reconstruct the 3D specimens. A user interface featured augment reality has been designed which fits the common process of anatomy learning. To improve the interaction services, we have applied gestures as an input source and improve the accuracy of gestures recognition by an updated deep convolutional neural network. Our proposed learning method includes many separated train procedures using cloud computing. Each train model and its related inputs have been sent to our cloud and the results are returned to the server. The suggested cloud includes windows and android devices, which are able to install deep convolutional learning libraries. Compared with previous gesture recognition, our approach is not only more accurate but also has more potential for adding new gestures. Furthermore, we have shown that neural networks can be combined with augmented reality as a rising field, and the great potential of augmented reality and neural networks to be employed for medical learning and education systems.},
author = {Karambakhsh, Ahmad and Kamel, Aouaidjia and Sheng, Bin and Li, Ping and Yang, Po and Dagan, David and Feng, David Dagan},
doi = {10.1016/j.ijinfomgt.2018.03.004},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Karambakhsh et al. - 2019 - Deep gesture interaction for augmented anatomy learning(2).pdf:pdf},
issn = {02684012},
journal = {International Journal of Information Management},
keywords = {3D reconstruction,Augmented reality,Medical education,Mobile cloud,Neural network},
pages = {328--336},
publisher = {Elsevier},
title = {{Deep gesture interaction for augmented anatomy learning}},
url = {https://doi.org/10.1016/j.ijinfomgt.2018.03.004},
volume = {45},
year = {2018}
}
@article{Hafner2018a,
abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this problem using a latent dynamics model with both deterministic and stochastic transition components and a multi-step variational inference objective that we call latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
archivePrefix = {arXiv},
arxivId = {1811.04551},
author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
eprint = {1811.04551},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Learning Latent Dynamics for Planning from Pixels{\_}Hafner et al.pdf:pdf},
month = {nov},
title = {{Learning Latent Dynamics for Planning from Pixels}},
url = {http://arxiv.org/abs/1811.04551},
year = {2018}
}
@article{Lample2016,
abstract = {Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios.},
archivePrefix = {arXiv},
arxivId = {1609.05521},
author = {Lample, Guillaume and Chaplot, Devendra Singh},
eprint = {1609.05521},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Playing FPS Games with Deep Reinforcement Learning{\_}Lample, Chaplot.pdf:pdf},
keywords = {Machine Learning Methods},
pages = {2140--2146},
title = {{Playing FPS Games with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1609.05521},
year = {2016}
}
@article{Wang2018,
abstract = {Although cost-effective at-home blood pressure monitors are available, a complementary mobile solution can ease the bur-den of measuring BP at critical points throughout the day. In this work, we developed and evaluated a smartphone-based BP monitoring application called Seismo. The technique re-lies on measuring the time between the opening of the aortic valve and the pulse later reaching a periphery arterial site. It uses the smartphone's accelerometer to measure the vibration caused by the heart valve movements and the smartphone's camera to measure the pulse at the fingertip. The system was evaluated in a nine participant longitudinal BP perturbation study. Each participant participated in four sessions that in-volved stationary biking at multiple intensities. The Pearson correlation coefficient of the blood pressure estimation across participants is 0.20-0.77 (µ=0.55, $\sigma$ =0.19), with an RMSE of 3.3-9.2 mmHg (µ=5.2, $\sigma$ =2.0).},
author = {Wang, Edward Jay and Zhu, Junyi and Jain, Mohit and Lee, Tien-jui and Saba, Elliot and Nachman, Lama and Patel, Shwetak N},
doi = {10.1145/3173574.3173999},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Seismo Blood Pressure Monitoring using Built-in Smartphone Accelerometer and Camera{\_}Wang et al.pdf:pdf},
isbn = {9781450356206},
journal = {Chi '18},
keywords = {Applications Author Keywords Physiological sensing,I54 Image Processing and Computer Vision,Miscellaneous,PPG,PTT,SCG,noninvasive blood pressure,photoplethysmography,pulse transit time,seismocardiography},
pages = {425},
title = {{Seismo: Blood Pressure Monitoring using Built-in Smartphone Accelerometer and Camera}},
url = {https://dl.acm.org/citation.cfm?id=3173999},
year = {2018}
}
@article{Zhang2019,
abstract = {In this paper, we deal with the reality gap from a novel perspective, targeting transferring Deep Reinforcement Learning (DRL) policies learned in simulated environments to the real-world domain for visual control tasks. Instead of adopting the common solutions to the problem by increasing the visual fidelity of synthetic images output from simulators during the training phase, we seek to tackle the problem by translating the real-world image streams back to the synthetic domain during the deployment phase, to make the robot feel at home. We propose this as a lightweight, flexible, and efficient solution for visual control, as 1) no extra transfer steps are required during the expensive training of DRL agents in simulation; 2) the trained DRL agents will not be constrained to being deployable in only one specific real-world environment; 3) the policy training and the transfer operations are decoupled, and can be conducted in parallel. Besides this, we propose a simple yet effective shift loss that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs. We validate the shift loss for artistic style transfer for videos and domain adaptation, and validate our visual control approach in indoor and outdoor robotics experiments.},
archivePrefix = {arXiv},
arxivId = {1802.00265},
author = {Zhang, Jingwei and Tai, Lei and Yun, Peng and Xiong, Yufeng and Liu, Ming and Boedecker, Joschka and Burgard, Wolfram},
doi = {10.1109/LRA.2019.2894216},
eprint = {1802.00265},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2018 - VR-Goggles for Robots Real-to-sim Domain Adaptation for Visual Control.pdf:pdf;:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2019 - VR-Goggles for Robots Real-to-Sim Domain Adaptation for Visual Control.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Deep learning in robotics and automation,model learning for control,visual-based navigation},
number = {2},
pages = {1148--1155},
publisher = {IEEE},
title = {{VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control}},
url = {http://arxiv.org/abs/1802.00265},
volume = {4},
year = {2019}
}
@article{Silver2017a,
abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {Van Den Driessche}, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2017{\_}Mastering the game of Go without human knowledge{\_}Silver et al.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7676},
pages = {354--359},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go without human knowledge}},
url = {http://dx.doi.org/10.1038/nature24270},
volume = {550},
year = {2017}
}
@article{Isola2017b,
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
author = {Isola, Phillip and Zhu, Jun Yan and Zhou, Tinghui and Efros, Alexei A.},
doi = {10.1109/CVPR.2017.632},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Isola et al. - 2017 - Image-to-image translation with conditional adversarial networks.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Image-to-image translation with conditional adversarial networks{\_}Isola et al.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {5967--5976},
title = {{Image-to-image translation with conditional adversarial networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Silver2017,
abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
archivePrefix = {arXiv},
arxivId = {1712.01815},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
eprint = {1712.01815},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm{\_}Silver et al.pdf:pdf},
pages = {1--19},
title = {{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}},
url = {http://arxiv.org/abs/1712.01815},
year = {2017}
}
@article{Redmon2015a,
abstract = {We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.},
author = {Redmon, Joseph and Angelova, Anelia},
doi = {10.1109/ICRA.2015.7139361},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Redmon, Angelova - 2015 - Real-time grasp detection using convolutional neural networks.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}Real-time grasp detection using convolutional neural networks{\_}Redmon, Angelova.pdf:pdf},
isbn = {9781479969234},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
month = {may},
number = {June},
pages = {1316--1322},
publisher = {IEEE},
title = {{Real-time grasp detection using convolutional neural networks}},
url = {http://ieeexplore.ieee.org/document/7139361/},
volume = {2015-June},
year = {2015}
}
@article{Mudigonda2017,
abstract = {Grasping objects with high dimensional controllers such as an anthropomorphic hand using reinforcement learning is a challenging problem. In this work we ex- periment with a 16-D simulated version of a prosthetic hand developed for South- Hampton Hand Assessment Procedure (SHAP). To the best of our knowledge, we provide the first demonstration of learning successful grasping policies for an an- thropomorphic hand from scratch using deep reinforcement learning. We find that our grasping model is robust to sensor noise, variations in object shape, position of the object and physical parameters such as the density of the object. We also test some hypotheses about the utility of touch sensing for grasping objects. We believe that our results and analysis provide useful insights and strong baselines for future research into the very exciting direction of object manipulation with anthropomorphic hands using proprioceptive and sensory feedback. 1},
author = {Mudigonda, Mayur and Agrawal, Pulkit and Deweese, Michael R and Malik, Jitendra},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2017{\_}Investigating Deep Reinforcement Learning for Grasping Objects with an Anthropomorphic Hand{\_}Mudigonda et al.pdf:pdf},
journal = {Deep Reinforcement Learning Symposium, NIPS 2017},
number = {25},
pages = {1--5},
title = {{Investigating Deep Reinforcement Learning for Grasping Objects with an Anthropomorphic Hand}},
year = {2017}
}
@article{Gu2017,
abstract = {Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on offpolicy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efﬁciently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
doi = {10.1109/ICRA.2017.7989385},
eprint = {1610.00633},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2017{\_}Deep Reinforcement Learning for Robotic Manipulation{\_}Gu et al.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Deep Reinforcement Learning for Robotic Manipulation{\_}Gu et al.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Learning and Adaptive Systems},
pages = {3389--3396},
pmid = {26774160},
title = {{Deep Reinforcement Learning for Robotic Manipulation}},
year = {2017}
}
@article{Chen2019a,
abstract = {As a sub-domain of text-to-image synthesis, text-to-face generation has huge potentials in public safety domain. With lack of dataset, there are almost no related research focusing on text-to-face synthesis. In this paper, we propose a fully-trained Generative Adversarial Network (FTGAN) that trains the text encoder and image decoder at the same time for fine-grained text-to-face generation. With a novel fully-trained generative network, FTGAN can synthesize higher-quality images and urge the outputs of the FTGAN are more relevant to the input sentences. In addition, we build a dataset called SCU-Text2face for text-to-face synthesis. Through extensive experiments, the FTGAN shows its superiority in boosting both generated images' quality and similarity to the input descriptions. The proposed FTGAN outperforms the previous state of the art, boosting the best reported Inception Score to 4.63 on the CUB dataset. On SCU-text2face, the face images generated by our proposed FTGAN just based on the input descriptions is of average 59{\%} similarity to the ground-truth, which set a baseline for text-to-face synthesis.},
archivePrefix = {arXiv},
arxivId = {1904.05729},
author = {Chen, Xiang and Qing, Lingbo and He, Xiaohai and Luo, Xiaodong and Xu, Yining},
eprint = {1904.05729},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2019 - FTGAN A Fully-trained Generative Adversarial Networks for Text to Face Generation.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}FTGAN A Fully-trained Generative Adversarial Networks for Text to Face Generation{\_}Chen et al.pdf:pdf},
isbn = {2016222050},
month = {apr},
number = {1},
title = {{FTGAN: A Fully-trained Generative Adversarial Networks for Text to Face Generation}},
url = {http://arxiv.org/abs/1904.05729},
year = {2019}
}
@article{Jaderberg2018,
abstract = {Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.},
archivePrefix = {arXiv},
arxivId = {1807.01281},
author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
eprint = {1807.01281},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Jaderberg et al. - 2018 - Human-level performance in first-person multiplayer games with population-based deep reinforcement learning(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Human-level performance in first-person multiplayer games with population-based deep reinforcement learning{\_}Jaderberg et al.pdf:pdf},
title = {{Human-level performance in first-person multiplayer games with population-based deep reinforcement learning}},
url = {http://arxiv.org/abs/1807.01281},
year = {2018}
}
@article{Kalashnikov2018,
abstract = {In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96{\%} grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.},
archivePrefix = {arXiv},
arxivId = {1806.10293},
author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and Levine, Sergey},
eprint = {1806.10293},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}QT-Opt Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation{\_}Kalashnikov et al.pdf:pdf},
keywords = {deep learning,grasping,reinforcement learning},
number = {CoRL},
pages = {1--23},
title = {{QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation}},
url = {http://arxiv.org/abs/1806.10293},
year = {2018}
}
@article{Nanayakkara2013a,
abstract = {Finger-worn interfaces remain a vastly unexplored space for user interfaces, despite the fact that our fingers and hands are naturally used for referencing and interacting with the environment. In this paper we present design guidelines and implementation of a finger-worn I/O device, the EyeRing, which leverages the universal and natural gesture of pointing. We present use cases of EyeRing for both visually impaired and sighted people. We discuss initial reactions from visually impaired users which suggest that EyeRing may indeed offer a more seamless solution for dealing with their immediate surroundings than the solutions they currently use. We also report on a user study that demonstrates how EyeRing reduces effort and disruption to a sighted user. We conclude that this highly promising form factor offers both audiences enhanced, seamless interaction with information related to objects in the environment.},
author = {Nanayakkara, Suranga and Shilkrot, Roy and Yeo, Kian Peen and Maes, Pattie},
doi = {10.1145/2459236.2459240},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2013{\_}EyeRing a finger-worn input device for seamless interactions with our surroundings{\_}Nanayakkara et al.pdf:pdf},
isbn = {9781450319041},
issn = {1133-2654},
journal = {AH '13: Proceedings of the 4th Augmented Human International Conference},
pages = {13--20},
title = {{EyeRing: a finger-worn input device for seamless interactions with our surroundings}},
url = {http://dl.acm.org/citation.cfm?doid=2459236.2459240{\%}5Cnpapers3://publication/doi/10.1145/2459236.2459240},
year = {2013}
}
@article{Lopes2018,
abstract = {We present a mo bile system that enhances mixed r eality experiences and games with force feedback by means of electrical muscle stimulation (EMS). The benefit of our approach is that it adds physical forces while keep ing the users' hands free to interact unencumbered — not only with virtual objects, but also with physical o bjects, such as props and appliances. We demonstrate how this supports three classes of applications along the mixed - reality continuum: (1) enti rely virtual objects, such as furniture with EMS fri c tion when pushed or an EMS - based catapult game. (2) V irtual objects augmented via passive props with EMS - constraints, such as a light control panel made tan g ible by means of a physical cup or a bal ance - the - marble game with an actuated tray. (3) A ugment ed appliances with virtual behaviors, such as a physical t hermo stat dial with EMS - detents or an escape - r oom that repurposes lamps as levers with detents . We present a use r - study in which participants rated the EMS - feedback as significantly more realistic than a no - EMS baseline.},
author = {Lopes, Pedro and You, Sijing and Ion, Alexandra and Baudisch, Patrick},
doi = {10.1145/3173574.3174020},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Lopes et al. - 2018 - Adding Force Feedback to Mixed Reality Experiences and Games using Electrical Muscle Stimulation(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Adding Force Feedback to Mixed Reality Experiences and Games using Electrical Muscle Stimulation{\_}Lopes et al.pdf:pdf},
isbn = {9781450356206},
number = {c},
pages = {1--13},
title = {{Adding Force Feedback to Mixed Reality Experiences and Games using Electrical Muscle Stimulation}},
year = {2018}
}
@article{Zeng2019,
abstract = {We investigate whether a robot arm can learn to pick and throw arbitrary objects into selected boxes quickly and accurately. Throwing has the potential to increase the physical reachability and picking speed of a robot arm. However, precisely throwing arbitrary objects in unstructured settings presents many challenges: from acquiring reliable pre-throw conditions (e.g. initial pose of object in manipulator) to handling varying object-centric properties (e.g. mass distribution, friction, shape) and dynamics (e.g. aerodynamics). In this work, we propose an end-to-end formulation that jointly learns to infer control parameters for grasping and throwing motion primitives from visual observations (images of arbitrary objects in a bin) through trial and error. Within this formulation, we investigate the synergies between grasping and throwing (i.e., learning grasps that enable more accurate throws) and between simulation and deep learning (i.e., using deep networks to predict residuals on top of control parameters predicted by a physics simulator). The resulting system, TossingBot, is able to grasp and throw arbitrary objects into boxes located outside its maximum reach range at 500+ mean picks per hour (600+ grasps per hour with 85{\%} throwing accuracy); and generalizes to new objects and target locations. Videos are available at https://tossingbot.cs.princeton.edu},
archivePrefix = {arXiv},
arxivId = {1903.11239},
author = {Zeng, Andy and Song, Shuran and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
eprint = {1903.11239},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Zeng et al. - 2019 - TossingBot Learning to Throw Arbitrary Objects with Residual Physics.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}TossingBot Learning to Throw Arbitrary Objects with Residual Physics{\_}Zeng et al.pdf:pdf},
month = {mar},
title = {{TossingBot: Learning to Throw Arbitrary Objects with Residual Physics}},
url = {http://arxiv.org/abs/1903.11239 https://ai.googleblog.com/2019/03/unifying-physics-and-deep-learning-with.html},
year = {2019}
}
@article{Harwath2017,
abstract = {Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the word 'lighthouse' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.},
archivePrefix = {arXiv},
arxivId = {1701.07481},
author = {Harwath, David and Glass, James R.},
eprint = {1701.07481},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Harwath, Glass - 2017 - Learning Word-Like Units from Joint Audio-Visual Analysis(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Learning Word-Like Units from Joint Audio-Visual Analysis{\_}Harwath, Glass.pdf:pdf},
title = {{Learning Word-Like Units from Joint Audio-Visual Analysis}},
url = {http://arxiv.org/abs/1701.07481},
year = {2017}
}
@article{Wang2015a,
abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
archivePrefix = {arXiv},
arxivId = {1511.06581},
author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
eprint = {1511.06581},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2015 - Dueling Network Architectures for Deep Reinforcement Learning(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}Dueling Network Architectures for Deep Reinforcement Learning{\_}Wang et al.pdf:pdf},
number = {9},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1511.06581},
year = {2015}
}
@article{Hilliges2012,
abstract = {Abstract HoloDesk is an interactive system combining an optical see through display and Kinect camera to create the illusion that users are directly interacting with 3D graphics. A virtual image of a 3D scene is rendered through a half silvered mirror and spatially aligned ... $\backslash$n},
author = {Hilliges, Otmar and Kim, David and Izadi, Shahram and Weiss, Malte and Wilson, Andrew},
doi = {10.1145/2207676.2208405},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Hilliges et al. - 2012 - HoloDesk Direct 3D Interactions with a Situated See-Through Display Otmar(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2012{\_}HoloDesk Direct 3D Interactions with a Situated See-Through Display Otmar{\_}Hilliges et al.pdf:pdf},
isbn = {9781450310154},
journal = {Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems - CHI '12},
pages = {2421},
title = {{HoloDesk: Direct 3D Interactions with a Situated See-Through Display Otmar}},
url = {http://dl.acm.org/citation.cfm?doid=2207676.2208405},
year = {2012}
}
@article{Yan2019a,
abstract = {Many previous works approach vision-based robotic grasping by training a value network that evaluates grasp proposals. These approaches require an optimization process at run-time to infer the best action from the value network. As a result, the inference time grows exponentially as the dimension of action space increases. We propose an alternative method, by directly training a neural density model to approximate the conditional distribution of successful grasp poses from the input images. We construct a neural network that combines Gaussian mixture and normalizing flows, which is able to represent multi-modal, complex probability distributions. We demonstrate on both simulation and real robot that the proposed actor model achieves similar performance compared to the value network using the Cross-Entropy Method (CEM) for inference, on top-down grasping with a 4 dimensional action space. Our actor model reduces the inference time by 3 times compared to the state-of-the-art CEM method. We believe that actor models will play an important role when scaling up these approaches to higher dimensional action spaces.},
archivePrefix = {arXiv},
arxivId = {1904.07319},
author = {Yan, Mengyuan and Li, Adrian and Kalakrishnan, Mrinal and Pastor, Peter},
eprint = {1904.07319},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Yan et al. - 2019 - Learning Probabilistic Multi-Modal Actor Models for Vision-Based Robotic Grasping.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Learning Probabilistic Multi-Modal Actor Models for Vision-Based Robotic Grasping{\_}Yan et al.pdf:pdf},
month = {apr},
title = {{Learning Probabilistic Multi-Modal Actor Models for Vision-Based Robotic Grasping}},
url = {http://arxiv.org/abs/1904.07319},
year = {2019}
}
@article{Kajimoto2004,
author = {Kajimoto, Hiroyuki and Kawakami, Naoki and Tachi, Susumu and Inami, Masahiko},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2004{\_}SmartTouch Electric Skin to Touch the{\_}Kajimoto et al.pdf:pdf},
journal = {Image (Rochester, N.Y.)},
number = {February},
pages = {36--43},
title = {{SmartTouch : Electric Skin to Touch the}},
year = {2004}
}
@article{Erat2018,
abstract = {Drones allow exploring dangerous or impassable areas safely from a distant point of view. However, flight control from an egocentric view in narrow or constrained environments can be challenging. Arguably, an exocentric view would afford a better overview and, thus, more intuitive flight control of the drone. Unfortunately, such an exocentric view is unavailable when exploring indoor environments. This paper investigates the potential of drone-augmented human vision, i.e., of exploring the environment and controlling the drone indirectly from an exocentric viewpoint. If used with a see-through display, this approach can simulate X-ray vision to provide a natural view into an otherwise occluded environment. The user's view is synthesized from a three-dimensional reconstruction of the indoor environment using image-based rendering. This user interface is designed to reduce the cognitive load of the drone's flight control. The user can concentrate on the exploration of the inaccessible space, while flight control is largely delegated to the drone's autopilot system. We assess our system with a first experiment showing how drone-augmented human vision supports spatial understanding and improves natural interaction with the drone. IEEE},
author = {Erat, Okan and Isop, Werner Alexander and Kalkofen, Denis and Schmalstieg, Dieter},
doi = {10.1109/TVCG.2018.2794058},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Erat et al. - 2018 - Drone-Augmented human vision Exocentric control for drones exploring hidden areas(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Drone-Augmented human vision Exocentric control for drones exploring hidden areas{\_}Erat et al.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Drone,Hololens,Mixed reality,Pick-And-place,X-ray},
number = {4},
pages = {1437--1446},
title = {{Drone-Augmented human vision: Exocentric control for drones exploring hidden areas}},
volume = {24},
year = {2018}
}
@article{Polydoros2017a,
abstract = {{\textcopyright} 2017 Springer Science+Business Media DordrechtReinforcement learning is an appealing approach for allowing robots to learn new tasks. Relevant literature reveals a plethora of methods, but at the same time makes clear the lack of implementations for dealing with real life challenges. Current expectations raise the demand for adaptable robots. We argue that, by employing model-based reinforcement learning, the—now limited—adaptability characteristics of robotic systems can be expanded. Also, model-based reinforcement learning exhibits advantages that makes it more applicable to real life use-cases compared to model-free methods. Thus, in this survey, model-based methods that have been applied in robotics are covered. We categorize them based on the derivation of an optimal policy, the definition of the returns function, the type of the transition model and the learned task. Finally, we discuss the applicability of model-based reinforcement learning approaches in new applications, taking into consideration the state of the art in both algorithms and hardware.},
author = {Polydoros, Athanasios S. and Nalpantidis, Lazaros},
doi = {10.1007/s10846-017-0468-y},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Polydoros, Nalpantidis - 2017 - Survey of Model-Based Reinforcement Learning Applications on Robotics.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Survey of Model-Based Reinforcement Learning Applications on Robotics{\_}Polydoros, Nalpantidis.pdf:pdf},
issn = {15730409},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Intelligent robotics,Machine learning,Model-based reinforcement learning,Policy search,Reward functions,Robot learning,Transition models},
month = {may},
number = {2},
pages = {153--173},
publisher = {Springer Netherlands},
title = {{Survey of Model-Based Reinforcement Learning: Applications on Robotics}},
url = {http://link.springer.com/10.1007/s10846-017-0468-y},
volume = {86},
year = {2017}
}
@article{Mnih2015a,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2015{\_}Human-level control through deep reinforcement learning.{\_}Mnih et al.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
number = {7540},
pages = {529--33},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25719670},
volume = {518},
year = {2015}
}
@article{Bhagat2019a,
abstract = {The increasing trend of studying the innate softness of robotic structures and amalgamating it with the benefits of the extensive developments in the field of embodied intelligence has led to the sprouting of a relatively new yet rewarding sphere of technology in intelligent soft robotics. The fusion of deep reinforcement algorithms with soft bio-inspired structures positively directs to a fruitful prospect of designing completely self-sufficient agents that are capable of learning from observations collected from their environment. For soft robotic structures possessing countless degrees of freedom, it is at times not convenient to formulate mathematical models necessary for training a deep reinforcement learning (DRL) agent. Deploying current imitation learning algorithms on soft robotic systems has provided competent results. This review article posits an overview of various such algorithms along with instances of being applied to real-world scenarios, yielding frontier results. Brief descriptions highlight the various pristine branches of DRL research in soft robotics.},
author = {Bhagat, Sarthak and Banerjee, Hritwick and {Ho Tse}, Zion and Ren, Hongliang},
doi = {10.3390/robotics8010004},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Bhagat et al. - 2019 - Deep Reinforcement Learning for Soft, Flexible Robots Brief Review with Impending Challenges.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Deep Reinforcement Learning for Soft, Flexible Robots Brief Review with Impending Challenges{\_}Bhagat et al.pdf:pdf},
journal = {Robotics},
number = {1},
pages = {4},
title = {{Deep Reinforcement Learning for Soft, Flexible Robots: Brief Review with Impending Challenges}},
volume = {8},
year = {2019}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2012{\_}ImageNet Classification with Deep Convolutional Neural Networks{\_}Krizhevsky, Sutskever, Hinton.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2012{\_}ImageNet Classification with Deep Convolutional Neural Networks{\_}Krizhevsky, Sutskever, Hinton.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-w{\%}5Cnpapers3://publication/uuid/1ECF396A-CEDA-45CD-9A9F-03344449DA2A},
year = {2012}
}
@article{Ha2018,
abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper at https://worldmodels.github.io},
archivePrefix = {arXiv},
arxivId = {1809.01999},
author = {Ha, David and Schmidhuber, J{\"{u}}rgen},
eprint = {1809.01999},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Recurrent World Models Facilitate Policy Evolution{\_}Ha, Schmidhuber.pdf:pdf},
month = {sep},
number = {C},
title = {{Recurrent World Models Facilitate Policy Evolution}},
url = {http://arxiv.org/abs/1809.01999},
year = {2018}
}
@article{Levine2018b,
abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.02199v4},
author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
doi = {10.1177/0278364917710318},
eprint = {arXiv:1603.02199v4},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection{\_}Levine et al(6).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection{\_}Levine et al.{\_}Internation{\_}ijrr.pdf:pdf;:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Levine et al. - 2018 - Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection(7).pdf:pdf;:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Levine et al. - 2018 - Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection(8).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection{\_}Levine et al.pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Robotics,deep learning,neural networks},
number = {4-5},
pages = {421--436},
title = {{Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection}},
volume = {37},
year = {2018}
}
@article{OpenAI2018b,
abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM},
archivePrefix = {arXiv},
arxivId = {1808.00177},
author = {OpenAI and Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
eprint = {1808.00177},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Learning Dexterous In-Hand Manipulation{\_}OpenAI et al.pdf:pdf;:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/OpenAI et al. - 2018 - Learning Dexterous In-Hand Manipulation.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Learning Dexterous In-Hand Manipulation{\_}OpenAI et al.pdf:pdf},
pages = {1--27},
title = {{Learning Dexterous In-Hand Manipulation}},
url = {http://arxiv.org/abs/1808.00177},
year = {2018}
}
@article{Tuceryan2002a,
abstract = {Augmented reality (AR) is a technology in which a user's view of the real world is enhanced or augmented with additional information generated from a computer model. To have a working AR system, the see-through display system must be calibrated so that the graphics are properly rendered. The optical see-through systems present an additional challenge because, unlike the video see-through systems, we do not have direct access to the image data to be used in various calibration procedures: This paper reports on a calibration method we developed for optical see-through head-mounted displays. We first introduce a method for calibrating monocular optical see-through displays (that is, a display for one eye only) and then extend it to stereo optical see-through displays in which the displays for both eyes are calibrated in a single procedure. The method integrates the measurements for the camera and a six-degrees-of-freedom tracker that is attached to the camera to do the calibration. We have used both an off-the-shelf magnetic tracker as well as a vision-based infrared tracker we have built In the monocular case, the calibration is based on the alignment of image points with a single 3D point in the world coordinate system from various viewpoints. In this method, the user interaction to perform the calibration is extremely easy compared to prior methods, and there is no requirement for keeping the head immobile while performing the calibration. In the stereo calibration case, the user aligns a stereoscopically fused 2D marker, which is perceived in depth, with a single target point in the world whose coordinates are known. As in the monocular case, there is no requirement that the user keep his or her head fixed.},
author = {Tuceryan, Mihran and Genc, Yakup and Navab, Nassir},
doi = {10.1162/105474602317473213},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Tuceryan, Genc, Navab - 2002 - Single-point active alignment method (SPAAM) for optical see-through HMD calibration for augmented rea(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2002{\_}Single-point active alignment method (SPAAM) for optical see-through HMD calibration for augmented reality{\_}Tuceryan, Genc, Navab.pdf:pdf},
issn = {10547460},
journal = {Presence: Teleoperators and Virtual Environments},
number = {3},
pages = {259--276},
title = {{Single-point active alignment method (SPAAM) for optical see-through HMD calibration for augmented reality}},
volume = {11},
year = {2002}
}
@article{Nishi2017,
abstract = {—The device characteristics of semiconductor lasers have been improved with progress in active layer structures. Carrier confinement dimension plays an important role espe-cially in temperature sensitivity as well as slope efficiency. Three-dimensional carrier confinement to nano-scale semiconductor crystal, known as " quantum dots (QDs) " had been predicted to show ultimately superior device performances. Self-assembly formed InAs QDs grown on GaAs had been intensively promoted in order to achieve QD lasers with superior device performances. Now high-density, high-optical quality QDs have been realized through improved molecular beam epitaxy growths and QD lasers with bet-ter temperature characteristics are in the stage of mass-production for a data-com market. Fabry–Perot type, as well as distributed feedback type QD lasers show quite improved laser characteris-tics. Also, the unique device characteristics of QD lasers opened new application fields such as the use for resource searching by utilizing high-temperature operation such as lasing at higher than 200 °C. For silicon-photonics, QD lasers are used as an optical source for athermal operation. In this paper, the evolution of QDs, as well as improved device performances for novel application fields are discussed.},
author = {Nishi, Kenichi and Takemasa, Keizo and Sugawara, Mitsuru and Arakawa, Yasuhiko},
doi = {10.1109/JSTQE.2017.2699787},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Nishi et al. - 2017 - Development of Quantum Dot Lasers for Data-Com and Silicon Photonics Applications(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Development of Quantum Dot Lasers for Data-Com and Silicon Photonics Applications{\_}Nishi et al.pdf:pdf},
issn = {21910359},
journal = {IEEE Journal of Selected Topics in Quantum Electronics},
keywords = {InAs,high-temperature operation,quantum dot lasers,quantum dots,silicon-photonics},
number = {6},
title = {{Development of Quantum Dot Lasers for Data-Com and Silicon Photonics Applications}},
volume = {23},
year = {2017}
}
@article{Zhou2015a,
abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1{\%} top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2{\%} top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them},
archivePrefix = {arXiv},
arxivId = {1512.04150},
author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
eprint = {1512.04150},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}Learning Deep Features for Discriminative Localization{\_}Zhou et al.pdf:pdf},
pages = {2921--2929},
title = {{Learning Deep Features for Discriminative Localization}},
url = {http://arxiv.org/abs/1512.04150},
year = {2015}
}
@article{Sabour2017,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
archivePrefix = {arXiv},
arxivId = {1710.09829},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
eprint = {1710.09829},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Sabour, Frosst, Hinton - 2017 - Dynamic Routing Between Capsules(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Dynamic Routing Between Capsules{\_}Sabour, Nov, Hinton.pdf:pdf},
number = {Nips},
title = {{Dynamic Routing Between Capsules}},
url = {http://arxiv.org/abs/1710.09829},
year = {2017}
}
@article{Singh2019,
abstract = {The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.},
archivePrefix = {arXiv},
arxivId = {1904.07854},
author = {Singh, Avi and Yang, Larry and Hartikainen, Kristian and Finn, Chelsea and Levine, Sergey},
eprint = {1904.07854},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2019{\_}End-to-End Robotic Reinforcement Learning without Reward Engineering{\_}Singh et al.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}End-to-End Robotic Reinforcement Learning without Reward Engineering{\_}Singh et al.pdf:pdf},
month = {apr},
title = {{End-to-End Robotic Reinforcement Learning without Reward Engineering}},
url = {https://sites.google.com/view/reward-learning-rl/home http://arxiv.org/abs/1904.07854 https://github.com/avisingh599/reward-learning-rl http://mrl.snu.ac.kr/research/ProjectScalable/Page.htm},
year = {2019}
}
@article{Chu2018,
abstract = {Neural networks have recently become good at engaging in dialog. However, current approaches are based solely on verbal text, lacking the richness of a real face-to-face conversation. We propose a neural conversation model that aims to read and generate facial gestures alongside with text. This allows our model to adapt its response based on the "mood" of the conversation. In particular, we introduce an RNN encoder-decoder that exploits the movement of facial muscles, as well as the verbal conversation. The decoder consists of two layers, where the lower layer aims at generating the verbal response and coarse facial expressions, while the second layer fills in the subtle gestures, making the generated output more smooth and natural. We train our neural network by having it "watch" 250 movies. We showcase our joint face-text model in generating more natural conversations through automatic metrics and a human study. We demonstrate an example application with a face-to-face chatting avatar.},
author = {Chu, Hang and Li, Daiqing and Fidler, Sanja},
doi = {10.1109/CVPR.2018.00743},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Chu, Li, Fidler - 2018 - A Face-to-Face Neural Conversation Model(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}A Face-to-Face Neural Conversation Model{\_}Chu, Li, Fidler.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {7113--7121},
title = {{A Face-to-Face Neural Conversation Model}},
year = {2018}
}
@article{Zhang2019,
abstract = {In this paper, we deal with the reality gap from a novel perspective, targeting transferring Deep Reinforcement Learning (DRL) policies learned in simulated environments to the real-world domain for visual control tasks. Instead of adopting the common solutions to the problem by increasing the visual fidelity of synthetic images output from simulators during the training phase, we seek to tackle the problem by translating the real-world image streams back to the synthetic domain during the deployment phase, to make the robot feel at home. We propose this as a lightweight, flexible, and efficient solution for visual control, as 1) no extra transfer steps are required during the expensive training of DRL agents in simulation; 2) the trained DRL agents will not be constrained to being deployable in only one specific real-world environment; 3) the policy training and the transfer operations are decoupled, and can be conducted in parallel. Besides this, we propose a simple yet effective shift loss that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs. We validate the shift loss for artistic style transfer for videos and domain adaptation, and validate our visual control approach in indoor and outdoor robotics experiments.},
author = {Zhang, Jingwei and Tai, Lei and Yun, Peng and Xiong, Yufeng and Liu, Ming and Boedecker, Joschka and Burgard, Wolfram},
doi = {10.1109/LRA.2019.2894216},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2019 - VR-Goggles for Robots Real-to-Sim Domain Adaptation for Visual Control.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Deep learning in robotics and automation,model learning for control,visual-based navigation},
number = {2},
pages = {1148--1155},
title = {{VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control}},
volume = {4},
year = {2019}
}
@article{Hinton2012,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
eprint = {1207.0580},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2012{\_}Improving neural networks by preventing co-adaptation of feature detectors{\_}Hinton et al.pdf:pdf},
isbn = {9781467394673},
pages = {254--256},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@article{Lee2019a,
abstract = {Many anatomical factors, such as bone geometry and muscle condition, interact to affect human movements. This work aims to build a comprehensive musculoskeletal model and its control system that reproduces realistic human movements driven by muscle contraction dynamics. The variations in the anatomic model generate a spectrum of human movements ranging from typical to highly stylistic movements. To do so, we discuss scalable and reliable simulation of anatomical features, robust control of under-actuated dynamical systems based on deep reinforcement learning, and modeling of pose-dependent joint limits. The key technical contribution is a scalable, two-level imitation learning algorithm that can deal with a comprehensive full-body musculoskeletal model with 346 muscles. We demonstrate the predictive simulation of dynamic motor skills under anatomical conditions including bone deformity, muscle weakness, contracture, and the use of a prosthesis. We also simulate various pathological gaits and predictively visualize how orthopedic surgeries improve post-operative gaits.},
author = {Lee, Seunghwan and Lee, Kyoungmin and Park, Moonseok and Lee, Jehee},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2019 - Scalable Muscle-actuated Human Simulation and Control.pdf:pdf;:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2019 - Scalable Muscle-Actuated Human Simulation and Control(2).pdf:pdf},
journal = {ACM Transactions on Graphics},
number = {4},
pages = {ACM Transactions on Graphics},
title = {{Scalable Muscle-Actuated Human Simulation and Control}},
url = {http://mrl.snu.ac.kr/research/ProjectScalable/Preprint.pdf{\%}0Ahttps://www.youtube.com/watch?v=a3jfyJ9JVeM{\&}feature=youtu.be http://mrl.snu.ac.kr/research/ProjectScalable/Page.htm http://mrl.snu.ac.kr/research/ProjectScalable/Preprint.pdf},
volume = {38},
year = {2019}
}
@article{James2018b,
abstract = {Real world data, especially in the domain of robotics, is notoriously costly to collect. One way to circumvent this can be to leverage the power of simulation to produce large amounts of labelled data. However, training models on simulated images does not readily transfer to real-world ones. Using domain adaptation methods to cross this "reality gap" requires a large amount of unlabelled real-world data, whilst domain randomization alone can waste modeling power. In this paper, we present Randomized-to-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no real-world data. Our method learns to translate randomized rendered images into their equivalent non-randomized, canonical versions. This in turn allows for real images to also be translated into canonical sim images. We demonstrate the effectiveness of this sim-to-real approach by training a vision-based closed-loop grasping reinforcement learning agent in simulation, and then transferring it to the real world to attain 70{\%} zero-shot grasp success on unseen objects, a result that almost doubles the success of learning the same task directly on domain randomization alone. Additionally, by joint finetuning in the real-world with only 5,000 real-world grasps, our method achieves 91{\%}, attaining comparable performance to a state-of-the-art system trained with 580,000 real-world grasps, resulting in a reduction of real-world data by more than 99{\%}.},
archivePrefix = {arXiv},
arxivId = {1812.07252},
author = {James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos},
eprint = {1812.07252},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Sim-to-Real via Sim-to-Sim Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks{\_}James et al.pdf:pdf},
title = {{Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks}},
url = {http://arxiv.org/abs/1812.07252},
year = {2018}
}
@article{Gu2016,
archivePrefix = {arXiv},
arxivId = {1603.00748},
author = {Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
doi = {10.3390/robotics2030122},
eprint = {1603.00748},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Gu et al. - 2016 - Continuous deep q-learning with model-based acceleration(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Continuous deep q-learning with model-based acceleration{\_}Gu et al.pdf:pdf},
isbn = {3405062780},
issn = {{\textless}null{\textgreater}},
journal = {International Conference on Machine Learning},
pages = {2829--2838},
pmid = {21487784},
title = {{Continuous deep q-learning with model-based acceleration}},
year = {2016}
}
@article{Harrison2008,
abstract = {We present Scratch Input, an acoustic-based input technique that relies on the unique sound produced when a fingernail is dragged over the surface of a textured material, such as wood, fabric, or wall paint. We employ a simple sensor that can be easily coupled with existing surfaces, such as walls and tables, turning them into large, unpowered and ad hoc finger input surfaces. Our sensor is sufficiently small that it could be incorporated into a mobile device, allowing any suitable surface on which it rests to be appropriated as a gestural input surface. Several example applications were developed to demonstrate possible interactions. We conclude with a study that shows users can perform six Scratch Input gestures at about 90{\%} accuracy with less than five minutes of training and on wide variety of surfaces.},
author = {Harrison, Chris and Hudson, SE Scott E},
doi = {10.1145/1449715.1449747},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Harrison, Hudson - 2008 - Scratch Input Creating Large, Inexpensive, Unpowered and Mobile Finger Input Surfaces(2).pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2008{\_}Scratch Input Creating Large, Inexpensive, Unpowered and Mobile Finger Input Surfaces{\_}Harrison, Hudson.pdf:pdf},
isbn = {978-1-59593-975-3},
journal = {Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology},
keywords = {acoustic sens-,acoustic sensing,ad hoc interaction,finger input,gestures,ing,mobile devices,surfaces},
pages = {205--208},
title = {{Scratch Input: Creating Large, Inexpensive, Unpowered and Mobile Finger Input Surfaces}},
url = {http://doi.acm.org/10.1145/1449715.1449747{\%}5Cnhttp://dl.acm.org/citation.cfm?id=1449747},
year = {2008}
}
@article{CHIN2012a,
author = {CHIN, Takaaki},
doi = {10.2490/jjrmc.49.31},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/CHIN - 2012 - Myoelectric Prostheses Current Status and Problems to be Solved, a Rehabilitation Strategy for Higher Level Amputation (2).pdf:pdf},
issn = {1881-3526},
journal = {The Japanese Journal of Rehabilitation Medicine},
number = {1},
pages = {31--36},
title = {{Myoelectric Prostheses : Current Status and Problems to be Solved, a Rehabilitation Strategy for Higher Level Amputation Patients, and the Future Outlook}},
volume = {49},
year = {2012}
}
@article{Isola2017b,
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
author = {Isola, Phillip and Zhu, Jun Yan and Zhou, Tinghui and Efros, Alexei A.},
doi = {10.1109/CVPR.2017.632},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Image-to-image translation with conditional adversarial networks{\_}Isola et al.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {5967--5976},
title = {{Image-to-image translation with conditional adversarial networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Lenz2013,
abstract = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast, as well as robust, we present a two-step cascaded structure with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs well, for which we present a method to apply structured regularization on the weights based on multimodal group reg-ularization. We demonstrate that our method outperforms the previous state-of-the-art methods in robotic grasp detection, and can be used to successfully execute grasps on a Baxter robot. 1},
author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2013{\_}Deep learning for detecting robotic grasps{\_}Lenz, Lee, Saxena.pdf:pdf},
journal = {Robotics: Science adn Systems2},
title = {{Deep Learning for Detecting Robotic Grasps}},
url = {http://www.roboticsproceedings.org/rss09/p12.pdf},
year = {2013}
}
@article{Hashimoto2013,
abstract = {As concern for Japan's aging population and shrinking workforce continues to mount, many researchers are focusing on nursing and healthcare-related robots to help ease the burden on society. In order to successfully integrate robots into the home environment to coexist with family members, three features are considered essential: safe interaction, a compact and lightweight body, and a simple interface. In this paper the authors introduce a robot that possesses these qualities: the human support robot (HSR). The current prototype HSR is designed to support independent living of persons with limited limb mobility. This paper discusses the development of the HSR and the results of user testing conducted in the homes of two persons with disabilities. ? 2013 IEEE.},
author = {Hashimoto, Kunimatsu and Saito, Fuminori and Yamamoto, Takashi and Ikeda, Koichi},
doi = {10.1109/ARSO.2013.6705520},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2013{\_}A field study of the human support robot in the home environment{\_}Hashimoto et al.pdf:pdf},
isbn = {9781479923694},
issn = {21627568},
journal = {Proceedings of IEEE Workshop on Advanced Robotics and its Social Impacts, ARSO},
pages = {143--150},
publisher = {IEEE},
title = {{A field study of the human support robot in the home environment}},
year = {2013}
}
@article{Fei2019,
abstract = {Insects and hummingbirds exhibit extraordinary flight capabilities and can simultaneously master seemingly conflicting goals: stable hovering and aggressive maneuvering, unmatched by small scale man-made vehicles. Flapping Wing Micro Air Vehicles (FWMAVs) hold great promise for closing this performance gap. However, design and control of such systems remain challenging due to various constraints. Here, we present an open source high fidelity dynamic simulation for FWMAVs to serve as a testbed for the design, optimization and flight control of FWMAVs. For simulation validation, we recreated the hummingbird-scale robot developed in our lab in the simulation. System identification was performed to obtain the model parameters. The force generation, open-loop and closed-loop dynamic response between simulated and experimental flights were compared and validated. The unsteady aerodynamics and the highly nonlinear flight dynamics present challenging control problems for conventional and learning control algorithms such as Reinforcement Learning. The interface of the simulation is fully compatible with OpenAI Gym environment. As a benchmark study, we present a linear controller for hovering stabilization and a Deep Reinforcement Learning control policy for goal-directed maneuvering. Finally, we demonstrate direct simulation-to-real transfer of both control policies onto the physical robot, further demonstrating the fidelity of the simulation.},
archivePrefix = {arXiv},
arxivId = {1902.09628},
author = {Fei, Fan and Tu, Zhan and Yang, Yilun and Zhang, Jian and Deng, Xinyan},
eprint = {1902.09628},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Flappy Hummingbird An Open Source Dynamic Simulation of Flapping Wing Robots and Animals{\_}Fei et al.pdf:pdf},
month = {feb},
title = {{Flappy Hummingbird: An Open Source Dynamic Simulation of Flapping Wing Robots and Animals}},
url = {http://arxiv.org/abs/1902.09628},
year = {2019}
}
@article{2017,
author = {内田, 祐介 and 山下, 隆義},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}畳み込みニューラルネットワークの研究動向{\_}内田, 山下.pdf:pdf},
journal = {電子情報通信学会 信学技報},
pages = {1--14},
title = {畳み込みニューラルネットワークの研究動向},
year = {2017}
}
@article{kawamura2001,
author = {川村, 次郎},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2001{\_}筋電義手の普及への展望{\_}川村.pdf:pdf},
journal = {日本義肢装具学会誌},
number = {4},
title = {筋電義手の普及への展望},
volume = {17},
year = {2001}
}
@article{Jain2010,
abstract = {Assistive mobile robots that autonomously manipulate objects within everyday settings have the potential to improve the lives of the elderly, injured, and disabled. Within this paper, we present the most recent version of the assistive mobile manipulator EL-E with a focus on the subsystem that enables the robot to retrieve objects from and deliver objects to flat surfaces. Once provided with a 3D location via brief illumination with a laser pointer, the robot autonomously approaches the location and then either grasps the nearest object or places an object. We describe our implementation in detail, while highlighting design principles and themes, including the use of specialized behaviors, task-relevant features, and low-dimensional representations. We also present evaluations of EL-E's performance relative to common forms of variation. We tested EL-E's ability to approach and grasp objects from the 25 object categories that were ranked most important for robotic retrieval by motor-impaired patients from the Emory ALS Center. Although reliability varied, EL-E succeeded at least once with objects from 21 out of 25 of these categories. EL-E also approached and grasped a cordless telephone on 12 different surfaces including floors, tables, and counter tops with 100{\%} success. The same test using a vitamin pill (ca. 15 mm × 5 mm × 5 mm) resulted in 58{\%} success. {\textcopyright} 2009 Springer Science+Business Media, LLC.},
author = {Jain, Advait and Kemp, Charles C.},
doi = {10.1007/s10514-009-9148-5},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2010{\_}EL-E An assistive mobile manipulator that autonomously fetches objects from flat surfaces{\_}Jain, Kemp.pdf:pdf},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Assistive robotics,Mobile manipulation},
number = {1},
pages = {45--64},
title = {{EL-E: An assistive mobile manipulator that autonomously fetches objects from flat surfaces}},
volume = {28},
year = {2010}
}
@article{Wang2007,
abstract = {The STanford Artificial Intelligence Robot (STAIR) project is a long-term group effort aimed at producing a viable home and office assistant robot. As a small concrete step towards this goal, we showed a demonstration video at the 2007 AAAI Mobile Robot Exhibition of the STAIR 1 robot responding to a verbal command to fetch an item. Carrying out this task involved the integration of multiple components, including spoken dialog, navigation, computer visual object detection, and robotic grasping. This paper describes the hardware and software integration frameworks used to facilitate the devel- opment of these components and to bring them together for the demonstration.},
author = {Wang, Endong and Zhang, Qing and Shen, Bo and Zhang, Guangyong and Lu, Xiaowei and Wu, Qing and Wang, Yajuan and Wang, Endong and Zhang, Qing and Shen, Bo and Zhang, Guangyong and Lu, Xiaowei and Wu, Qing and Wang, Yajuan},
doi = {10.1007/978-3-319-06486-4_2},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2007 - STAIR Hardware and Software Architecture.pdf:pdf},
journal = {Association for the Advancement of Artificial Intelligence},
pages = {31--37},
title = {{STAIR: Hardware and Software Architecture}},
year = {2007}
}
@article{Stuckler2016,
abstract = {Cognitive service robots that shall assist persons in need in performing their activities of daily living have recently received much attention in robotics research. Such robots require a vast set of control and perception capabilities to provide useful assistance through mobile manipulation and human-robot interaction. In this article, we present hardware design, perception, and control methods for our cognitive service robot Cosero. We complement autonomous capabilities with handheld teleoperation interfaces on three levels of autonomy. The robot demonstrated various advanced skills, including the use of tools. With our robot we participated in the annual international RoboCup@Home competitions, winning them three times in a row.},
author = {St{\"{u}}ckler, J{\"{o}}rg and Schwarz, Max and Behnke, Sven},
doi = {10.3389/frobt.2016.00058},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Mobile Manipulation, Tool Use, and Intuitive Interaction for Cognitive Service Robot Cosero{\_}St{\"{u}}ckler, Schwarz, Behnke.pdf:pdf},
journal = {Frontiers in Robotics and AI},
keywords = {cognitive service robots,cognitive service robots, mobile manipulation, obj,human,mobile manipulation,object perception,robot interaction,shared},
number = {November},
title = {{Mobile Manipulation, Tool Use, and Intuitive Interaction for Cognitive Service Robot Cosero}},
volume = {3},
year = {2016}
}
@article{Chitta2010,
abstract = {Computing a motion that enables a mobile manip- ulator to open a door is challenging because it requires tight coordination between the motions of the arm and the base. Hard-coding the motion, on the other hand, is infeasible since doors vary widely in their sizes and types, some doors are opened by pulling and others by pushing, and indoor spaces often contain obstacles that limit the freedom of the mobile manipulator and the degree to which the doors open up. In this paper, we show how to overcome the high-dimensionality of the planning problem by identifying a graph-based representation that is small enough for efficient planning yet rich enough to contain feasible motions that open doors. The use of graph search-based motion planning enables us to handle consistently the wide variance of conditions under which doors need to be open. We demonstrate our approach on the PR2 robot - a mobile manipulator with an omnidirectional base and a 7 degree of freedom arm. The robot was successful in opening a variety of doors both by pulling and pushing.},
author = {Chitta, Sachin and Cohen, Benjamin and Likhachev, Maxim},
doi = {10.1109/ROBOT.2010.5509475},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2010{\_}Planning for autonomous door opening with a mobile manipulator{\_}Chitta, Cohen, Likhachev.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1799--1806},
publisher = {IEEE},
title = {{Planning for autonomous door opening with a mobile manipulator}},
year = {2010}
}
@article{Pages2016,
abstract = {— This paper describes TIAGo, the mobile manip-ulator created by PAL Robotics, which was conceived since its very begining as a modular robot. TIAGo inherits several modular components from its eldest brothers REEM, the service robot, and REEM-C, the biped humanoid robot, and incorporates new modular parts that can be used to build up other robots. The modularity techniques used to create TIAGo are introduced in this paper.},
author = {Pages, Jordi and Marchionni, Luca and Ferro, Francesco},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}TIAGo the modular robot that adapts to different research needs{\_}Pages, Marchionni, Ferro.pdf:pdf},
pages = {3--6},
title = {{TIAGo: the modular robot that adapts to different research needs}},
url = {https://clawar.org/wp-content/uploads/2016/10/P2.pdf},
year = {2016}
}
@article{MeloneeWise2016,
abstract = {Since the introduction and widespread adop-tion of the Robot Operating System (ROS), the mobile manipulation and mobile service robot communities have seen great advances in robot capabilities. However, the lack of af-fordable and commercially available fully inte-grated standard platforms remains a major bar-rier to further and faster advances. The Fetch mobile manipulator is designed to be the afford-able standard platform for the next generation of mobile manipulator applications. Fetch's lit-tle brother, Freight, is designed to be the afford-able standard platform for mobile service robot applications. This paper highlights the design decisions and trade-offs made in achieving the low cost of the platforms while continuing to provide the required capabilities for such appli-cations.},
author = {{Melonee Wise} and Ferguson, Michael and King, Derek and Diehr, Eric and Dymesich, David},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Fetch {\&} Freight Standard Platforms for Service Robot Applications{\_}Melonee Wise et al.pdf:pdf},
journal = {Workshop on Autonomous Mobile Service Robots, held at the 2016 International Joint Conference on Artificial Intelligence},
pages = {2--7},
title = {{Fetch {\&} Freight: Standard Platforms for Service Robot Applications}},
url = {http://fetchrobotics.com/research/},
year = {2016}
}
@article{Iriondo2019,
abstract = {Programming robots to perform complex tasks is a very expensive job. Traditional path planning and control are able to generate point to point collision free trajectories, but when the tasks to be performed are complex, traditional planning and control become complex tasks. This study focused on robotic operations in logistics, specifically, on picking objects in unstructured areas using a mobile manipulator configuration. The mobile manipulator has to be able to place its base in a correct place so the arm is able to plan a trajectory up to an object in a table. A deep reinforcement learning (DRL) approach was selected to solve this type of complex control tasks. Using the arm planner's feedback, a controller for the robot base is learned, which guides the platform to such a place where the arm is able to plan a trajectory up to the object. In addition the performance of two DRL algorithms ((Deep Deterministic Policy Gradient (DDPG)) and (Proximal Policy Optimisation (PPO)) is compared within the context of a concrete robotic task.},
author = {Iriondo, Ander and Lazkano, Elena and Susperregi, Loreto and Urain, Julen and Fernandez, Ane and Molina, Jorge},
doi = {10.3390/app9020348},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Pick and Place Operations in Logistics Using a Mobile Manipulator Controlled with Deep Reinforcement Learning{\_}Iriondo et al.pdf:pdf},
journal = {Applied Sciences},
keywords = {deep reinforcement learning,mobile manipulation,robot learning},
number = {2},
pages = {348},
title = {{Pick and Place Operations in Logistics Using a Mobile Manipulator Controlled with Deep Reinforcement Learning}},
volume = {9},
year = {2019}
}
@article{Li2018b,
abstract = {Abstract—It is important for humanoid-like mobile robots to learn the complex motion sequences in human-robot environment such that the robots can adapt such motions. This paper describes a reinforcement learning (RL) strategy for manipulation and grasping of a mobile manipulator, which reduces the complexity of the visual feedback and handle varying manipulation dynamics and uncertain external perturbations. Two hierarchies plannings have been considered in the proposed strategy, high-level online redundancy resolution based on the neural-dynamic optimization algorithm in operational space, and low-level reinforcement learning in joint space. At this level, the dynamic movement primitives (DMPs) have been considered to model and learn the joint trajectories, and then the reinforcement learning is em- ployed to learn the trajectories with uncertainties. Experimental results on the developed humanoid-like mobile robot demonstrate that the presented approach can suppress the uncertain external perturbations.},
author = {Li, Zhijun and Zhao, Ting and Chen, Fei and Hu, Yingbai and Su, Chun Yi and Fukuda, Toshio},
doi = {10.1109/TMECH.2017.2717461},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Reinforcement Learning of Manipulation and Grasping Using Dynamical Movement Primitives for a Humanoidlike Mobile Manipulator{\_}Li et.pdf:pdf},
issn = {10834435},
journal = {IEEE/ASME Transactions on Mechatronics},
keywords = {Dynamic movement primitive (DMP),mobile manipulation,redundancy resolution,reinforcement learning (RL)},
number = {1},
pages = {121--131},
publisher = {IEEE},
title = {{Reinforcement Learning of Manipulation and Grasping Using Dynamical Movement Primitives for a Humanoidlike Mobile Manipulator}},
volume = {23},
year = {2018}
}
@article{Tamei2011,
abstract = {This study aims at robotic clothing assistance as it is yet an open field for robotics despite it is one of the basic and important assistance activities in daily life of elderly as well as disabled people. The clothing assistance is a challenging problem since robots must interact with non-rigid clothes generally represented in a high-dimensional space, and with the assisted person whose posture can vary during the assistance. Thus, the robot is required to manage two difficulties to perform the task of the clothing assistance: 1) handling of non-rigid materials and 2) adaptation of the assisting movements to the assisted person's posture. To overcome these difficulties, we propose to use reinforcement learning with the cloth's state which is low-dimensionally represented in topology coordinates, and with the reward defined in the low-dimensional coordinates. With our developed experimental system, for T-shirt clothing assistance, including an anthropomorphic dual-arm robot and a soft mannequin, we demonstrate the robot quickly learns a suitable arm motion for putting the mannequin's head into a T-shirt.},
author = {Tamei, Tomoya and Matsubara, Takamitsu and Rai, Akshara and Shibata, Tomohiro},
doi = {10.1109/Humanoids.2011.6100915},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2011{\_}Reinforcement learning of clothing assistance with a dual-arm robot{\_}Tamei et al.pdf:pdf},
isbn = {9781612848679},
issn = {21640572},
journal = {IEEE-RAS International Conference on Humanoid Robots},
pages = {733--738},
publisher = {IEEE},
title = {{Reinforcement learning of clothing assistance with a dual-arm robot}},
year = {2011}
}
@article{Heinig1997,
abstract = {Cultured female white sturgeon (Acipenser transmontanus ) fail to reach sexual maturity be 8 years of age even though cultured males reach sexual maturity by 3-4 years of age. Sexual development in the cultured females is arrested at the previtellogenic stage with insufficient follicular development to initiate estrogen synthesis. Exogenous estradiol treatment will induce vitellogenin synthesis in these fish, but even when the plasma concentrations of vitellogenin are elevated, the follicle is not capable of incorporating vitellogenin. Administration of synthetic gonadotropin releasing hormone (LHRHa) alone or in combination with estradiol did not induce uptake of vitellogenin by the follicle. Likewise, administration of carp pituitary extract in combination with estradiol did not influence vitellogenin uptake. The combined treatment of estradiol and LHRHa resulted in significantly higher circulating concentrations of estrogens than the administration of estradiol alone, suggesting that the combined treatment may be the result of induced estrogen synthesis.},
author = {Heinig, M. Jane},
doi = {10.1177/089033449701300102},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2010{\_}Medical and Health-Care Robotics{\_}Heinig.pdf:pdf},
issn = {08903344},
journal = {Journal of Human Lactation},
number = {1},
pages = {3--4},
title = {{Medical and Health-Care Robotics}},
volume = {13},
year = {2010}
}
@article{Yamamoto2018a,
abstract = {There has been an increasing interest in mobile manipulators that is capable of performing physical work in living spaces worldwide, corresponding to population aging with declining birth rates with the expectation of improving quality of life (QOL). Research and development is a must in intelligent sensing and software which enable advanced recognition, judgment, and motion to realize household work by robots. In order to accelerate this research, we have developed a compact and safe research platform, Human Support Robot (HSR), which can be operated in an actual home environment. We assume that overall R{\&}D will accelerate by using a common robot platform among many researchers since that enables them to Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored.},
author = {Yamamoto, Takashi and Nishino, Tamaki and Kajima, Hideki and Ohta, Mitsunori and Ikeda, Koichi},
doi = {10.1145/3214907.3233972},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Human support robot (HSR){\_}Yamamoto et al.pdf:pdf},
isbn = {9781450358101},
pages = {1--2},
title = {{Human support robot (HSR)}},
url = {https://doi.org/10.1145/3214907.3233972},
year = {2018}
}
@article{Lu2017,
abstract = {—In this paper, we propose a new deep hashing (DH) approach to learn compact binary codes for scalable image search. Unlike most existing binary codes learning methods which usually seek a single linear projection to map each sample into a binary feature vector, we develop a deep neural network to seek multiple hierarchical non-linear transformations to learn these binary codes, so that the nonlinear relationship of samples can be well exploited. Our model is learned under three constraints at the top layer of the developed deep network: 1) the loss between the compact real-valued code and the learned binary vector is minimized, 2) the binary codes distribute evenly on each bit, and 3) different bits are as independent as possible. To further improve the discriminative power of the learned binary codes, we extend DH into supervised DH (SDH) and multi-label supervised DH (MSDH) by including a discriminative term into the objective function of DH which simultaneously maximizes the inter-class variations and minimizes the intra-class variations of the learned binary codes with the single-label and multi-label settings, respectively. Extensive experimental results on eight widely used image search datasets show that our proposed methods achieve very competitive results with the state-of-the-arts.},
author = {Lu, Jiwen and Liong, Venice Erin and Zhou, Jie},
doi = {10.1109/TIP.2017.2678163},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Deep Hashing for Scalable Image Search{\_}Lu, Liong, Zhou.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Scalable image search,deep learning,fast similarity search,hashing,multi-label learning},
number = {5},
pages = {2352--2367},
title = {{Deep Hashing for Scalable Image Search}},
volume = {26},
year = {2017}
}
@article{Tahara2013,
author = {Tahara, Kenji},
doi = {10.7210/jrsj.31.364},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2013{\_}Object Manipulation based on the Dynamic Stability{\_}Tahara.pdf:pdf},
issn = {0289-1824},
journal = {Journal of the Robotics Society of Japan},
number = {4},
pages = {364--369},
title = {{Object Manipulation based on the Dynamic Stability}},
volume = {31},
year = {2013}
}
@article{1393,
author = {星川, 英 and 迫田, 辰太郎 and 山野井, 佑介 and 加藤, 龍 and 森下, 壮一郎 and 中村, 達弘 and 關, 達也 and 姜, 銀来 and 横井, 浩史},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}基本把持機能を有する簡易型筋電義手の開発と評価{\_}星川 et al.pdf:pdf},
journal = {知能と情報(日本知能情報ファジィ学会誌)},
number = {6},
pages = {885},
title = {基本把持機能を有する簡易型筋電義手の開発と評価},
volume = {27},
year = {2015}
}
@article{Tani2019,
author = {Tani, Naoyuki and Jiang, Yinlai and Yokoi, Hiroshi},
doi = {https://doi.org/10.7210/jrsj.37.168},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}握力把握・精密把握における安定把持のための筋電義手用関節屈伸機構の開発{\_}Tani, Jiang, Yokoi.pdf:pdf},
journal = {日本ロボット学会誌},
keywords = {grasp stability,myoelectric prosthetic hand,power grasp,precision grasp,sti ff ness},
number = {2},
pages = {168--178},
title = {握力把握・精密把握における安定把持のための筋電義手用関節屈伸機構の開発},
volume = {37},
year = {2019}
}
@article{Eysenbach2018,
abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1802.06070},
author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
eprint = {1802.06070},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Diversity is All You Need Learning Skills without a Reward Function{\_}Eysenbach et al.pdf:pdf},
month = {feb},
title = {{Diversity is All You Need: Learning Skills without a Reward Function}},
url = {http://arxiv.org/abs/1802.06070},
year = {2018}
}
@article{Sather2019,
abstract = {Autonomous harvesting may provide a viable solution to mounting labor pressures in the United States's strawberry industry. However, due to bottlenecks in machine perception and economic viability, a profitable and commercially adopted strawberry harvesting system remains elusive. In this research, we explore the feasibility of using deep reinforcement learning to overcome these bottlenecks and develop a practical algorithm to address the sub-objective of viewpoint optimization, or the development of a control policy to direct a camera to favorable vantage points for autonomous harvesting. We evaluate the algorithm's performance in a custom, open-source simulated environment and observe affirmative results. Our trained agent yields 8.7 times higher returns than random actions and 8.8 percent faster exploration than our best baseline policy, which uses visual servoing. Visual investigation shows the agent is able fixate on favorable viewpoints, despite having no explicit means to propagate information through time. Overall, we conclude that deep reinforcement learning is a promising area of research to advance the state of the art in autonomous strawberry harvesting.},
archivePrefix = {arXiv},
arxivId = {1903.02074},
author = {Sather, Jonathon},
eprint = {1903.02074},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Viewpoint Optimization for Autonomous Strawberry Harvesting with Deep Reinforcement Learning{\_}Sather.pdf:pdf},
month = {mar},
title = {{Viewpoint Optimization for Autonomous Strawberry Harvesting with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1903.02074},
year = {2019}
}
@article{Botvinick2019,
abstract = {Deep reinforcement learning (RL) methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient – that is, it may simply be too slow – to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning.},
author = {Botvinick, Mathew and Ritter, Sam and Wang, Jane X. and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
doi = {10.1016/j.tics.2019.02.006},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Reinforcement Learning, Fast and Slow{\_}Botvinick et al.pdf:pdf},
issn = {1879307X},
journal = {Trends in Cognitive Sciences},
month = {apr},
number = {0},
pmid = {31003893},
publisher = {Elsevier},
title = {{Reinforcement Learning, Fast and Slow}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/31003893},
volume = {0},
year = {2019}
}
@article{Lecarpentier2019,
abstract = {This work tackles the problem of robust zero-shot planning in non-stationary stochastic environments. We study Markov Decision Processes (MDPs) evolving over time and consider Model-Based Reinforcement Learning algorithms in this setting. We make two hypotheses: 1) the environment evolves continuously and its evolution rate is bounded, 2) a current model is known at each decision epoch but not its evolution. Our contribution can be presented in four points. First, we define this specific class of MDPs that we call Non-Stationary MDPs (NSMDPs). We introduce the notion of regular evolution by making an hypothesis of Lipschitz-Continuity on the transition and reward functions w.r.t. time. Secondly, we consider a planning agent using the current model of the environment, but unaware of its future evolution. This leads us to consider a worst-case method where the environment is seen as an adversarial agent. Third, following this approach, we propose the Risk-Averse Tree-Search (RATS) algorithm. This is a zero-shot Model-Based method similar to Minimax search. Finally, we illustrate the benefits brought by RATS empirically and compare its performance with reference Model-Based algorithms.},
archivePrefix = {arXiv},
arxivId = {1904.10090},
author = {Lecarpentier, Erwan and Rachelson, Emmanuel},
eprint = {1904.10090},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Non-Stationary Markov Decision Processes a Worst-Case Approach using Model-Based Reinforcement Learning{\_}Lecarpentier, Rachelson.pdf:pdf},
month = {apr},
title = {{Non-Stationary Markov Decision Processes a Worst-Case Approach using Model-Based Reinforcement Learning}},
url = {http://arxiv.org/abs/1904.10090},
year = {2019}
}
@techreport{Silver2018,
abstract = {We present Residual Policy Learning (RPL): a simple method for improving nondifferentiable policies using model-free deep reinforcement learning. RPL thrives in complex robotic manipulation tasks where good but imperfect controllers are available. In these tasks, reinforcement learning from scratch remains data-inefficient or intractable, but learning a residual on top of the initial controller can yield substantial improvements. We study RPL in six challenging MuJoCo tasks involving partial observability, sensor noise, model misspecification, and controller miscalibration. For initial controllers, we consider both hand-designed policies and model-predictive controllers with known or learned transition models. By combining learning with control algorithms, RPL can perform long-horizon, sparse-reward tasks for which reinforcement learning alone fails. Moreover, we find that RPL consistently and substantially improves on the initial controllers. We argue that RPL is a promising approach for combining the complementary strengths of deep reinforcement learning and robotic control, pushing the boundaries of what either can achieve independently. Video and code at https://k-r-allen.github.io/residual-policy-learning/.},
archivePrefix = {arXiv},
arxivId = {1812.06298},
author = {Silver, Tom and Allen, Kelsey and Tenenbaum, Josh and Kaelbling, Leslie},
eprint = {1812.06298},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Residual Policy Learning{\_}Silver et al.pdf:pdf},
isbn = {1812.06298v2},
title = {{Residual Policy Learning}},
url = {http://arxiv.org/abs/1812.06298},
year = {2018}
}
@techreport{Johannink2018,
abstract = {Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.},
archivePrefix = {arXiv},
arxivId = {1812.03201},
author = {Johannink, Tobias and Bahl, Shikhar and Nair, Ashvin and Luo, Jianlan and Kumar, Avinash and Loskyll, Matthias and Ojea, Juan Aparicio and Solowjow, Eugen and Levine, Sergey},
eprint = {1812.03201},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Residual Reinforcement Learning for Robot Control{\_}Johannink et al.pdf:pdf},
title = {{Residual Reinforcement Learning for Robot Control}},
url = {http://arxiv.org/abs/1812.03201},
year = {2018}
}
@techreport{Ho2016,
abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
archivePrefix = {arXiv},
arxivId = {1606.03476},
author = {Ho, Jonathan and Ermon, Stefano},
eprint = {1606.03476},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Generative Adversarial Imitation Learning{\_}Ho, Ermon.pdf:pdf},
title = {{Generative Adversarial Imitation Learning}},
url = {http://arxiv.org/abs/1606.03476},
year = {2016}
}
@article{Ha2018b,
abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
archivePrefix = {arXiv},
arxivId = {1803.10122},
author = {Ha, David and Schmidhuber, J{\"{u}}rgen},
doi = {10.5281/zenodo.1207631},
eprint = {1803.10122},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Ha, Schmidhuber - 2018 - World Models.pdf:pdf},
month = {mar},
title = {{World Models}},
url = {http://arxiv.org/abs/1803.10122{\%}0Ahttp://dx.doi.org/10.5281/zenodo.1207631 https://worldmodels.github.io},
year = {2018}
}
@article{Oh2018,
abstract = {This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.},
archivePrefix = {arXiv},
arxivId = {1806.05635},
author = {Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
eprint = {1806.05635},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Self-Imitation Learning{\_}Oh et al.pdf:pdf},
month = {jun},
title = {{Self-Imitation Learning}},
url = {http://arxiv.org/abs/1806.05635},
year = {2018}
}
@article{Hester2017,
abstract = {Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classification of the demonstrator's actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfD's performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.},
archivePrefix = {arXiv},
arxivId = {1704.03732},
author = {Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Dulac-Arnold, Gabriel and Osband, Ian and Agapiou, John and Leibo, Joel Z. and Gruslys, Audrunas},
eprint = {1704.03732},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Deep Q-learning from Demonstrations{\_}Hester et al.pdf:pdf},
month = {apr},
title = {{Deep Q-learning from Demonstrations}},
url = {http://arxiv.org/abs/1704.03732},
year = {2017}
}
@article{Lee2019,
abstract = {Many anatomical factors, such as bone geometry and muscle condition, interact to affect human movements. This work aims to build a comprehensive musculoskeletal model and its control system that reproduces realistic human movements driven by muscle contraction dynamics. The variations in the anatomic model generate a spectrum of human movements ranging from typical to highly stylistic movements. To do so, we discuss scalable and reliable simulation of anatomical features, robust control of under-actuated dynamical systems based on deep reinforcement learning, and modeling of pose-dependent joint limits. The key technical contribution is a scalable, two-level imitation learning algorithm that can deal with a comprehensive full-body musculoskeletal model with 346 muscles. We demonstrate the predictive simulation of dynamic motor skills under anatomical conditions including bone deformity, muscle weakness, contracture, and the use of a prosthesis. We also simulate various pathological gaits and predictively visualize how orthopedic surgeries improve post-operative gaits.},
author = {Lee, Seunghwan and Lee, Kyoungmin and Park, Moonseok and Lee, Jehee},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2019 - Scalable Muscle-actuated Human Simulation and Control.pdf:pdf},
journal = {ACM Transactions on Graphics},
title = {{Scalable Muscle-actuated Human Simulation and Control}},
url = {http://mrl.snu.ac.kr/research/ProjectScalable/Page.htm http://mrl.snu.ac.kr/research/ProjectScalable/Preprint.pdf},
year = {2019}
}
@article{Singh2019a,
abstract = {The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.},
archivePrefix = {arXiv},
arxivId = {1904.07854},
author = {Singh, Avi and Yang, Larry and Hartikainen, Kristian and Finn, Chelsea and Levine, Sergey},
eprint = {1904.07854},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2019{\_}End-to-End Robotic Reinforcement Learning without Reward Engineering{\_}Singh et al.pdf:pdf},
month = {apr},
title = {{End-to-End Robotic Reinforcement Learning without Reward Engineering}},
url = {http://arxiv.org/abs/1904.07854},
year = {2019}
}
@article{Buslaev2018,
abstract = {Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve output labels. In computer vision domain, image augmentations have become a common implicit regularization technique to combat overfitting in deep convolutional neural networks and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations and combinations of flipping, rotating, scaling, and cropping. Moreover, the image processing speed varies in existing tools for image augmentation. We present Albumentations, a fast and flexible library for image augmentations with many various image transform operations available, that is also an easy-to-use wrapper around other augmentation libraries. We provide examples of image augmentations for different computer vision tasks and show that Albumentations is faster than other commonly used image augmentation tools on the most of commonly used image transformations. The source code for Albumentations is made publicly available online at https://github.com/albu/albumentations},
archivePrefix = {arXiv},
arxivId = {1809.06839},
author = {Buslaev, Alexander and Parinov, Alex and Khvedchenya, Eugene and Iglovikov, Vladimir I. and Kalinin, Alexandr A.},
eprint = {1809.06839},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Albumentations fast and flexible image augmentations{\_}Buslaev et al.pdf:pdf},
month = {sep},
title = {{Albumentations: fast and flexible image augmentations}},
url = {http://arxiv.org/abs/1809.06839},
year = {2018}
}
@article{Wulfmier2018a,
abstract = {In order to enable more widespread application of robots, we are required to reduce the human effort for the introduction of existing robotic platforms to new environments and tasks. In this thesis, we identify three complementary strategies to address this challenge, via the use of imitation learning, domain adaptation, and transfer learning based on simulations. The overall work strives to reduce the effort of generating training data by employing inexpensively obtainable labels and by transferring information between different domains with deviating underlying properties.
Imitation learning enables a straightforward way for untrained personnel to teach robots to perform tasks by providing demonstrations, which represent a comparably inexpensive source of supervision. We develop a scalable approach to identify the preferences underlying demonstration data via the framework of inverse reinforcement learning. The method enables integration of the extracted preferences as cost maps into existing motion planning systems. We further incorporate prior domain knowledge and demonstrate that the approach outperforms the baselines including manually crafted cost functions.
In addition to employing low-cost labels from demonstration, we investigate the adaptation of models to domains without available supervisory information. Specifically, the challenge of appearance changes in outdoor robotics such as illumination and weather shifts is addressed using an adversarial domain adaptation approach. A principal advantage of the method over prior work is the straightforwardness of adapting arbitrary, state-of-the-art neural network architectures. Finally, we demonstrate performance benefits of the method for semantic segmentation of drivable terrain.
Our last contribution focuses on simulation to real world transfer learning, where the characteristic differences are not only regarding the visual appearance but the underlying system dynamics. Our work aims at parallel training in both systems and mutual guidance via auxiliary alignment rewards to accelerate training for real world systems. The approach is shown to outperform various baselines as well as a unilateral alignment variant.},
archivePrefix = {arXiv},
arxivId = {1904.07346},
author = {Wulfmier, Markus},
doi = {10.1007/s13218-019-00587-0},
eprint = {1904.07346},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Efficient Supervision for Robot Learning via Imitation, Simulation, and Adaptation{\_}Wulfmier.pdf:pdf},
month = {apr},
title = {{Efficient Supervision for Robot Learning via Imitation, Simulation, and Adaptation}},
url = {https://ora.ox.ac.uk/objects/uuid:2b5eeb55-639a-40ae-83b7-bd01fc8fd6cc},
year = {2018}
}
@article{Kirillov2019,
abstract = {The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.},
archivePrefix = {arXiv},
arxivId = {1901.02446},
author = {Kirillov, Alexander and Girshick, Ross and He, Kaiming and Doll{\'{a}}r, Piotr},
eprint = {1901.02446},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Panoptic Feature Pyramid Networks{\_}Kirillov et al.pdf:pdf},
month = {jan},
title = {{Panoptic Feature Pyramid Networks}},
url = {http://arxiv.org/abs/1901.02446},
year = {2019}
}
@techreport{Bousmalis2016,
abstract = {The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We suggest that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained not only to perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.},
archivePrefix = {arXiv},
arxivId = {1608.06019},
author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
eprint = {1608.06019},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Domain Separation Networks{\_}Bousmalis et al.pdf:pdf},
title = {{Domain Separation Networks}},
url = {http://arxiv.org/abs/1608.06019},
year = {2016}
}
@article{Chang2015,
abstract = {We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.},
archivePrefix = {arXiv},
arxivId = {1512.03012},
author = {Chang, Angel X. and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},
eprint = {1512.03012},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}ShapeNet An Information-Rich 3D Model Repository{\_}Chang et al.pdf:pdf},
month = {dec},
title = {{ShapeNet: An Information-Rich 3D Model Repository}},
url = {http://arxiv.org/abs/1512.03012},
year = {2015}
}
@article{Matas2018,
abstract = {We have seen much recent progress in rigid object manipulation, but interaction with deformable objects has notably lagged behind. Due to the large configuration space of deformable objects, solutions using traditional modelling approaches require significant engineering work. Perhaps then, bypassing the need for explicit modelling and instead learning the control in an end-to-end manner serves as a better approach? Despite the growing interest in the use of end-to-end robot learning approaches, only a small amount of work has focused on their applicability to deformable object manipulation. Moreover, due to the large amount of data needed to learn these end-to-end solutions, an emerging trend is to learn control policies in simulation and then transfer them over to the real world. To-date, no work has explored whether it is possible to learn and transfer deformable object policies. We believe that if sim-to-real methods are to be employed further, then it should be possible to learn to interact with a wide variety of objects, and not only rigid objects. In this work, we use a combination of state-of-the-art deep reinforcement learning algorithms to solve the problem of manipulating deformable objects (specifically cloth). We evaluate our approach on three tasks --- folding a towel up to a mark, folding a face towel diagonally, and draping a piece of cloth over a hanger. Our agents are fully trained in simulation with domain randomisation, and then successfully deployed in the real world without having seen any real deformable objects.},
archivePrefix = {arXiv},
arxivId = {1806.07851},
author = {Matas, Jan and James, Stephen and Davison, Andrew J.},
eprint = {1806.07851},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Matas, James, Davison - 2018 - Sim-to-Real Reinforcement Learning for Deformable Object Manipulation.pdf:pdf},
keywords = {deformable objects,manipulation,reinforcement learning},
number = {CoRL},
title = {{Sim-to-Real Reinforcement Learning for Deformable Object Manipulation}},
url = {http://arxiv.org/abs/1806.07851},
year = {2018}
}
@article{Zhang2018b,
abstract = {In this paper, we deal with the reality gap from a novel perspective, targeting transferring Deep Reinforcement Learning (DRL) policies learned in simulated environments to the real-world domain for visual control tasks. Instead of adopting the common solutions to the problem by increasing the visual fidelity of synthetic images output from simulators during the training phase, we seek to tackle the problem by translating the real-world image streams back to the synthetic domain during the deployment phase, to make the robot feel at home. We propose this as a lightweight, flexible, and efficient solution for visual control, as 1) no extra transfer steps are required during the expensive training of DRL agents in simulation; 2) the trained DRL agents will not be constrained to being deployable in only one specific real-world environment; 3) the policy training and the transfer operations are decoupled, and can be conducted in parallel. Besides this, we propose a simple yet effective shift loss that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs. We validate the shift loss for artistic style transfer for videos and domain adaptation, and validate our visual control approach in indoor and outdoor robotics experiments.},
archivePrefix = {arXiv},
arxivId = {1802.00265},
author = {Zhang, Jingwei and Tai, Lei and Yun, Peng and Xiong, Yufeng and Liu, Ming and Boedecker, Joschka and Burgard, Wolfram},
doi = {10.1109/LRA.2019.2894216},
eprint = {1802.00265},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2018 - VR-Goggles for Robots Real-to-sim Domain Adaptation for Visual Control.pdf:pdf},
journal = {IEEE Robotics and Automation Letters},
number = {2},
pages = {1148--1155},
publisher = {IEEE},
title = {{VR-Goggles for Robots: Real-to-sim Domain Adaptation for Visual Control}},
url = {http://arxiv.org/abs/1802.00265},
volume = {4},
year = {2018}
}
@inproceedings{Pinto2016,
abstract = {Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.},
author = {Pinto, Lerrel and Gupta, Abhinav},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2016.7487517},
isbn = {9781467380263},
issn = {10504729},
month = {may},
pages = {3406--3413},
publisher = {IEEE},
title = {{Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours}},
url = {http://ieeexplore.ieee.org/document/7487517/},
volume = {2016-June},
year = {2016}
}
@article{Lenz2015,
abstract = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast, as well as robust, we present a two-step cascaded structure with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs well, for which we present a method to apply structured regularization on the weights based on multimodal group regularization. We demonstrate that our method outperforms the previous state-of-the-art methods in robotic grasp detection, and can be used to successfully execute grasps on two different robotic platforms.},
author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
doi = {10.1177/0278364914549607},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2013{\_}Deep learning for detecting robotic grasps{\_}Lenz, Lee, Saxena.pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {3D feature learning,Baxter,PR2,RGB-D multi-modal data,Robotic grasping,deep learning},
month = {apr},
number = {4-5},
pages = {705--724},
title = {{Deep learning for detecting robotic grasps}},
url = {http://journals.sagepub.com/doi/10.1177/0278364914549607},
volume = {34},
year = {2013}
}
@inproceedings{VenkatramanNarayanan2017,
abstract = {Reinforcement learning holds the promise of en- abling autonomous robots to learn large repertoires of behav- ioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learn- ing alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off- policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations. I.},
author = {{Venkatraman Narayanan}, Maxim Likhachev},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA) Singapore, May 29 - June 3, 2017},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Venkatraman Narayanan - 2017 - Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates Shixiang.pdf:pdf},
isbn = {9781509046348},
keywords = {Automatisierungstechnik,Computer Vision for Automa},
title = {{Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates Shixiang}},
url = {http://ieeexplore.ieee.org/servlet/opac?punumber=7960754},
year = {2017}
}
@article{Polydoros2017,
abstract = {{\textcopyright} 2017 Springer Science+Business Media DordrechtReinforcement learning is an appealing approach for allowing robots to learn new tasks. Relevant literature reveals a plethora of methods, but at the same time makes clear the lack of implementations for dealing with real life challenges. Current expectations raise the demand for adaptable robots. We argue that, by employing model-based reinforcement learning, the—now limited—adaptability characteristics of robotic systems can be expanded. Also, model-based reinforcement learning exhibits advantages that makes it more applicable to real life use-cases compared to model-free methods. Thus, in this survey, model-based methods that have been applied in robotics are covered. We categorize them based on the derivation of an optimal policy, the definition of the returns function, the type of the transition model and the learned task. Finally, we discuss the applicability of model-based reinforcement learning approaches in new applications, taking into consideration the state of the art in both algorithms and hardware.},
author = {Polydoros, Athanasios S. and Nalpantidis, Lazaros},
doi = {10.1007/s10846-017-0468-y},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Polydoros, Nalpantidis - 2017 - Survey of Model-Based Reinforcement Learning Applications on Robotics.pdf:pdf},
issn = {15730409},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Intelligent robotics,Machine learning,Model-based reinforcement learning,Policy search,Reward functions,Robot learning,Transition models},
month = {may},
number = {2},
pages = {153--173},
publisher = {Springer Netherlands},
title = {{Survey of Model-Based Reinforcement Learning: Applications on Robotics}},
url = {http://link.springer.com/10.1007/s10846-017-0468-y},
volume = {86},
year = {2017}
}
@inproceedings{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.322},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2017 - Mask R-CNN.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
month = {oct},
pages = {2980--2988},
publisher = {IEEE},
title = {{Mask R-CNN}},
url = {http://ieeexplore.ieee.org/document/8237584/},
volume = {2017-Octob},
year = {2017}
}
@article{Chen2019,
abstract = {As a sub-domain of text-to-image synthesis, text-to-face generation has huge potentials in public safety domain. With lack of dataset, there are almost no related research focusing on text-to-face synthesis. In this paper, we propose a fully-trained Generative Adversarial Network (FTGAN) that trains the text encoder and image decoder at the same time for fine-grained text-to-face generation. With a novel fully-trained generative network, FTGAN can synthesize higher-quality images and urge the outputs of the FTGAN are more relevant to the input sentences. In addition, we build a dataset called SCU-Text2face for text-to-face synthesis. Through extensive experiments, the FTGAN shows its superiority in boosting both generated images' quality and similarity to the input descriptions. The proposed FTGAN outperforms the previous state of the art, boosting the best reported Inception Score to 4.63 on the CUB dataset. On SCU-text2face, the face images generated by our proposed FTGAN just based on the input descriptions is of average 59{\%} similarity to the ground-truth, which set a baseline for text-to-face synthesis.},
archivePrefix = {arXiv},
arxivId = {1904.05729},
author = {Chen, Xiang and Qing, Lingbo and He, Xiaohai and Luo, Xiaodong and Xu, Yining},
eprint = {1904.05729},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2019 - FTGAN A Fully-trained Generative Adversarial Networks for Text to Face Generation.pdf:pdf},
month = {apr},
title = {{FTGAN: A Fully-trained Generative Adversarial Networks for Text to Face Generation}},
url = {http://arxiv.org/abs/1904.05729},
year = {2019}
}
@article{Xie2019a,
abstract = {Machine learning techniques have enabled robots to learn narrow, yet complex tasks and also perform broad, yet simple skills with a wide variety of objects. However, learning a model that can both perform complex tasks and generalize to previously unseen objects and goals remains a significant challenge. We study this challenge in the context of "improvisational" tool use: a robot is presented with novel objects and a user-specified goal (e.g., sweep some clutter into the dustpan), and must figure out, using only raw image observations, how to accomplish the goal using the available objects as tools. We approach this problem by training a model with both a visual and physical understanding of multi-object interactions, and develop a sampling-based optimizer that can leverage these interactions to accomplish tasks. We do so by combining diverse demonstration data with self-supervised interaction data, aiming to leverage the interaction data to build generalizable models and the demonstration data to guide the model-based RL planner to solve complex tasks. Our experiments show that our approach can solve a variety of complex tool use tasks from raw pixel inputs, outperforming both imitation learning and self-supervised learning individually. Furthermore, we show that the robot can perceive and use novel objects as tools, including objects that are not conventional tools, while also choosing dynamically to use or not use tools depending on whether or not they are required.},
archivePrefix = {arXiv},
arxivId = {1904.05538},
author = {Xie, Annie and Ebert, Frederik and Levine, Sergey and Finn, Chelsea},
eprint = {1904.05538},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Improvisation through Physical Understanding Using Novel Objects as Tools with Visual Foresight{\_}Xie et al.pdf:pdf},
month = {apr},
title = {{Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight}},
url = {http://arxiv.org/abs/1904.05538},
year = {2019}
}
@article{Wan2019,
abstract = {Weakly supervised object detection (WSOD) is a challenging task when provided with image category supervision but required to simultaneously learn object locations and object detectors. Many WSOD approaches adopt multiple instance learning (MIL) and have non-convex loss functions which are prone to get stuck into local minima (falsely localize object parts) while missing full object extent during training. In this paper, we introduce a continuation optimization method into MIL and thereby creating continuation multiple instance learning (C-MIL), with the intention of alleviating the non-convexity problem in a systematic way. We partition instances into spatially related and class related subsets, and approximate the original loss function with a series of smoothed loss functions defined within the subsets. Optimizing smoothed loss functions prevents the training procedure falling prematurely into local minima and facilitates the discovery of Stable Semantic Extremal Regions (SSERs) which indicate full object extent. On the PASCAL VOC 2007 and 2012 datasets, C-MIL improves the state-of-the-art of weakly supervised object detection and weakly supervised object localization with large margins.},
archivePrefix = {arXiv},
arxivId = {1904.05647},
author = {Cvpr, Anonymous and Id, Paper},
eprint = {1904.05647},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}C-MIL Continuation Multiple Instance Learning for Weakly Supervised Object Detection{\_}Cvpr, Id(2).pdf:pdf},
title = {{C-MIL : Continuation Multiple Instance Learning for Weakly Supervised Object Detection}},
url = {http://arxiv.org/abs/1904.05647},
year = {2019}
}
@article{Nittala2018a,
abstract = {Skin-based touch input opens up new opportunities for direct, subtle, and expressive interaction. However, existing skin-worn sensors are restricted to single-touch input and limited by a low resolution. We present the first skin overlay that can capture high-resolution multi-touch input. Our main contributions are: 1) Based on an exploration of functional materials, we present a fabrication approach for printing thin and flexible multi-touch sensors for on-skin interactions. 2) We present the first non-rectangular multi-touch sensor overlay for use on skin and introduce a design tool that generates such sensors in custom shapes and sizes. 3) To validate the feasibility and versatility of our approach, we present four application examples and empirical results from two technical evaluations. They confirm that the sensor achieves a high signal-to-noise ratio on the body under various grounding conditions and has a high spatial accuracy even when subjected to strong deformations.},
author = {Nittala, Aditya Shekhar and Withana, Anusha and Pourjafarian, Narjes and Steimle, J{\"{u}}rgen},
doi = {10.1145/3173574.3173607},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Multi-Touch Skin A Thin and Flexible Multi-Touch Sensor for On-Skin Input{\_}Nittala et al.pdf:pdf},
isbn = {9781450356206},
journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI '18},
pages = {1--12},
title = {{Multi-Touch Skin: A Thin and Flexible Multi-Touch Sensor for On-Skin Input}},
url = {http://dl.acm.org/citation.cfm?doid=3173574.3173607},
year = {2018}
}
@article{Tai2016,
abstract = {Deep learning techniques have been widely applied, achieving state-of-the-art results in various fields of study. This survey focuses on deep learning solutions that target learning control policies for robotics applications. We carry out our discussions on the two main paradigms for learning control with deep networks: deep reinforcement learning and imitation learning. For deep reinforcement learning (DRL), we begin from traditional reinforcement learning algorithms, showing how they are extended to the deep context and effective mechanisms that could be added on top of the DRL algorithms. We then introduce representative works that utilize DRL to solve navigation and manipulation tasks in robotics. We continue our discussion on methods addressing the challenge of the reality gap for transferring DRL policies trained in simulation to real-world scenarios, and summarize robotics simulation platforms for conducting DRL research. For imitation leaning, we go through its three main categories, behavior cloning, inverse reinforcement learning and generative adversarial imitation learning, by introducing their formulations and their corresponding robotics applications. Finally, we discuss the open challenges and research frontiers.},
archivePrefix = {arXiv},
arxivId = {1612.07139},
author = {Tai, Lei and Zhang, Jingwei and Liu, Ming and Boedecker, Joschka and Burgard, Wolfram},
eprint = {1612.07139},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2016{\_}A Survey of Deep Network Solutions for Learning Control in Robotics From Reinforcement to Imitation{\_}Tai et al.pdf:pdf},
number = {8},
pages = {1--19},
title = {{A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation}},
url = {http://arxiv.org/abs/1612.07139},
volume = {14},
year = {2016}
}
@article{Florence2018a,
abstract = {What is the right object representation for manipulation? We would like robots to visually perceive scenes and learn an understanding of the objects in them that (i) is task-agnostic and can be used as a building block for a variety of manipulation tasks, (ii) is generally applicable to both rigid and non-rigid objects, (iii) takes advantage of the strong priors provided by 3D vision, and (iv) is entirely learned from self-supervision. This is hard to achieve with previous methods: much recent work in grasping does not extend to grasping specific objects or other tasks, whereas task-specific learning may require many trials to generalize well across object configurations or other tasks. In this paper we present Dense Object Nets, which build on recent developments in self-supervised dense descriptor learning, as a consistent object representation for visual understanding and manipulation. We demonstrate they can be trained quickly (approximately 20 minutes) for a wide variety of previously unseen and potentially non-rigid objects. We additionally present novel contributions to enable multi-object descriptor learning, and show that by modifying our training procedure, we can either acquire descriptors which generalize across classes of objects, or descriptors that are distinct for each object instance. Finally, we demonstrate the novel application of learned dense descriptors to robotic manipulation. We demonstrate grasping of specific points on an object across potentially deformed object configurations, and demonstrate using class general descriptors to transfer specific grasps across objects in a class.},
archivePrefix = {arXiv},
arxivId = {1806.08756},
author = {Florence, Peter R. and Manuelli, Lucas and Tedrake, Russ},
eprint = {1806.08756},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Dense Object Nets Learning Dense Visual Object Descriptors By and For Robotic Manipulation{\_}Florence, Manuelli, Tedrake.pdf:pdf},
keywords = {robot manipulation,self-supervision,visual descriptor learning},
pages = {1--12},
title = {{Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation}},
url = {http://arxiv.org/abs/1806.08756},
year = {2018}
}
@article{Wilson2018,
author = {Wilson, Andrew D. and Throm, Nick and Schwarz, Julia and Benko, Hrvoje and Xiao, Robert},
doi = {10.1109/tvcg.2018.2794222},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}MRTouch Adding Touch Input to Head-Mounted Mixed Reality{\_}Wilson et al.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {4},
pages = {1653--1660},
title = {{MRTouch: Adding Touch Input to Head-Mounted Mixed Reality}},
volume = {24},
year = {2018}
}
@article{AndrewBagnell2014a,
abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
author = {{Andrew Bagnell}, J.},
doi = {10.1007/978-3-319-03194-1_2},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2014{\_}Reinforcement Learning in Robotics A Survey{\_}Andrew Bagnell.pdf:pdf},
issn = {1610742X},
journal = {Springer Tracts in Advanced Robotics},
keywords = {learning control,reinforcement learning,robot,survey},
pages = {9--67},
title = {{Reinforcement Learning in Robotics: A Survey}},
volume = {97},
year = {2014}
}
@article{Tuceryan2002a,
abstract = {Augmented reality (AR) is a technology in which a user's view of the real world is enhanced or augmented with additional information generated from a computer model. To have a working AR system, the see-through display system must be calibrated so that the graphics are properly rendered. The optical see-through systems present an additional challenge because, unlike the video see-through systems, we do not have direct access to the image data to be used in various calibration procedures: This paper reports on a calibration method we developed for optical see-through head-mounted displays. We first introduce a method for calibrating monocular optical see-through displays (that is, a display for one eye only) and then extend it to stereo optical see-through displays in which the displays for both eyes are calibrated in a single procedure. The method integrates the measurements for the camera and a six-degrees-of-freedom tracker that is attached to the camera to do the calibration. We have used both an off-the-shelf magnetic tracker as well as a vision-based infrared tracker we have built In the monocular case, the calibration is based on the alignment of image points with a single 3D point in the world coordinate system from various viewpoints. In this method, the user interaction to perform the calibration is extremely easy compared to prior methods, and there is no requirement for keeping the head immobile while performing the calibration. In the stereo calibration case, the user aligns a stereoscopically fused 2D marker, which is perceived in depth, with a single target point in the world whose coordinates are known. As in the monocular case, there is no requirement that the user keep his or her head fixed.},
author = {Tuceryan, Mihran and Genc, Yakup and Navab, Nassir},
doi = {10.1162/105474602317473213},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2002{\_}Single-point active alignment method (SPAAM) for optical see-through HMD calibration for augmented reality{\_}Tuceryan, Genc, Navab.pdf:pdf;:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Tuceryan, Genc, Navab - 2002 - Single-point active alignment method (SPAAM) for optical see-through HMD calibration for augmented rea(2).pdf:pdf},
issn = {10547460},
journal = {Presence: Teleoperators and Virtual Environments},
number = {3},
pages = {259--276},
title = {{Single-point active alignment method (SPAAM) for optical see-through HMD calibration for augmented reality}},
volume = {11},
year = {2002}
}
@article{Jang2018a,
abstract = {Well structured visual representations can make robot learning faster and can improve generalization. In this paper, we study how we can acquire effective object-centric representations for robotic manipulation tasks without human labeling by using autonomous robot interaction with the environment. Such representation learning methods can benefit from continuous refinement of the representation as the robot collects more experience, allowing them to scale effectively without human intervention. Our representation learning approach is based on object persistence: when a robot removes an object from a scene, the representation of that scene should change according to the features of the object that was removed. We formulate an arithmetic relationship between feature vectors from this observation, and use it to learn a representation of scenes and objects that can then be used to identify object instances, localize them in the scene, and perform goal-directed grasping tasks where the robot must retrieve commanded objects from a bin. The same grasping procedure can also be used to automatically collect training data for our method, by recording images of scenes, grasping and removing an object, and recording the outcome. Our experiments demonstrate that this self-supervised approach for tasked grasping substantially outperforms direct reinforcement learning from images and prior representation learning methods.},
archivePrefix = {arXiv},
arxivId = {1811.06964},
author = {Jang, Eric and Devin, Coline and Vanhoucke, Vincent and Levine, Sergey},
eprint = {1811.06964},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Grasp2Vec Learning Object Representations from Self-Supervised Grasping{\_}Jang et al.pdf:pdf;:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Jang et al. - 2018 - Grasp2Vec Learning Object Representations from Self-Supervised Grasping(2).pdf:pdf},
keywords = {instance grasping,reinforcement learning,unsupervised learning},
number = {CoRL},
pages = {1--15},
title = {{Grasp2Vec: Learning Object Representations from Self-Supervised Grasping}},
url = {http://arxiv.org/abs/1811.06964},
year = {2018}
}
@article{Zhang2018,
abstract = {{\textcopyright} 2018 WILEY-VCH Verlag GmbH  {\&}  Co. KGaA, Weinheim Nanostructured composites built with microporous cellulose fibers and carbon nanotubes (CNTs) have potential impacts in the fields of energy storage, sensors, and flexible electronics. Few results have been shown for high mechanoelectrical sensitivity of CNT-paper composite because of numerous current paths in the network. Here, CNT-paper-based nanostructured composite sensors whose sensitivities are generated by controlled tensile fracture of the composite are presented. Under uniaxial load, the cellulose fibers in the paper experience straightening, stiffening, and fracture. The cellulose fibers originally parallel to the tension are fractured while those inclined and perpendicular to the tension are reorganized to form crossbar junctions in the vicinity of a crack. The cross junctions exhibit resistive and capacitive sensitivity to the out-of-plane force by the compression of the junctions. Such piezoresistive and piezocapacitive sensitivities are characterized and evaluated for human behavior monitoring.},
author = {Zhang, Jinyuan and Lee, Gil Yong and Cerwyn, Chiew and Yang, Jinkyu and Fondjo, Fabrice and Kim, Jong Hoon and Taya, Minoru and Gao, Dayong and Chung, Jae Hyun},
doi = {10.1002/admt.201700266},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Fracture-Induced Mechanoelectrical Sensitivities of Paper-Based Nanocomposites{\_}Zhang et al.pdf:pdf},
issn = {2365709X},
journal = {Advanced Materials Technologies},
keywords = {fracture,multiwall carbon nanotubes,nanocomposites,paper,wearable sensors},
number = {3},
pages = {1--7},
title = {{Fracture-Induced Mechanoelectrical Sensitivities of Paper-Based Nanocomposites}},
volume = {3},
year = {2018}
}
@article{Jaderberg2018,
abstract = {Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.},
archivePrefix = {arXiv},
arxivId = {1807.01281},
author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
eprint = {1807.01281},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Human-level performance in first-person multiplayer games with population-based deep reinforcement learning{\_}Jaderberg et al.pdf:pdf;:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Jaderberg et al. - 2018 - Human-level performance in first-person multiplayer games with population-based deep reinforcement learning(2).pdf:pdf},
title = {{Human-level performance in first-person multiplayer games with population-based deep reinforcement learning}},
url = {http://arxiv.org/abs/1807.01281},
year = {2018}
}
@article{Erat2018,
abstract = {Drones allow exploring dangerous or impassable areas safely from a distant point of view. However, flight control from an egocentric view in narrow or constrained environments can be challenging. Arguably, an exocentric view would afford a better overview and, thus, more intuitive flight control of the drone. Unfortunately, such an exocentric view is unavailable when exploring indoor environments. This paper investigates the potential of drone-augmented human vision, i.e., of exploring the environment and controlling the drone indirectly from an exocentric viewpoint. If used with a see-through display, this approach can simulate X-ray vision to provide a natural view into an otherwise occluded environment. The user's view is synthesized from a three-dimensional reconstruction of the indoor environment using image-based rendering. This user interface is designed to reduce the cognitive load of the drone's flight control. The user can concentrate on the exploration of the inaccessible space, while flight control is largely delegated to the drone's autopilot system. We assess our system with a first experiment showing how drone-augmented human vision supports spatial understanding and improves natural interaction with the drone. IEEE},
author = {Erat, Okan and Isop, Werner Alexander and Kalkofen, Denis and Schmalstieg, Dieter},
doi = {10.1109/TVCG.2018.2794058},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Drone-Augmented human vision Exocentric control for drones exploring hidden areas{\_}Erat et al.pdf:pdf;:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Erat et al. - 2018 - Drone-Augmented human vision Exocentric control for drones exploring hidden areas(2).pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Drone,Hololens,Mixed reality,Pick-And-place,X-ray},
number = {4},
pages = {1437--1446},
title = {{Drone-Augmented human vision: Exocentric control for drones exploring hidden areas}},
volume = {24},
year = {2018}
}
@article{Mavrin2019a,
abstract = {Learning an effective representation for high-dimensional data is a challenging problem in reinforcement learning (RL). Deep reinforcement learning (DRL) such as Deep Q networks (DQN) achieves remarkable success in computer games by learning deeply encoded representation from convolution networks. In this paper, we propose a simple yet very effective method for representation learning with DRL algorithms. Our key insight is that features learned by DRL algorithms are highly correlated, which interferes with learning. By adding a regularized loss that penalizes correlation in latent features (with only slight computation), we decorrelate features represented by deep neural networks incrementally. On 49 Atari games, with the same regularization factor, our decorrelation algorithms perform {\$}70\backslash{\%}{\$} in terms of human-normalized scores, which is {\$}40\backslash{\%}{\$} better than DQN. In particular, ours performs better than DQN on 39 games with 4 close ties and lost only slightly on {\$}6{\$} games. Empirical results also show that the decorrelation method applies to Quantile Regression DQN (QR-DQN) and significantly boosts performance. Further experiments on the losing games show that our decorelation algorithms can win over DQN and QR-DQN with a fined tuned regularization factor.},
archivePrefix = {arXiv},
arxivId = {1903.07765},
author = {Mavrin, Borislav and Yao, Hengshuai and Kong, Linglong},
eprint = {1903.07765},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2019{\_}Deep Reinforcement Learning with Decorrelation{\_}Mavrin, Yao, Kong.pdf:pdf},
month = {mar},
title = {{Deep Reinforcement Learning with Decorrelation}},
url = {http://arxiv.org/abs/1903.07765},
year = {2019}
}
@article{Eslami2018a,
abstract = {Scene representation-the process of converting visual sensory data into concise descriptions-is a requirement for intelligent behavior. Recent work has shown that neural networks excel at this task when provided with large, labeled datasets. However, removing the reliance on human labeling remains an important open problem. To this end, we introduce the Generative Query Network (GQN), a framework within which machines learn to represent scenes using only their own sensors. The GQN takes as input images of a scene taken from different viewpoints, constructs an internal representation, and uses this representation to predict the appearance of that scene from previously unobserved viewpoints. The GQN demonstrates representation learning without human labels or domain knowledge, paving the way toward machines that autonomously learn to understand the world around them.},
archivePrefix = {arXiv},
arxivId = {arXiv:0811.2183v2},
author = {Eslami, S. M.Ali and {Jimenez Rezende}, Danilo and Besse, Frederic and Viola, Fabio and Morcos, Ari S. and Garnelo, Marta and Ruderman, Avraham and Rusu, Andrei A. and Danihelka, Ivo and Gregor, Karol and Reichert, David P. and Buesing, Lars and Weber, Theophane and Vinyals, Oriol and Rosenbaum, Dan and Rabinowitz, Neil and King, Helen and Hillier, Chloe and Botvinick, Matt and Wierstra, Daan and Kavukcuoglu, Koray and Hassabis, Demis},
doi = {10.1126/science.aar6170},
eprint = {arXiv:0811.2183v2},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Neural scene representation and rendering (supplementary material){\_}Eslami et al.pdf:pdf},
isbn = {9783642121425},
issn = {10959203},
journal = {Science (New York, N.Y.)},
number = {6394},
pages = {1204--1210},
pmid = {29903970},
title = {{Neural scene representation and rendering (supplementary material)}},
volume = {360},
year = {2018}
}
@article{Andrychowicz2017a,
abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
archivePrefix = {arXiv},
arxivId = {1707.01495},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
eprint = {1707.01495},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2017{\_}Hindsight Experience Replay{\_}Andrychowicz et al.pdf:pdf},
number = {Nips},
title = {{Hindsight Experience Replay}},
url = {http://arxiv.org/abs/1707.01495},
year = {2017}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {1509.02971},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2015{\_}Continuous control with deep reinforcement learning{\_}Lillicrap et al.pdf:pdf},
title = {{Continuous control with deep reinforcement learning}},
url = {http://arxiv.org/abs/1509.02971},
year = {2015}
}
@article{Yang2017,
abstract = {Automatic design of organic materials requires black-box optimization in a vast chemical space. In conventional molecular design algorithms, a molecule is built as a combination of predetermined fragments. Recently, deep neural network models such as variational auto encoders (VAEs) and recurrent neural networks (RNNs) are shown to be effective in de novo design of molecules without any predetermined fragments. This paper presents a novel python library ChemTS that explores the chemical space by combining Monte Carlo tree search (MCTS) and an RNN. In a benchmarking problem of optimizing the octanol-water partition coefficient and synthesizability, our algorithm showed superior efficiency in finding high-scoring molecules. ChemTS is available at https://github.com/tsudalab/ChemTS.},
author = {Yang, Xiufeng and Zhang, Jinzhe and Yoshizoe, Kazuki and Terayama, Kei and Tsuda, Koji},
doi = {10.1080/14686996.2017.1401424},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2017{\_}ChemTS an efficient python library for de novo molecular generation{\_}Yang et al.pdf:pdf},
issn = {18785514},
journal = {Science and Technology of Advanced Materials},
keywords = {Molecular design,Monte Carlo tree search,python library,recurrent neural network},
number = {1},
pages = {972--976},
publisher = {Taylor {\&} Francis},
title = {{ChemTS: an efficient python library for de novo molecular generation}},
url = {https://doi.org/10.1080/14686996.2017.1401424},
volume = {18},
year = {2017}
}
@article{Hessel2017a,
abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
archivePrefix = {arXiv},
arxivId = {1710.02298},
author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
eprint = {1710.02298},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2017{\_}Rainbow Combining Improvements in Deep Reinforcement Learning{\_}Hessel et al.pdf:pdf},
title = {{Rainbow: Combining Improvements in Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1710.02298},
year = {2017}
}
@article{Zhang2017a,
abstract = {Figure 1: We present a method for everyday eye contact detection. Our method takes images recorded from an off-the-shelf RGB camera close to a target object or person as input. It combines a state-of-the-art appearance-based gaze estimator with a novel approach for unsupervised gaze target discovery, i.e. without the need for tedious and time-consuming manual data annotation. ABSTRACT Eye contact is an important non-verbal cue in social signal processing and promising as a measure of overt attention in human-object interactions and attentive user interfaces. How-ever, robust detection of eye contact across different users, gaze targets, camera positions, and illumination conditions is notoriously challenging. We present a novel method for eye contact detection that combines a state-of-the-art appearance-based gaze estimator with a novel approach for unsupervised gaze target discovery, i.e. without the need for tedious and time-consuming manual data annotation. We evaluate our method in two real-world scenarios: detecting eye contact at the workplace, including on the main work display, from cameras mounted to target objects, as well as during everyday social interactions with the wearer of a head-mounted egocen-tric camera. We empirically evaluate the performance of our method in both scenarios and demonstrate its effectiveness for detecting eye contact independent of target object type and size, camera position, and user and recording environment.},
author = {Zhang, Xucong and Sugano, Yusuke and Bulling, Andreas},
doi = {10.1145/3126594.3126614},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2017{\_}Everyday Eye Contact Detection Using Unsupervised Gaze Target Discovery{\_}Zhang, Sugano, Bulling.pdf:pdf},
isbn = {9781450349819},
keywords = {Appearance-Based Gaze Estimation,Attentive User Interfaces,Eye Contact,Social Signal Processing},
pages = {193--203},
title = {{Everyday Eye Contact Detection Using Unsupervised Gaze Target Discovery}},
year = {2017}
}
@article{Liu2018a,
abstract = {To restore vision for the blind, several prosthetic approaches have been explored that convey raw images to the brain. So far, these schemes all suffer from a lack of bandwidth. An alternate approach would restore vision at the cognitive level, bypassing the need to convey sensory data. A wearable computer captures video and other data, extracts important scene knowledge, and conveys that to the user in compact form. Here, we implement an intuitive user interface for such a device using augmented reality: each object in the environment has a voice and communicates with the user on command. With minimal training, this system supports many aspects of visual cognition: obstacle avoidance, scene understanding, formation and recall of spatial memories, navigation. Blind subjects can traverse an unfamiliar multi-story building on their first attempt. To spur further development in this domain, we developed an open-source environment for standardized benchmarking of visual assistive devices.},
author = {Liu, Yang and Stiles, Noelle RB and Meister, Markus},
doi = {10.7554/elife.37841},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2018{\_}Augmented reality powers a cognitive assistant for the blind{\_}Liu, Stiles, Meister.pdf:pdf},
journal = {eLife},
pages = {1--41},
title = {{Augmented reality powers a cognitive assistant for the blind}},
volume = {7},
year = {2018}
}
@article{Yahya2017,
abstract = {In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative.},
archivePrefix = {arXiv},
arxivId = {arXiv:1610.00673v1},
author = {Yahya, Ali and Li, Adrian and Kalakrishnan, Mrinal and Chebotar, Yevgen and Levine, Sergey},
doi = {10.1109/IROS.2017.8202141},
eprint = {arXiv:1610.00673v1},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2017{\_}Collective robot reinforcement learning with distributed asynchronous guided policy search{\_}Yahya et al.pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {79--86},
title = {{Collective robot reinforcement learning with distributed asynchronous guided policy search}},
volume = {2017-Septe},
year = {2017}
}
@article{Lee2009,
abstract = {This paper proposes an interactive e-learning system using pattern recognition and augmented reality. The goal of proposed system is to provide students with realistic audio-visual contents when they are leaning. The proposed e- learning system consists of image recognition, color and polka-dot pattern recognition, and augmented reality engine with audio-visual contents. When the web camera on a PC captures the current page of textbook, the e-Iearning system first identifies the images on the page, and augments some audio-visual contents on the monitor. For interactive learning, the proposed e-learning system exploits the color-band or polka-dot markers which are stuck to the end of a finger. The color-band and polka-dot marker act like the mouse cursor to indicate the position in the textbook image. Appropriate interactive audio-visual contents are augmented as the marker is located on the predefined image objects in the textbook. The proposed e-learning system was applied to the edu},
author = {Lee, Sang Hwa and Choi, Junyeong and Park, Jong Il and {Sang Hwa}, Lee and Junyeong, Choi and Jong-Il, Park},
doi = {10.1109/TCE.2009.5174470},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2009 - Interactive E-Learning System Using Pattern Recognition and Augmented Reality(2).pdf:pdf;:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2009 - Interactive E-Learning System Using Pattern Recognition and Augmented Reality(3).pdf:pdf},
issn = {00983063},
journal = {IEEE Transactions on Consumer Electronics},
keywords = {AUDIOVISUAL education,Augmented reality,COMPUTER network resources,E-learning system,INTERACTIVE multimedia,Interactive learning,PATTERN perception,Pattern recognition,STUDENTS -- Services for,WEB-based instruction,augmented reality,interactive learning,pattern recognition},
number = {2},
pages = {883--890},
title = {{Interactive E-Learning System Using Pattern Recognition and Augmented Reality.}},
url = {http://search-1ebscohost-1com-1ebsco.han.buw.uw.edu.pl/login.aspx?direct=true{\&}db=a9h{\&}AN=43924116{\&}lang=pl{\&}site=ehost-live},
volume = {55},
year = {2009}
}
@article{Tian2019a,
abstract = {Touch sensing is widely acknowledged to be important for dexterous robotic manipulation, but exploiting tactile sensing for continuous, non-prehensile manipulation is challenging. General purpose control techniques that are able to effectively leverage tactile sensing as well as accurate physics models of contacts and forces remain largely elusive, and it is unclear how to even specify a desired behavior in terms of tactile percepts. In this paper, we take a step towards addressing these issues by combining high-resolution tactile sensing with data-driven modeling using deep neural network dynamics models. We propose deep tactile MPC, a framework for learning to perform tactile servoing from raw tactile sensor inputs, without manual supervision. We show that this method enables a robot equipped with a GelSight-style tactile sensor to manipulate a ball, analog stick, and 20-sided die, learning from unsupervised autonomous interaction and then using the learned tactile predictive model to reposition each object to user-specified configurations, indicated by a goal tactile reading. Videos, visualizations and the code are available here: https://sites.google.com/view/deeptactilempc},
archivePrefix = {arXiv},
arxivId = {1903.04128},
author = {Tian, Stephen and Ebert, Frederik and Jayaraman, Dinesh and Mudigonda, Mayur and Finn, Chelsea and Calandra, Roberto and Levine, Sergey},
eprint = {1903.04128},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2019{\_}Manipulation by Feel Touch-Based Control with Deep Predictive Models{\_}Tian et al.pdf:pdf},
month = {mar},
title = {{Manipulation by Feel: Touch-Based Control with Deep Predictive Models}},
url = {http://arxiv.org/abs/1903.04128},
year = {2019}
}
@article{YannLeCunLeonBottouYoshuaBengio1998,
author = {{Yann LeCun, Leon Bottou, Yoshua Bengio}, Partrick Haffner},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/1998{\_}Gradient-Based Learning Applied to Document Recognition{\_}Yann LeCun, Leon Bottou, Yoshua Bengio.pdf:pdf},
journal = {proc. OF THE IEEE},
title = {{Gradient-Based Learning Applied to Document Recognition}},
year = {1998}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}Deep Residual Learning for Image Recognition{\_}He et al.pdf:pdf},
pages = {1--9},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Gu2017a,
abstract = {Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on offpolicy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efﬁciently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
doi = {10.1109/ICRA.2017.7989385},
eprint = {1610.00633},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper//2017{\_}Deep Reinforcement Learning for Robotic Manipulation{\_}Gu et al.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3389--3396},
pmid = {26774160},
title = {{Deep Reinforcement Learning for Robotic Manipulation}},
year = {2017}
}
@article{Nishi2017a,
abstract = {—The device characteristics of semiconductor lasers have been improved with progress in active layer structures. Carrier confinement dimension plays an important role espe-cially in temperature sensitivity as well as slope efficiency. Three-dimensional carrier confinement to nano-scale semiconductor crystal, known as " quantum dots (QDs) " had been predicted to show ultimately superior device performances. Self-assembly formed InAs QDs grown on GaAs had been intensively promoted in order to achieve QD lasers with superior device performances. Now high-density, high-optical quality QDs have been realized through improved molecular beam epitaxy growths and QD lasers with bet-ter temperature characteristics are in the stage of mass-production for a data-com market. Fabry–Perot type, as well as distributed feedback type QD lasers show quite improved laser characteris-tics. Also, the unique device characteristics of QD lasers opened new application fields such as the use for resource searching by utilizing high-temperature operation such as lasing at higher than 200 °C. For silicon-photonics, QD lasers are used as an optical source for athermal operation. In this paper, the evolution of QDs, as well as improved device performances for novel application fields are discussed.},
author = {Nishi, Kenichi and Takemasa, Keizo and Sugawara, Mitsuru and Arakawa, Yasuhiko},
doi = {10.1109/JSTQE.2017.2699787},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Development of Quantum Dot Lasers for Data-Com and Silicon Photonics Applications{\_}Nishi et al.pdf:pdf},
issn = {21910359},
journal = {IEEE Journal of Selected Topics in Quantum Electronics},
keywords = {InAs,high-temperature operation,quantum dot lasers,quantum dots,silicon-photonics},
number = {6},
title = {{Development of Quantum Dot Lasers for Data-Com and Silicon Photonics Applications}},
volume = {23},
year = {2017}
}
@article{Sabour2017a,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
archivePrefix = {arXiv},
arxivId = {1710.09829},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
eprint = {1710.09829},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2017{\_}Dynamic Routing Between Capsules{\_}Sabour, Nov, Hinton.pdf:pdf},
number = {Nips},
title = {{Dynamic Routing Between Capsules}},
url = {http://arxiv.org/abs/1710.09829},
year = {2017}
}
@article{RonaldJ.WilliamsandDavidZipser1989,
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {{Ronald J. Williams and David Zipser}},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2014{\_}Deep Learning in Neural Networks{\_}Ronald J. Williams and David Zipser.pdf:pdf},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Computation},
number = {No. 2},
pages = {270--280},
pmid = {25462637},
title = {{Deep Learning in Neural Networks}},
volume = {1},
year = {2014}
}
@article{Durugkar2016,
abstract = {Deep reinforcement learning has been shown to be a powerful framework for learning policies from complex high-dimensional sensory inputs to actions in complex tasks, such as the Atari domain. In this paper, we explore output representation modeling in the form of temporal abstraction to improve convergence and reliability of deep reinforcement learning approaches. We concentrate on macro-actions, and evaluate these on different Atari 2600 games, where we show that they yield significant improvements in learning speed. Additionally, we show that they can even achieve better scores than DQN. We offer analysis and explanation for both convergence and final results, revealing a problem deep RL approaches have with sparse reward signals.},
archivePrefix = {arXiv},
arxivId = {1606.04615},
author = {Durugkar, Ishan P. and Rosenbaum, Clemens and Dernbach, Stefan and Mahadevan, Sridhar},
eprint = {1606.04615},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Deep Reinforcement Learning With Macro-Actions{\_}Durugkar et al.pdf:pdf},
pages = {2094--2100},
title = {{Deep Reinforcement Learning With Macro-Actions}},
url = {http://arxiv.org/abs/1606.04615},
year = {2016}
}
@article{Levine2018c,
abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.02199v4},
author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
doi = {10.1177/0278364917710318},
eprint = {arXiv:1603.02199v4},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Levine et al. - 2018 - Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection(8).pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Robotics,deep learning,neural networks},
number = {4-5},
pages = {421--436},
title = {{Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection}},
volume = {37},
year = {2018}
}
@article{Lee2018a,
abstract = {joonhyub.lee | sang-gyun.an | yongkwan.kim | seokhyung.bae @ kaist.ac.kr Figure 1. In a mock setup depicting the flow of Projective Windows, (a) the user wishing to adjust the position and scale of an AR window (b) grabs the window, (c) moves it, (d) makes it bigger by bringing it closer, and (e) projects it to the desired position. ABSTRACT In augmented and virtual reality (AR and VR), there may be many 3D planar windows with 2D texts, images, and videos on them. However, managing the position, orientation, and scale of such a window in an immersive 3D workspace can be difficult. Projective Windows strategically uses the absolute and apparent sizes of the window at various stages of the interaction to enable the grabbing, moving, scaling, and releasing of the window in one continuous hand gesture. With it, the user can quickly and intuitively manage and interact with windows in space without any controller hardware or dedicated widget. Through an evaluation, we demonstrate that our technique is performant and preferable, and that projective geometry plays an important role in the design of spatial user interfaces.},
author = {Lee, Joon Hyub and An, Sang-Gyun and Kim, Yongkwan and Bae, Seok-Hyung},
doi = {10.1145/3170427.3186524},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Projective Windows Bringing Windows in Space to the Fingertip{\_}Lee et al.pdf:pdf},
isbn = {978-1-4503-5621-3},
journal = {Proc. of CHI},
keywords = {3d window management,augmented reality,virtual reality},
pages = {D412:1----D412:1},
title = {{Projective Windows: Bringing Windows in Space to the Fingertip}},
url = {http://doi.acm.org/10.1145/3170427.3186524},
year = {2018}
}
@article{Karambakhsh2019a,
abstract = {Augmented reality is very useful in medical education because of the problem of having body organs in a regular classroom. In this paper, we propose to apply augmented reality to improve the way of teaching in medical schools and institutes. We propose a novel convolutional neural network (CNN) for gesture recognition, which recognizes the human's gestures as a certain instruction. We use augmented reality technology for anatomy learning, which simulates the scenarios where students can learn Anatomy with HoloLens instead of rare specimens. We have used the mesh reconstruction to reconstruct the 3D specimens. A user interface featured augment reality has been designed which fits the common process of anatomy learning. To improve the interaction services, we have applied gestures as an input source and improve the accuracy of gestures recognition by an updated deep convolutional neural network. Our proposed learning method includes many separated train procedures using cloud computing. Each train model and its related inputs have been sent to our cloud and the results are returned to the server. The suggested cloud includes windows and android devices, which are able to install deep convolutional learning libraries. Compared with previous gesture recognition, our approach is not only more accurate but also has more potential for adding new gestures. Furthermore, we have shown that neural networks can be combined with augmented reality as a rising field, and the great potential of augmented reality and neural networks to be employed for medical learning and education systems.},
author = {Karambakhsh, Ahmad and Kamel, Aouaidjia and Sheng, Bin and Li, Ping and Yang, Po and Feng, David Dagan},
doi = {10.1016/j.ijinfomgt.2018.03.004},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Karambakhsh et al. - 2019 - Deep gesture interaction for augmented anatomy learning(2).pdf:pdf},
issn = {02684012},
journal = {International Journal of Information Management},
keywords = {3D reconstruction,Augmented reality,Medical education,Mobile cloud,Neural network},
number = {October 2017},
pages = {328--336},
title = {{Deep gesture interaction for augmented anatomy learning}},
volume = {45},
year = {2019}
}
@book{Francois-Lavet2018,
abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
archivePrefix = {arXiv},
arxivId = {1811.12560},
author = {Francois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
doi = {10.1561/2200000071},
eprint = {1811.12560},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}An Introduction to Deep Reinforcement Learning{\_}Francois-Lavet et al.pdf:pdf},
isbn = {9783319994925},
keywords = {deep learning,deep reinforcement learning,machine learning,reinforcement learning},
pages = {298--328},
title = {{An Introduction to Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1811.12560{\%}0Ahttp://dx.doi.org/10.1561/2200000071},
year = {2018}
}
@article{Kimura2018a,
abstract = {We propose a system, called ExtVision, to augment visual experiences by generating and projecting context-images onto the periphery of the television or computer screen. A peripheral projection of the context-image is one of the most effective techniques to enhance visual experiences. However, the projection is not commonly used at present, because of the difficulty in preparing the context-image. In this paper, we propose a deep neural network-based method to generate context-images for peripheral projection. A user study was performed to investigate the manner in which the proposed system augments traditional visual experiences. In addition, we present applications and future prospects of the developed system.},
author = {Kimura, Naoki and Rekimoto, Jun},
doi = {10.1145/3173574.3174001},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}ExtVision Aumentation of Visual Experiences with Generation of Context Images for Peripheral Vision Using Deep Neural Network{\_}Kimur.pdf:pdf},
isbn = {9781450356206},
journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI '18},
pages = {1--10},
title = {{ExtVision: Aumentation of Visual Experiences with Generation of Context Images for Peripheral Vision Using Deep Neural Network}},
url = {http://dl.acm.org/citation.cfm?doid=3173574.3174001},
year = {2018}
}
@article{OpenAI2018b,
abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM},
archivePrefix = {arXiv},
arxivId = {1808.00177},
author = {OpenAI and Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
eprint = {1808.00177},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/OpenAI et al. - 2018 - Learning Dexterous In-Hand Manipulation.pdf:pdf},
pages = {1--27},
title = {{Learning Dexterous In-Hand Manipulation}},
url = {http://arxiv.org/abs/1808.00177},
year = {2018}
}
@article{Ha2018,
abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper at https://worldmodels.github.io},
archivePrefix = {arXiv},
arxivId = {1809.01999},
author = {Ha, David and Schmidhuber, J{\"{u}}rgen},
eprint = {1809.01999},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Ha, Schmidhuber - 2018 - Recurrent World Models Facilitate Policy Evolution(2).pdf:pdf},
month = {sep},
title = {{Recurrent World Models Facilitate Policy Evolution}},
url = {http://arxiv.org/abs/1809.01999},
year = {2018}
}
@book{Agostinelli2018,
author = {Agostinelli, Forest and Hocquet, Guillaume and Singh, Sameer and B, Pierre Baldi},
doi = {10.1007/978-3-319-99492-5},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Braverman Readings in Machine Learning. Key Ideas from Inception to Current State{\_}Agostinelli et al.pdf:pdf},
isbn = {978-3-319-99491-8},
keywords = {Machine learning,Reinforcement learning,Deep learn,deep learning,deep reinforcement learning,machine learning,reinforcement learning},
pages = {298--328},
publisher = {Springer International Publishing},
title = {{Braverman Readings in Machine Learning. Key Ideas from Inception to Current State}},
url = {http://link.springer.com/10.1007/978-3-319-99492-5},
volume = {11100},
year = {2018}
}
@article{Hilliges2012,
abstract = {Abstract HoloDesk is an interactive system combining an optical see through display and Kinect camera to create the illusion that users are directly interacting with 3D graphics. A virtual image of a 3D scene is rendered through a half silvered mirror and spatially aligned ... $\backslash$n},
author = {Hilliges, Otmar and Kim, David and Izadi, Shahram and Weiss, Malte and Wilson, Andrew},
doi = {10.1145/2207676.2208405},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Hilliges et al. - 2012 - HoloDesk Direct 3D Interactions with a Situated See-Through Display Otmar(2).pdf:pdf},
isbn = {9781450310154},
journal = {Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems - CHI '12},
pages = {2421},
title = {{HoloDesk: Direct 3D Interactions with a Situated See-Through Display Otmar}},
url = {http://dl.acm.org/citation.cfm?doid=2207676.2208405},
year = {2012}
}
@article{Kimura2018,
abstract = {We propose a system, called ExtVision, to augment visual experiences by generating and projecting context-images onto the periphery of the television or computer screen. A peripheral projection of the context-image is one of the most effective techniques to enhance visual experiences. However, the projection is not commonly used at present, because of the difficulty in preparing the context-image. In this paper, we propose a deep neural network-based method to generate context-images for peripheral projection. A user study was performed to investigate the manner in which the proposed system augments traditional visual experiences. In addition, we present applications and future prospects of the developed system.},
author = {Kimura, Naoki and Rekimoto, Jun},
doi = {10.1145/3173574.3174001},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}ExtVision Augmentation of Visual Experiences with Generation of Context Images for Peripheral Vision Using Deep Neural Network{\_}Kimu.pdf:pdf},
isbn = {9781450356206},
pages = {1--10},
title = {{ExtVision: Augmentation of Visual Experiences with Generation of Context Images for Peripheral Vision Using Deep Neural Network}},
year = {2018}
}
@article{Chu2018,
abstract = {Neural networks have recently become good at engaging in dialog. However, current approaches are based solely on verbal text, lacking the richness of a real face-to-face conversation. We propose a neural conversation model that aims to read and generate facial gestures alongside with text. This allows our model to adapt its response based on the "mood" of the conversation. In particular, we introduce an RNN encoder-decoder that exploits the movement of facial muscles, as well as the verbal conversation. The decoder consists of two layers, where the lower layer aims at generating the verbal response and coarse facial expressions, while the second layer fills in the subtle gestures, making the generated output more smooth and natural. We train our neural network by having it "watch" 250 movies. We showcase our joint face-text model in generating more natural conversations through automatic metrics and a human study. We demonstrate an example application with a face-to-face chatting avatar.},
author = {Chu, Hang and Li, Daiqing and Fidler, Sanja},
doi = {10.1109/CVPR.2018.00743},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Chu, Li, Fidler - 2018 - A Face-to-Face Neural Conversation Model(2).pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {7113--7121},
title = {{A Face-to-Face Neural Conversation Model}},
year = {2018}
}
@article{Hembury2003,
abstract = {How does the brain learn to recognize objects visually, and perform this difficult feat robustly in the face of many sources of ambiguity and variability? We present a computational model based on the biology of the relevant visual pathways that learns to reliably recognize 100 different object categories in the face of naturally occurring variability in location, rotation, size, and lighting. The model exhibits robustness to highly ambiguous, partially occluded inputs. Both the unified, biologically plausible learning mechanism and the robustness to occlusion derive from the role that recurrent connectivity and recurrent processing mechanisms play in the model. Furthermore, this interaction of recurrent connectivity and learning predicts that high-level visual representations should be shaped by error signals from nearby, associated brain areas over the course of visual learning. Consistent with this prediction, we show how semantic knowledge about object categories changes the nature of their learned visual representations, as well as how this representational shift supports the mapping between perceptual and conceptual knowledge. Altogether, these findings support the potential importance of ongoing recurrent processing throughout the brain's visual system and suggest ways in which object recognition can be understood in terms of interactions within and between processes over time.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Hembury, Guy A. and Borovkov, Victor V. and Lintuluoto, Juha M. and Inoue, Yoshihisa},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2003{\_}Deep Residual Learning for Image Recognition Kaiming{\_}Hembury et al.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {Chemistry Letters},
keywords = {2003,32,5,a -oxo,an acid,anti conformational switching in,base controlled molecular switch,chemistry letters vol,no,syn},
number = {5},
pages = {428--429},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition Kaiming}},
url = {http://joi.jlc.jst.go.jp/JST.JSTAGE/cl/2003.428?from=CrossRef},
volume = {32},
year = {2003}
}
@article{.2554,
author = {久保田遥},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Learning Deep Features for Discriminative Localization{\_}久保田遥.pdf:pdf},
title = {{Learning Deep Features for Discriminative Localization}},
year = {2016}
}
@article{Harwath2017,
abstract = {Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the word 'lighthouse' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.},
archivePrefix = {arXiv},
arxivId = {1701.07481},
author = {Harwath, David and Glass, James R.},
eprint = {1701.07481},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Harwath, Glass - 2017 - Learning Word-Like Units from Joint Audio-Visual Analysis(2).pdf:pdf},
title = {{Learning Word-Like Units from Joint Audio-Visual Analysis}},
url = {http://arxiv.org/abs/1701.07481},
year = {2017}
}
@article{Lee2018,
author = {Lee, Joon Hyub and An, Sang-Gyun and Kim, Yongkwan and Bae, Seok-Hyung},
doi = {10.1145/3170427.3186524},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Projective Windows Bringing Windows in Space to the Fingertip{\_}Lee et al(2).pdf:pdf},
isbn = {9781450356206},
pages = {1--1},
title = {{Projective Windows: Bringing Windows in Space to the Fingertip}},
year = {2018}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search(2).pdf:pdf},
issn = {1476-4687},
journal = {Nature},
number = {7587},
pages = {484--9},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26819042},
volume = {529},
year = {2016}
}
@article{Lee2009,
abstract = {This paper proposes an interactive e-learning system using pattern recognition and augmented reality. The goal of proposed system is to provide students with realistic audio-visual contents when they are leaning. The proposed e-learning system consists of image recognition, color and polka-dot pattern recognition, and augmented reality engine with audio-visual contents. When the Web camera on a PC captures the current page of textbook, the e-learning system first identifies the images on the page, and augments some audio-visual contents on the monitor. For interactive learning, the proposed e-learning system exploits the color-band or polka-dot markers which are stuck to the end of a finger. The color-band and polka-dot marker act like the mouse cursor to indicate the position in the textbook image. Appropriate interactive audio-visual contents are augmented as the marker is located on the predefined image objects in the textbook. The proposed e-learning system was applied to the educational courses in the elementary school, and we obtained satisfactory results for real applications. We expect that the proposed e-learning system is popular when the educational contents and scenarios are sufficiently provided.},
author = {Lee, Sang Hwa and Choi, Junyeong and Park, Jong Il},
doi = {10.1109/TCE.2009.5174470},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2009 - Interactive E-Learning System Using Pattern Recognition and Augmented Reality(3).pdf:pdf},
issn = {00983063},
journal = {IEEE Transactions on Consumer Electronics},
keywords = {Augmented reality,E-learning system,Interactive learning,Pattern recognition},
number = {2},
pages = {883--890},
title = {{Interactive e-learning system using pattern recognition and augmented reality}},
volume = {55},
year = {2009}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}Deep Learning in neural networks An overview{\_}Schmidhuber.pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
pages = {85--117},
publisher = {Elsevier Ltd},
title = {{Deep Learning in neural networks: An overview}},
url = {http://dx.doi.org/10.1016/j.neunet.2014.09.003},
volume = {61},
year = {2015}
}
@article{Hasselt20016,
abstract = {Deep reinforcement learning has been shown to be a powerful framework for learning policies from complex high-dimensional sensory inputs to actions in complex tasks, such as the Atari domain. In this paper, we explore output representation modeling in the form of temporal abstraction to improve convergence and reliability of deep reinforcement learning approaches. We concentrate on macro-actions, and evaluate these on different Atari 2600 games, where we show that they yield significant improvements in learning speed. Additionally, we show that they can even achieve better scores than DQN. We offer analysis and explanation for both convergence and final results, revealing a problem deep RL approaches have with sparse reward signals.},
archivePrefix = {arXiv},
arxivId = {1606.04615},
author = {Durugkar, Ishan P. and Rosenbaum, Clemens and Dernbach, Stefan and Mahadevan, Sridhar},
eprint = {1606.04615},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Double DQN.pdf{\_}Hasselt, Guez, Silver.pdf:pdf},
journal = {Aaai},
keywords = {Technical Papers: Machine Learning Methods},
pages = {2094--2100},
title = {{Deep Reinforcement Learning With Macro-Actions}},
url = {http://arxiv.org/abs/1606.04615},
year = {2016}
}
@article{Levine2018b,
abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
doi = {10.1177/0278364917710318},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection{\_}Levine et al.{\_}Internation{\_}ijrr.pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Robotics,deep learning,neural networks},
number = {4-5},
pages = {421--436},
title = {{Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection}},
volume = {37},
year = {2018}
}
@article{Sankai2019,
author = {Toyama, Hiroaki and Kawamoto, Hiroaki and Sankai, Yoshiyuki},
doi = {10.1109/embc.2019.8857106},
file = {:Users/atsushi/Downloads/08857106.pdf:pdf},
isbn = {9781538613115},
issn = {1557170X},
journal = {2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
pages = {1645--1650},
publisher = {IEEE},
title = {{Development of Cybernic Robot Arm to realize Support Action cooperated with Hemiplegic Person's Arm}},
year = {2019}
}
@article{HSR2019,
abstract = {There has been an increasing interest in mobile manipulators that are capable of performing physical work in living spaces worldwide, corresponding to an aging population with declining birth rates with the expectation of improving quality of life (QoL). Research and development is a must in intelligent sensing and software which will enable advanced recognition, judgment, and motion to realize household work by robots. In order to accelerate this research, we have developed a compact and safe research platform, Human Support Robot (HSR), which can be operated in an actual home environment. We assume that overall R{\&}D will accelerate by using a common robot platform among many researchers since that enables them to share their research results. Currently, the number of HSR users is expanding to 33 sites in 8 countries worldwide (as of February 15, 2018). Software and technical knowledge of all users is shared through a community website. HSR has been adopted as a standard platform for international robot competitions such as RoboCup@Home and World Robot Summit (WRS). HSR is provided to participants of those competitions through public offering. In this paper, we describe HSR's development background, and technical detail of its hardware and software. Specifically, we describe its omnidirectional mobile base using the dual-wheel caster-drive mechanism, which is the basis of HSR's operational movement and a novel whole body motion control system. Finally, we describe the results of utilization in RoboCup@Home and field tests in order to demonstrate the effect of introducing the platform.},
author = {Yamamoto, Takashi and Terada, Koji and Ochiai, Akiyoshi and Saito, Fuminori and Asahara, Yoshiaki and Murase, Kazuto},
doi = {10.1186/s40648-019-0132-3},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Yamamoto et al. - 2019 - Development of Human Support Robot as the research platform of a domestic mobile manipulator.pdf:pdf},
issn = {21974225},
journal = {ROBOMECH Journal},
keywords = {Domestic robots,Mobile manipulation,Motion and path planning},
number = {1},
title = {{Development of Human Support Robot as the research platform of a domestic mobile manipulator}},
volume = {6},
year = {2019}
}
@article{Toyama2018a,
abstract = {{\textcopyright} 2017 IEEE. Hemiplegic persons have difficulties in handling many kinds of tasks in daily life because they must perform the tasks by only the unaffected arm and hand. In order to realize upper limb work support for hemiplegic persons, a robot arm that has ability to grip various daily necessities and cooperate with the nonparalyzed side in same work space is required. The purposes of this study is to develop a novel assistance system "My Cybernic Robot Arm" that has measurement function and adjustment function of grip force, and to confirm the basic performance of the system through the benchmark experiments. The system is composed of a robot arm and hand, a sensing unit and a control unit. The system has grip force adjustment function and learning function of grip force by grip force teaching playback. The robot arm has six degrees of freedom and was designed with minimum size to cover an area of the table tasks so as not to narrow the work area of the nonparalyzed side. The sensing unit can measure the grip force and centroid position of grip force calculated by using a capacitance feature. In addition, developed sensing unit is small enough to be embedded in the fingertip of robot hand. We conducted verification of the sensing unit accuracy and grip force teaching and reproduction as basic experiments to confirm the basic performance of the system. The results showed that the system could measure grip force and load center of gravity position. In addition, it was confirmed that the system could learn the appropriate grip force and adjust the grip force by grip force teaching playback. In conclusion, the system has basic performance because it can grip the object with appropriate grip force. This system is expected to support upper limb work of hemiplegic persons such as desk work in everyday life, and improve QOL (Quality of life) and encourage independence.},
author = {Toyama, Hiroaki and Kawamoto, Hiroaki and Sankai, Yoshiyuki},
doi = {10.1109/ROBIO.2017.8324526},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Basic research of upper limb work support system My cybernic robot arm for hemiplegic persons{\_}Toyama, Kawamoto, Sankai.pdf:pdf},
isbn = {9781538637418},
journal = {2017 IEEE International Conference on Robotics and Biomimetics, ROBIO 2017},
keywords = {Grip force control,Grip force teaching,Load center of gravity position,Nonuniform load},
pages = {1--7},
title = {{Basic research of upper limb work support system "My cybernic robot arm" for hemiplegic persons}},
volume = {2018-Janua},
year = {2018}
}
@article{Weidner2013,
abstract = {Due to the volatile conditions in today's production, flexible assembly systems are required. However, current plants are often custom-made which are designed for a fix product spectrum. Especially for the assembly of unique products or products with a small-scale lot size and large-scale lot size with a high diversity of variants, many manufacturing steps are difficult or non economic to automate. Thus, a large part of the manufacturing processes is to be performed manually. These can be supported by assistance systems, but appropriate systems are seldom available. A new approach for supporting manual assembly tasks is the hybridization of biological and technical systems, a so-called "Human Hybrid Robot" (HHR). The kinematic chains of human, machines and tools are configured task-depended in serial and parallel arrangement. By doing so, the individual skills are used optimally. Main focus of this concept is improving the assembly accuracy and error prevention to boost the overall quality of the assembly processes. This paper presents the theoretical concept. Possible applications and realization of exemplary components are outlined to show the potential of HHR. {\textcopyright} 2013 German Academic Society for Production Engineering (WGP).},
author = {Weidner, R. and Kong, N. and Wulfsberg, J. P.},
doi = {10.1007/s11740-013-0487-x},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Weidner, Kong, Wulfsberg - 2013 - Human Hybrid Robot A new concept for supporting manual assembly tasks.pdf:pdf},
issn = {09446524},
journal = {Production Engineering},
keywords = {Assembly,Assistance system,HHR,Human robot integration,Human-robot-cooperation/interaction,Hybrid kinematics},
number = {6},
pages = {675--684},
title = {{Human Hybrid Robot: A new concept for supporting manual assembly tasks}},
volume = {7},
year = {2013}
}
@article{リハビリテーション,
abstract = {Rehabilitation using myoelectric prosthesis for trans-radial amputees has become wide spread and well established in several developed countries. However, the clinical use of myoelectric prostheses for trans-radial amputees has not yet spread in Japan. It is well known that once amputees become accustomed to using their prosthesis efficiently through adequate reha- bilitation, that various activities which the amputees had given up so far will become possible through enhanced bimanual activities. Although myoelectric prostheses have proved to be useful, the majority of amputees have not been satisfied with their function. As an amputee becomes a better user, they request not only simple tasks but also complicated ones. As a consequence, the amputee comes to know the limits of their myoelectric prosthesis, thus expectations for superior prostheses will arise. The recent remarkable development of engineering technology has enabled the progress of prosthetic limb technology, leading to the production of far superior functional prostheses which meet the user's expectations. However, there is a paradox in developing such superior prostheses. The more advanced the prosthesis we produce, the higher the cost. To achieve this end, it is absolutely imperative to secure the cooperation of both clinicians and engi- neers. Furthermore, a rehabilitation strategy for patients with a higher level of amputation （trans- humeral amputation, shoulder disarticulation） remains unsolved. In this paper, we propose a “Hybrid Myoelectric Prosthesis”, which consists of a myoelectric hand as a terminal device and a body-powered active elbow joint, as a realistic solution for higher level amputees. In addition, we introduce Targeted Reinnervation （TR） as a future strategy for reference.},
author = {陳, 隆明},
doi = {10.2490/jjrmc.49.31},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/CHIN - 2012 - Myoelectric Prostheses Current Status and Problems to be Solved, a Rehabilitation Strategy for Higher Level Amputation (2).pdf:pdf},
issn = {1881-3526},
journal = {The Japanese Journal of Rehabilitation Medicine},
number = {1},
pages = {31--36},
title = {筋電義手普及の現状と課題，高位切断者に対する戦略，そして今後の展望},
volume = {49},
year = {2012}
}
@article{fasterRCNN,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Ren et al. - 2017 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
month = {jun},
number = {6},
pages = {1137--1149},
pmid = {27295650},
publisher = {IEEE Computer Society},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
volume = {39},
year = {2017}
}
@article{Yamamoto2019b,
abstract = {There has been an increasing interest in mobile manipulators that are capable of performing physical work in living spaces worldwide, corresponding to an aging population with declining birth rates with the expectation of improving quality of life (QoL). We assume that overall research and development will accelerate by using a common robot platform among a lot of researchers since that enables them to share their research results. Therefore we have developed a compact and safe research platform, Human Support Robot (HSR), which can be operated in an actual home environment and we have provided it to various research institutes to establish the developers community. Currently, the number of HSR users is expanding to 44 sites in 12 countries worldwide (as of November 30th, 2018). To activate the community, we assume that the robot competition will be effective. As a result of international public offering, HSR has been adopted as a standard platform for international robot competitions such as RoboCup@Home and World Robot Summit (WRS). HSR is provided to participants of those competitions. In this paper, we describe HSR's development background since 2006, and technical detail of hardware design and software architecture. Specifically, we describe its omnidirectional mobile base using the dual-wheel caster-drive mechanism, which is the basis of HSR's operational movement and a novel whole body motion control system. Finally, we describe the verification of autonomous task capability and the results of utilization in RoboCup@Home in order to demonstrate the effect of introducing the platform.},
author = {Yamamoto, Takashi and Terada, Koji and Ochiai, Akiyoshi and Saito, Fuminori and Asahara, Yoshiaki and Murase, Kazuto},
doi = {10.1186/s40648-019-0132-3},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Development of the Research Platform of a Domestic Mobile Manipulator Utilized for International Competition and Field Test{\_}Yama(2).pdf:pdf},
issn = {21974225},
journal = {ROBOMECH Journal},
keywords = {Domestic robots,Mobile manipulation,Motion and path planning},
number = {1},
title = {{Development of Human Support Robot as the research platform of a domestic mobile manipulator}},
volume = {6},
year = {2019}
}
@article{Yamamoto2019,
abstract = {There has been an increasing interest in mobile manipulators that are capable of performing physical work in living spaces worldwide, corresponding to an aging population with declining birth rates with the expectation of improving quality of life (QoL). Research and development is a must in intelligent sensing and software which will enable advanced recognition, judgment, and motion to realize household work by robots. In order to accelerate this research, we have developed a compact and safe research platform, Human Support Robot (HSR), which can be operated in an actual home environment. We assume that overall R{\&}D will accelerate by using a common robot platform among many researchers since that enables them to share their research results. Currently, the number of HSR users is expanding to 33 sites in 8 countries worldwide (as of February 15, 2018). Software and technical knowledge of all users is shared through a community website. HSR has been adopted as a standard platform for international robot competitions such as RoboCup@Home and World Robot Summit (WRS). HSR is provided to participants of those competitions through public offering. In this paper, we describe HSR's development background, and technical detail of its hardware and software. Specifically, we describe its omnidirectional mobile base using the dual-wheel caster-drive mechanism, which is the basis of HSR's operational movement and a novel whole body motion control system. Finally, we describe the results of utilization in RoboCup@Home and field tests in order to demonstrate the effect of introducing the platform.},
author = {Yamamoto, Takashi and Terada, Koji and Ochiai, Akiyoshi and Saito, Fuminori and Asahara, Yoshiaki and Murase, Kazuto},
doi = {10.1109/IROS.2018.8593798},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Yamamoto et al. - 2019 - Development of the Research Platform of a Domestic Mobile Manipulator Utilized for International Competition an.pdf:pdf;:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Development of the Research Platform of a Domestic Mobile Manipulator Utilized for International Competition and Field Test{\_}Yamamot.pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Artificial Intelligence,Computational Intelligence,Control and Systems Theory,Mechatronics,Robotics and Automation},
month = {dec},
number = {1},
pages = {7675--7682},
publisher = {SpringerOpen},
title = {{Development of the Research Platform of a Domestic Mobile Manipulator Utilized for International Competition and Field Test}},
url = {https://robomechjournal.springeropen.com/articles/10.1186/s40648-019-0132-3},
volume = {6},
year = {2018}
}
@article{Yamamoto2019,
abstract = {There has been an increasing interest in mobile manipulators that are capable of performing physical work in living spaces worldwide, corresponding to an aging population with declining birth rates with the expectation of improving quality of life (QoL). Research and development is a must in intelligent sensing and software which will enable advanced recognition, judgment, and motion to realize household work by robots. In order to accelerate this research, we have developed a compact and safe research platform, Human Support Robot (HSR), which can be operated in an actual home environment. We assume that overall R{\&}D will accelerate by using a common robot platform among many researchers since that enables them to share their research results. Currently, the number of HSR users is expanding to 33 sites in 8 countries worldwide (as of February 15, 2018). Software and technical knowledge of all users is shared through a community website. HSR has been adopted as a standard platform for international robot competitions such as RoboCup@Home and World Robot Summit (WRS). HSR is provided to participants of those competitions through public offering. In this paper, we describe HSR's development background, and technical detail of its hardware and software. Specifically, we describe its omnidirectional mobile base using the dual-wheel caster-drive mechanism, which is the basis of HSR's operational movement and a novel whole body motion control system. Finally, we describe the results of utilization in RoboCup@Home and field tests in order to demonstrate the effect of introducing the platform.},
author = {Yamamoto, Takashi and Terada, Koji and Ochiai, Akiyoshi and Saito, Fuminori and Asahara, Yoshiaki and Murase, Kazuto},
doi = {10.1109/iros.2018.8593798},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Yamamoto et al. - 2019 - Development of the Research Platform of a Domestic Mobile Manipulator Utilized for International Competition an.pdf:pdf},
issn = {2197-4225},
journal = {ROBOMECH Journal},
keywords = {Artificial Intelligence,Computational Intelligence,Control and Systems Theory,Mechatronics,Robotics and Automation},
month = {dec},
number = {1},
pages = {7675--7682},
publisher = {SpringerOpen},
title = {{Development of the Research Platform of a Domestic Mobile Manipulator Utilized for International Competition and Field Test}},
url = {https://robomechjournal.springeropen.com/articles/10.1186/s40648-019-0132-3},
volume = {6},
year = {2019}
}
@article{Mielke2020,
abstract = {Human teams are able to easily perform collaborative manipulation tasks. However, for a robot and human to simultaneously manipulate an extended object is a difficult task using existing methods from the literature. Our approach in this paper is to use data from human-human dyad experiments to determine motion intent which we use for a physical human-robot co-manipulation task. We first present and analyze data from human-human dyads performing co-manipulation tasks. We show that our human-human dyad data has interesting trends including that interaction forces are non-negligible compared to the force required to accelerate an object and that the beginning of a lateral movement is characterized by distinct torque triggers from the leader of the dyad. We also examine different metrics to quantify performance of different dyads. We also develop a deep neural network based on motion data from human-human trials to predict human intent based on past motion. We then show how force and motion data can be used as a basis for robot control in a human-robot dyad. Finally, we compare the performance of two controllers for human-robot co-manipulation to human-human dyad performance.},
archivePrefix = {arXiv},
arxivId = {2001.00991},
author = {Mielke, Erich and Townsend, Eric and Wingate, David and Killpack, Marc D.},
eprint = {2001.00991},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Mielke et al. - 2020 - Human-robot co-manipulation of extended objects Data-driven models and control from analysis of human-human dyads.pdf:pdf},
month = {jan},
title = {{Human-robot co-manipulation of extended objects: Data-driven models and control from analysis of human-human dyads}},
url = {http://arxiv.org/abs/2001.00991},
year = {2020}
}
@article{Fan2018,
abstract = {In this paper, we present a decentralized sensor-level collision avoidance policy for multi-robot systems, which shows promising results in practical applications. In particular, our policy directly maps raw sensor measurements to an agent's steering commands in terms of the movement velocity. As a first step toward reducing the performance gap between decentralized and centralized methods, we present a multi-scenario multi-stage training framework to learn an optimal policy. The policy is trained over a large number of robots in rich, complex environments simultaneously using a policy gradient based reinforcement learning algorithm. The learning algorithm is also integrated into a hybrid control framework to further improve the policy's robustness and effectiveness. We validate the learned sensor-level collision avoidance policy in a variety of simulated and real-world scenarios with thorough performance evaluations for large-scale multi-robot systems. The generalization of the learned policy is verified in a set of unseen scenarios including the navigation of a group of heterogeneous robots and a large-scale scenario with 100 robots. Although the policy is trained using simulation data only, we have successfully deployed it on physical robots with shapes and dynamics characteristics that are different from the simulated agents, in order to demonstrate the controller's robustness against the sim-to-real modeling error. Finally, we show that the collision-avoidance policy learned from multi-robot navigation tasks provides an excellent solution to the safe and effective autonomous navigation for a single robot working in a dense real human crowd. Our learned policy enables a robot to make effective progress in a crowd without getting stuck. Videos are available at https://sites.google.com/view/hybridmrca},
archivePrefix = {arXiv},
arxivId = {1808.03841},
author = {Fan, Tingxiang and Long, Pinxin and Liu, Wenxi and Pan, Jia},
eprint = {1808.03841},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Fan et al. - 2018 - Fully Distributed Multi-Robot Collision Avoidance via Deep Reinforcement Learning for Safe and Efficient Navigation.pdf:pdf},
month = {aug},
title = {{Fully Distributed Multi-Robot Collision Avoidance via Deep Reinforcement Learning for Safe and Efficient Navigation in Complex Scenarios}},
url = {http://arxiv.org/abs/1808.03841},
year = {2018}
}
@article{Zeng2018,
abstract = {This article presents a robotic pick-and-place system that is capable of grasping and recognizing both known and novel objects in cluttered environments. The key new feature of the system is that it handles a wide range of object categories without needing any task-specific training data for novel objects. To achieve this, it first uses an object-agnostic grasping framework to map from visual observations to actions: inferring dense pixel-wise probability maps of the affordances for four different grasping primitive actions. It then executes the action with the highest affordance and recognizes picked objects with a cross-domain image classification framework that matches observed images to product images. Since product images are readily available for a wide range of objects (e.g., from the web), the system works out-of-the-box for novel objects without requiring any additional data collection or re-training. Exhaustive experimental results demonstrate that our multi-affordance grasping achieves high success rates for a wide variety of objects in clutter, and our recognition algorithm achieves high accuracy for both known and novel grasped objects. The approach was part of the MIT–Princeton Team system that took first place in the stowing task at the 2017 Amazon Robotics Challenge. All code, datasets, and pre-trained models are available online at http://arc.cs.princeton.edu/},
archivePrefix = {arXiv},
arxivId = {1710.01330},
author = {Zeng, Andy and Song, Shuran and Yu, Kuan Ting and Donlon, Elliott and Hogan, Francois R. and Bauza, Maria and Ma, Daolin and Taylor, Orion and Liu, Melody and Romo, Eudald and Fazeli, Nima and Alet, Ferran and Dafle, Nikhil Chavan and Holladay, Rachel and Morena, Isabella and {Qu Nair}, Prem and Green, Druck and Taylor, Ian and Liu, Weber and Funkhouser, Thomas and Rodriguez, Alberto},
doi = {10.1109/ICRA.2018.8461044},
eprint = {1710.01330},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Zeng et al. - 2017 - Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance Grasping and Cross-Domain Image Matching.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
month = {oct},
pages = {3750--3757},
title = {{Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching}},
url = {http://arxiv.org/abs/1710.01330},
year = {2018}
}
@inproceedings{Sankai2017,
abstract = {{\textcopyright} 2017 IEEE. Hemiplegic persons have difficulties in handling many kinds of tasks in daily life because they must perform the tasks by only the unaffected arm and hand. In order to realize upper limb work support for hemiplegic persons, a robot arm that has ability to grip various daily necessities and cooperate with the nonparalyzed side in same work space is required. The purposes of this study is to develop a novel assistance system "My Cybernic Robot Arm" that has measurement function and adjustment function of grip force, and to confirm the basic performance of the system through the benchmark experiments. The system is composed of a robot arm and hand, a sensing unit and a control unit. The system has grip force adjustment function and learning function of grip force by grip force teaching playback. The robot arm has six degrees of freedom and was designed with minimum size to cover an area of the table tasks so as not to narrow the work area of the nonparalyzed side. The sensing unit can measure the grip force and centroid position of grip force calculated by using a capacitance feature. In addition, developed sensing unit is small enough to be embedded in the fingertip of robot hand. We conducted verification of the sensing unit accuracy and grip force teaching and reproduction as basic experiments to confirm the basic performance of the system. The results showed that the system could measure grip force and load center of gravity position. In addition, it was confirmed that the system could learn the appropriate grip force and adjust the grip force by grip force teaching playback. In conclusion, the system has basic performance because it can grip the object with appropriate grip force. This system is expected to support upper limb work of hemiplegic persons such as desk work in everyday life, and improve QOL (Quality of life) and encourage independence.},
author = {Toyama, Hiroaki and Kawamoto, Hiroaki and Sankai, Yoshiyuki},
booktitle = {2017 IEEE International Conference on Robotics and Biomimetics, ROBIO 2017},
doi = {10.1109/ROBIO.2017.8324526},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Toyama, Kawamoto, Sankai - 2018 - Basic research of upper limb work support system My cybernic robot arm for hemiplegic persons(2).pdf:pdf},
isbn = {9781538637418},
keywords = {Grip force control,Grip force teaching,Load center of gravity position,Nonuniform load},
month = {mar},
pages = {1--7},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Basic research of upper limb work support system "My cybernic robot arm" for hemiplegic persons}},
volume = {2018-Janua},
year = {2018}
}
@article{Barreto2016,
abstract = {Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. We propose a transfer framework for the scenario where the reward function changes between tasks but the environment's dynamics remain the same. Our approach rests on two key ideas: "successor features", a value function representation that decouples the dynamics of the environment from the rewards, and "generalized policy improvement", a generalization of dynamic programming's policy improvement operation that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows the free exchange of information across tasks. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice, significantly outperforming alternative methods in a sequence of navigation tasks and in the control of a simulated robotic arm.},
archivePrefix = {arXiv},
arxivId = {1606.05312},
author = {Barreto, Andr{\'{e}} and Dabney, Will and Munos, R{\'{e}}mi and Hunt, Jonathan J. and Schaul, Tom and van Hasselt, Hado and Silver, David},
eprint = {1606.05312},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Barreto et al. - 2016 - Successor Features for Transfer in Reinforcement Learning.pdf:pdf},
month = {jun},
title = {{Successor Features for Transfer in Reinforcement Learning}},
url = {http://arxiv.org/abs/1606.05312},
year = {2016}
}
@article{Zeng2018a,
abstract = {Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors amid challenging cases of clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after only a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects. Qualitative results (videos), code, pre-trained models, and simulation environments are available at http://vpg.cs.princeton.edu},
archivePrefix = {arXiv},
arxivId = {1803.09956},
author = {Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
doi = {10.1109/IROS.2018.8593986},
eprint = {1803.09956},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Zeng et al. - 2018 - Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning.pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
month = {mar},
pages = {4238--4245},
title = {{Learning Synergies between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1803.09956},
year = {2018}
}
@article{Fu2019,
abstract = {Recently two-stage detectors have surged ahead of single-shot detectors in the accuracy-vs-speed trade-off. Nevertheless single-shot detectors are immensely popular in embedded vision applications. This paper brings single-shot detectors up to the same level as current two-stage techniques. We do this by improving training for the state-of-the-art single-shot detector, RetinaNet, in three ways: integrating instance mask prediction for the first time, making the loss function adaptive and more stable, and including additional hard examples in training. We call the resulting augmented network RetinaMask. The detection component of RetinaMask has the same computational cost as the original RetinaNet, but is more accurate. COCO test-dev results are up to 41.4 mAP for RetinaMask-101 vs 39.1mAP for RetinaNet-101, while the runtime is the same during evaluation. Adding Group Normalization increases the performance of RetinaMask-101 to 41.7 mAP. Code is at:https://github.com/chengyangfu/retinamask},
archivePrefix = {arXiv},
arxivId = {1901.03353},
author = {Fu, Cheng-Yang and Shvets, Mykhailo and Berg, Alexander C.},
eprint = {1901.03353},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Fu, Shvets, Berg - 2019 - RetinaMask Learning to predict masks improves state-of-the-art single-shot detection for free.pdf:pdf},
month = {jan},
title = {{RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free}},
url = {http://arxiv.org/abs/1901.03353},
year = {2019}
}
@article{Quillen2018,
abstract = {In this paper, we explore deep reinforcement learning algorithms for vision-based robotic grasping. Model-free deep reinforcement learning (RL) has been successfully applied to a range of challenging environments, but the proliferation of algorithms makes it difficult to discern which particular approach would be best suited for a rich, diverse task like grasping. To answer this question, we propose a simulated benchmark for robotic grasping that emphasizes off-policy learning and generalization to unseen objects. Off-policy learning enables utilization of grasping data over a wide variety of objects, and diversity is important to enable the method to generalize to new objects that were not seen during training. We evaluate the benchmark tasks against a variety of Q-function estimation methods, a method previously proposed for robotic grasping with deep neural network models, and a novel approach based on a combination of Monte Carlo return estimation and an off-policy correction. Our results indicate that several simple methods provide a surprisingly strong competitor to popular algorithms such as double Q-learning, and our analysis of stability sheds light on the relative tradeoffs between the algorithms.},
archivePrefix = {arXiv},
arxivId = {1802.10264},
author = {Quillen, Deirdre and Jang, Eric and Nachum, Ofir and Finn, Chelsea and Ibarz, Julian and Levine, Sergey},
doi = {10.1109/ICRA.2018.8461039},
eprint = {1802.10264},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Quillen et al. - 2018 - Deep Reinforcement Learning for Vision-Based Robotic Grasping A Simulated Comparative Evaluation of Off-Policy M.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
month = {feb},
pages = {6284--6291},
title = {{Deep reinforcement learning for vision-based robotic grasping: A simulated comparative evaluation of off-policy methods}},
url = {http://arxiv.org/abs/1802.10264},
year = {2018}
}
@article{Redmon2015a,
abstract = {We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.},
author = {Redmon, Joseph and Angelova, Anelia},
doi = {10.1109/ICRA.2015.7139361},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2015{\_}Real-time grasp detection using convolutional neural networks{\_}Redmon, Angelova.pdf:pdf},
isbn = {9781479969234},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {1316--1322},
title = {{Real-time grasp detection using convolutional neural networks}},
volume = {2015-June},
year = {2015}
}
@article{Pinto2015,
abstract = {— Current learning-based robot grasping ap-proaches exploit human-labeled datasets for training the mod-els. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.},
author = {Pinto, Lerrel and Gupta, Abhinav},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}Supersizing self-supervision Learning to grasp from 50K tries and 700 robot hours{\_}Pinto, Gupta.pdf:pdf},
isbn = {9781467380263},
journal = {arXiv preprint arXiv:1509.06825},
pages = {3406--3413},
title = {{Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours}},
year = {2015}
}
@article{Quigley2007,
abstract = {The STanford Artificial Intelligence Robot (STAIR) project is a long-term group effort aimed at producing a viable home and office assistant robot. As a small concrete step towards this goal, we showed a demonstration video at the 2007 AAAI Mobile Robot Exhibition of the STAIR 1 robot responding to a verbal command to fetch an item. Carrying out this task involved the integration of multiple components, including spoken dialog, navigation, computer visual object detection, and robotic grasping. This paper describes the hardware and software integration frameworks used to facilitate the development of these components and to bring them together for the demonstration. Copyright {\textcopyright} 2007, American Association for Artificial Intelligence.},
author = {Quigley, Morgan and Berger, Eric and Ng, AY},
file = {:Users/atsushi/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2007 - STAIR Hardware and Software Architecture.pdf:pdf},
journal = {AAAI, Robotics Workshop},
keywords = {AAAI Technical Report WS-07-15},
pages = {31--37},
title = {{Stair: Hardware and software architecture}},
url = {http://www.aaai.org/Papers/Workshops/2007/WS-07-15/WS07-15-008.pdf},
year = {2007}
}
@article{Images2016,
author = {Images, Using Depth and Engineering, Mathematical Information and Engineering, Mathematical Information},
file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2016{\_}深度画像を用いたロボットアームによる 平面上未知物体の把持方法{\_}Images, Engineering, Engineering.pdf:pdf},
pages = {235--236},
title = {深度画像を用いたロボットアームによる 平面上未知物体の把持方法},
year = {2016}
}
% Encoding: UTF-8

@article{Sankai2017,
    abstract = {{\textcopyright} 2017 IEEE. Hemiplegic persons have difficulties in handling many kinds of tasks in daily life because they must perform the tasks by only the unaffected arm and hand. In order to realize upper limb work support for hemiplegic persons, a robot arm that has ability to grip various daily necessities and cooperate with the nonparalyzed side in same work space is required. The purposes of this study is to develop a novel assistance system "My Cybernic Robot Arm" that has measurement function and adjustment function of grip force, and to confirm the basic performance of the system through the benchmark experiments. The system is composed of a robot arm and hand, a sensing unit and a control unit. The system has grip force adjustment function and learning function of grip force by grip force teaching playback. The robot arm has six degrees of freedom and was designed with minimum size to cover an area of the table tasks so as not to narrow the work area of the nonparalyzed side. The sensing unit can measure the grip force and centroid position of grip force calculated by using a capacitance feature. In addition, developed sensing unit is small enough to be embedded in the fingertip of robot hand. We conducted verification of the sensing unit accuracy and grip force teaching and reproduction as basic experiments to confirm the basic performance of the system. The results showed that the system could measure grip force and load center of gravity position. In addition, it was confirmed that the system could learn the appropriate grip force and adjust the grip force by grip force teaching playback. In conclusion, the system has basic performance because it can grip the object with appropriate grip force. This system is expected to support upper limb work of hemiplegic persons such as desk work in everyday life, and improve QOL (Quality of life) and encourage independence.},
    author = {Toyama, Hiroaki and Kawamoto, Hiroaki and Sankai, Yoshiyuki},
    doi = {10.1109/ROBIO.2017.8324526},
    file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2018{\_}Basic research of upper limb work support system My cybernic robot arm for hemiplegic persons{\_}Toyama, Kawamoto, Sankai.pdf:pdf},
    isbn = {9781538637418},
    journal = {2017 IEEE Int. Conf. Robot. Biomimetics, ROBIO 2017},
    keywords = {Grip force control,Grip force teaching,Load center of gravity position,Nonuniform load},
    mendeley-groups = {RobotHand},
    pages = {1--7},
    title = {{Basic research of upper limb work support system "My cybernic robot arm" for hemiplegic persons}},
    volume = {2018-Janua},
    year = {2018}
}

@article{Sankai2019,
    author = {Toyama, Hiroaki and Kawamoto, Hiroaki and Sankai, Yoshiyuki},
    doi = {10.1109/embc.2019.8857106},
    file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2019{\_}Development of Cybernic Robot Arm to realize Support Action cooperated with Hemiplegic Person's Arm{\_}Toyama, Kawamoto, Sankai.pdf:pdf},
    isbn = {9781538613115},
    issn = {1557170X},
    journal = {2019 41st Annu. Int. Conf. IEEE Eng. Med. Biol. Soc.},
    mendeley-groups = {RobotHand},
    pages = {1645--1650},
    publisher = {IEEE},
    title = {{Development of Cybernic Robot Arm to realize Support Action cooperated with Hemiplegic Person's Arm}},
    year = {2019}
}

@article{リハビリテーション,
    abstract = {Rehabilitation using myoelectric prosthesis for trans-radial amputees has become wide spread and well established in several developed countries. However, the clinical use of myoelectric prostheses for trans-radial amputees has not yet spread in Japan. It is well known that once amputees become accustomed to using their prosthesis efficiently through adequate reha- bilitation, that various activities which the amputees had given up so far will become possible through enhanced bimanual activities. Although myoelectric prostheses have proved to be useful, the majority of amputees have not been satisfied with their function. As an amputee becomes a better user, they request not only simple tasks but also complicated ones. As a consequence, the amputee comes to know the limits of their myoelectric prosthesis, thus expectations for superior prostheses will arise. The recent remarkable development of engineering technology has enabled the progress of prosthetic limb technology, leading to the production of far superior functional prostheses which meet the user's expectations. However, there is a paradox in developing such superior prostheses. The more advanced the prosthesis we produce, the higher the cost. To achieve this end, it is absolutely imperative to secure the cooperation of both clinicians and engi- neers. Furthermore, a rehabilitation strategy for patients with a higher level of amputation （trans- humeral amputation, shoulder disarticulation） remains unsolved. In this paper, we propose a “Hybrid Myoelectric Prosthesis”, which consists of a myoelectric hand as a terminal device and a body-powered active elbow joint, as a realistic solution for higher level amputees. In addition, we introduce Targeted Reinnervation （TR） as a future strategy for reference.},
    author = {陳, 隆明},
    doi = {10.2490/jjrmc.49.31},
    file = {:Users/atsushi/Google{\_}Drive{\_}a{\_}yamada0703/東大/paper/2012{\_}Myoelectric Prostheses Current Status and Problems to be Solved, a Rehabilitation Strategy for Higher Level Amputation Patients,(2).pdf:pdf},
    issn = {1881-3526},
    journal = {Japanese J. Rehabil. Med.},
    mendeley-groups = {RobotHand},
    number = {1},
    pages = {31--36},
    title = {筋電義手普及の現状と課題，高位切断者に対する戦略，そして今後の展望},
    volume = {49},
    year = {2012}
}


@Misc{お片づけロボット,
    title = {Autonomous Tidying-up Robot System - Preferred Networks},
    howpublished = {https://projects.preferred.jp/tidying-up-robot/}
}

@Misc{HSRサイト,
    title = {トヨタ自動車、生活支援ロボットの実用化に向けて研究機関等と技術開発を推進するコミュニティを発足 | トヨタ自動車株式会社 公式企業サイト},
    howpublished = {https://global.toyota/jp/detail/8709536}
}

@inproceedings{HSR2018,
    author = {Yamamoto, Takashi and Terada, Koji and Ochiai, Akiyoshi and Saito, Fuminori and Asahara, Yoshiaki and Murase, Kazuto},
    year = {2018},
    month = {10},
    pages = {7675-7682},
    title = {Development of the Research Platform of a Domestic Mobile Manipulator Utilized for International Competition and Field Test},
    doi = {10.1109/IROS.2018.8593798}
}

@article{sekitani2016ultraflexible,
    title={Ultraflexible organic amplifier with biocompatible gel electrodes},
    author={Sekitani, Tsuyoshi and Yokota, Tomoyuki and Kuribara, Kazunori and Kaltenbrunner, Martin and Fukushima, Takanori and Inoue, Yusuke and Sekino, Masaki and Isoyama, Takashi and Abe, Yusuke and Onodera, Hiroshi and others},
    journal={Nature communications},
    volume={7},
    pages={11425},
    year={2016},
    publisher={Nature Publishing Group}
}

@article{esteva2017dermatologist,
    title={Dermatologist-level classification of skin cancer with deep neural networks},
    author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
    journal={Nature},
    volume={542},
    number={7639},
    pages={115},
    year={2017},
    publisher={Nature Publishing Group}
}

@article{wang2016deep,
    title={Deep learning for identifying metastatic breast cancer},
    author={Wang, Dayong and Khosla, Aditya and Gargeya, Rishab and Irshad, Humayun and Beck, Andrew H},
    journal={arXiv preprint arXiv:1606.05718},
    year={2016}
}

@article{rathore2015novel,
    title={Novel structural descriptors for automated colon cancer detection and grading},
    author={Rathore, Saima and Hussain, Mutawarra and Iftikhar, Muhammad Aksam and Jalil, Abdul},
    journal={Computer methods and programs in biomedicine},
    volume={121},
    number={2},
    pages={92--108},
    year={2015},
    publisher={Elsevier}
}


@article{fmri,
    added-at = {2018-08-13T00:00:00.000+0200},
    author = {Sarraf, Saman and Tofighi, Ghassem},
    biburl = {https://www.bibsonomy.org/bibtex/2f76303c736027c896b06d5f237ee0647/dblp},
    ee = {http://arxiv.org/abs/1603.08631},
    interhash = {754c41d33f9793be141dacdabc9d4b9e},
    intrahash = {f76303c736027c896b06d5f237ee0647},
    journal = {CoRR},
    keywords = {dblp},
    timestamp = {2018-08-14T14:48:39.000+0200},
    title = {Classification of Alzheimer's Disease using fMRI Data and Deep Learning Convolutional Neural Networks.},
    url = {http://dblp.uni-trier.de/db/journals/corr/corr1603.html#SarrafT16},
    volume = {abs/1603.08631},
    year = 2016
}

@article{litjens2017survey,
    title={A survey on deep learning in medical image analysis},
    author={Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and Van Der Laak, Jeroen Awm and Van Ginneken, Bram and S{\'a}nchez, Clara I},
    journal={Medical image analysis},
    volume={42},
    pages={60--88},
    year={2017},
    publisher={Elsevier}
}

@article{cho2014learning,
    title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
    author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
    journal={arXiv preprint arXiv:1406.1078},
    year={2014}
}

@inproceedings{Donahue_2015_CVPR,
    added-at = {2015-10-16T00:00:00.000+0200},
    author = {Donahue, Jeff and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Darrell, Trevor and Saenko, Kate},
    biburl = {https://www.bibsonomy.org/bibtex/2637bb837c6e53a7ffd53730013b73117/dblp},
    booktitle = {CVPR},
    
    ee = {http://dx.doi.org/10.1109/CVPR.2015.7298878},
    interhash = {0b1e8c68fcc14312dfc5770e77d708c4},
    intrahash = {637bb837c6e53a7ffd53730013b73117},
    isbn = {978-1-4673-6964-0},
    keywords = {dblp},
    pages = {2625-2634},
    publisher = {IEEE Computer Society},
    timestamp = {2016-04-29T11:54:14.000+0200},
    title = {Long-term recurrent convolutional networks for visual recognition and description.},
    url = {http://dblp.uni-trier.de/db/conf/cvpr/cvpr2015.html#DonahueHGRVDS15},
    year = 2015
}



@inproceedings{wang2016image,
    title={Image captioning with deep bidirectional LSTMs},
    author={Wang, Cheng and Yang, Haojin and Bartz, Christian and Meinel, Christoph},
    booktitle={Proceedings of the 2016 ACM on Multimedia Conference},
    pages={988--997},
    year={2016},
    organization={ACM}
}

@article{ji20133d,
    title={3D convolutional neural networks for human action recognition},
    author={Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
    journal={IEEE transactions on pattern analysis and machine intelligence},
    volume={35},
    number={1},
    pages={221--231},
    year={2013},
    publisher={IEEE}
}

@article{hinton2006reducing,
    title={Reducing the dimensionality of data with neural networks},
    author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
    journal={science},
    volume={313},
    number={5786},
    pages={504--507},
    year={2006},
    publisher={American Association for the Advancement of Science}
}

@article{kingma2013auto,
    added-at = {2018-08-13T00:00:00.000+0200},
    author = {Kingma, Diederik P. and Welling, Max},
    biburl = {https://www.bibsonomy.org/bibtex/2486ad13a443259d137cb57be1dc77002/dblp},
    ee = {http://arxiv.org/abs/1312.6114},
    interhash = {85731e0fbdb10b8543ea9f55301b37a5},
    intrahash = {486ad13a443259d137cb57be1dc77002},
    journal = {CoRR},
    keywords = {dblp},
    timestamp = {2018-08-14T14:45:20.000+0200},
    title = {Auto-Encoding Variational Bayes.},
    url = {http://dblp.uni-trier.de/db/journals/corr/corr1312.html#KingmaW13},
    volume = {abs/1312.6114},
    year = 2013
}





@inproceedings{abbasnejad2017infinite,
    title={Infinite variational autoencoder for semi-supervised learning},
    author={Abbasnejad, M Ehsan and Dick, Anthony and van den Hengel, Anton},
    booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages={781--790},
    year={2017},
    organization={IEEE}
}

@inproceedings{narayanaswamy2017learning,
    title={Learning disentangled representations with semi-supervised deep generative models},
    author={Narayanaswamy, Siddharth and Paige, T Brooks and Van de Meent, Jan-Willem and Desmaison, Alban and Goodman, Noah and Kohli, Pushmeet and Wood, Frank and Torr, Philip},
    booktitle={Advances in Neural Information Processing Systems},
    pages={5925--5935},
    year={2017}
}
@Article{Erat2018,
    author    = {Okan Erat and Werner Alexander Isop and Denis Kalkofen and Dieter Schmalstieg},
    title     = {Drone-Augmented Human Vision: Exocentric Control for Drones Exploring Hidden Areas},
    journal   = {{IEEE} Transactions on Visualization and Computer Graphics},
    year      = {2018},
    volume    = {24},
    number    = {4},
    pages     = {1437--1446},
    month     = {apr},
    doi       = {10.1109/tvcg.2018.2794058},
    file      = {:08260942.pdf:PDF},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Karambakhsh2018,
    author    = {Ahmad Karambakhsh and Aouaidjia Kamel and Bin Sheng and Ping Li and Po Yang and David Dagan Feng},
    title     = {Deep gesture interaction for augmented anatomy learning},
    journal   = {Int. J. Inf. Manage.},
    year      = {2018},
    month     = {mar},
    doi       = {10.1016/j.ijinfomgt.2018.03.004},
    publisher = {Elsevier {BV}},
}

@Article{OpenAI2018,
    author        = {OpenAI},
    title         = {Learning Dexterous In-Hand Manipulation},
    journal       = {CoRR},
    year          = {2018},
    volume        = {abs/1808.00177},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1808-00177},
    eprint        = {1808.00177},
    timestamp     = {Sun, 02 Sep 2018 15:01:56 +0200},
    url           = {http://arxiv.org/abs/1808.00177},
}

@Article{Levine2016,
    author        = {Sergey Levine and Peter Pastor and Alex Krizhevsky and Deirdre Quillen},
    title         = {Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection},
    journal       = {CoRR},
    year          = {2016},
    volume        = {abs/1603.02199},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/LevinePKQ16},
    eprint        = {1603.02199},
    timestamp     = {Mon, 13 Aug 2018 16:48:48 +0200},
    url           = {http://arxiv.org/abs/1603.02199},
}

@Article{Kober2013,
    author    = {Jens Kober and J. Andrew Bagnell and Jan Peters},
    title     = {Reinforcement learning in robotics: A survey},
    journal   = {The International Journal of Robotics Research},
    year      = {2013},
    volume    = {32},
    number    = {11},
    pages     = {1238--1274},
    month     = {aug},
    doi       = {10.1177/0278364913495721},
    publisher = {{SAGE} Publications},
}

@Article{Li2017,
    author        = {Yuxi Li},
    title         = {Deep Reinforcement Learning: An Overview},
    journal       = {CoRR},
    year          = {2017},
    volume        = {abs/1701.07274},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/Li17b},
    eprint        = {1701.07274},
    timestamp     = {Mon, 13 Aug 2018 16:48:40 +0200},
    url           = {http://arxiv.org/abs/1701.07274},
}

@Article{Yahya2016,
    author        = {Ali Yahya and Adrian Li and Mrinal Kalakrishnan and Yevgen Chebotar and Sergey Levine},
    title         = {Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search},
    journal       = {CoRR},
    year          = {2016},
    volume        = {abs/1610.00673},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/YahyaLKCL16},
    eprint        = {1610.00673},
    timestamp     = {Mon, 13 Aug 2018 16:46:51 +0200},
    url           = {http://arxiv.org/abs/1610.00673},
}

@inproceedings{shotton2008semantic,
    title={Semantic texton forests for image categorization and segmentation},
    author={Shotton, Jamie and Johnson, Matthew and Cipolla, Roberto},
    booktitle={Computer vision and pattern recognition, 2008. CVPR 2008. IEEE Conference on},
    pages={1--8},
    year={2008},
    organization={IEEE}
}

@inproceedings{kontschieder2011structured,
    title={Structured class-labels in random forests for semantic image labelling},
    author={Kontschieder, Peter and Bulo, Samuel Rota and Bischof, Horst and Pelillo, Marcello},
    booktitle={Computer Vision (ICCV), 2011 IEEE International Conference on},
    pages={2190--2197},
    year={2011},
    organization={IEEE}
}

@Article{Levine2015,
    author        = {Sergey Levine and Chelsea Finn and Trevor Darrell and Pieter Abbeel},
    title         = {End-to-End Training of Deep Visuomotor Policies},
    journal       = {CoRR},
    year          = {2015},
    volume        = {abs/1504.00702},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/LevineFDA15},
    eprint        = {1504.00702},
    timestamp     = {Mon, 13 Aug 2018 16:47:04 +0200},
    url           = {http://arxiv.org/abs/1504.00702},
}

@Article{alphaGoZero,
    author    = {David Silver and Julian Schrittwieser and Karen Simonyan and Ioannis Antonoglou and Aja Huang and Arthur Guez and Thomas Hubert and Lucas Baker and Matthew Lai and Adrian Bolton and Yutian Chen and Timothy Lillicrap and Fan Hui and Laurent Sifre and George van den Driessche and Thore Graepel and Demis Hassabis},
    title     = {Mastering the game of Go without human knowledge},
    journal   = {Nature},
    year      = {2017},
    volume    = {550},
    number    = {7676},
    pages     = {354--359},
    month     = {oct},
    doi       = {10.1038/nature24270},
    publisher = {Springer Nature},
}

@Article{Jaderberg2018,
    author        = {Max Jaderberg and Wojciech M. Czarnecki and Iain Dunning and Luke Marris and Guy Lever and Antonio Garc{\'{\i}}a Casta{\~{n}}eda and Charles Beattie and Neil C. Rabinowitz and Ari S. Morcos and Avraham Ruderman and Nicolas Sonnerat and Tim Green and Louise Deason and Joel Z. Leibo and David Silver and Demis Hassabis and Koray Kavukcuoglu and Thore Graepel},
    title         = {Human-level performance in first-person multiplayer games with population-based deep reinforcement learning},
    journal       = {CoRR},
    year          = {2018},
    volume        = {abs/1807.01281},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1807-01281},
    eprint        = {1807.01281},
    timestamp     = {Mon, 13 Aug 2018 16:48:08 +0200},
    url           = {http://arxiv.org/abs/1807.01281},
}

@Article{DQNnature,
    author    = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
    title     = {Human-level control through deep reinforcement learning},
    journal   = {Nature},
    year      = {2015},
    volume    = {518},
    number    = {7540},
    pages     = {529--533},
    month     = {feb},
    doi       = {10.1038/nature14236},
    publisher = {Springer Nature},
}

@Article{DQN,
    author        = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin A. Riedmiller},
    title         = {Playing Atari with Deep Reinforcement Learning},
    journal       = {CoRR},
    year          = {2013},
    volume        = {abs/1312.5602},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/MnihKSGAWR13},
    eprint        = {1312.5602},
    timestamp     = {Mon, 13 Aug 2018 16:47:42 +0200},
    url           = {http://arxiv.org/abs/1312.5602},
}

@Article{Schmidhuber2015,
    author    = {Jürgen Schmidhuber},
    title     = {Deep learning in neural networks: An overview},
    journal   = {Neural Networks},
    year      = {2015},
    volume    = {61},
    pages     = {85--117},
    month     = {jan},
    doi       = {10.1016/j.neunet.2014.09.003},
    publisher = {Elsevier {BV}},
}

@Article{alphaZero,
    author        = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy P. Lillicrap and Karen Simonyan and Demis Hassabis},
    title         = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
    journal       = {CoRR},
    year          = {2017},
    volume        = {abs/1712.01815},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1712-01815},
    eprint        = {1712.01815},
    timestamp     = {Mon, 13 Aug 2018 16:46:01 +0200},
    url           = {http://arxiv.org/abs/1712.01815},
}

@Article{alphaGo,
    author    = {David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
    title     = {Mastering the game of Go with deep neural networks and tree search},
    journal   = {Nature},
    year      = {2016},
    volume    = {529},
    number    = {7587},
    pages     = {484--489},
    month     = {jan},
    doi       = {10.1038/nature16961},
    publisher = {Springer Nature},
}

@Article{Levine2017,
    author    = {Sergey Levine and Peter Pastor and Alex Krizhevsky and Julian Ibarz and Deirdre Quillen},
    title     = {Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection},
    journal   = {The International Journal of Robotics Research},
    year      = {2017},
    volume    = {37},
    number    = {4-5},
    pages     = {421--436},
    month     = {jun},
    doi       = {10.1177/0278364917710318},
    publisher = {{SAGE} Publications},
}

@Article{Lample2016,
    author        = {Guillaume Lample and Devendra Singh Chaplot},
    title         = {Playing {FPS} Games with Deep Reinforcement Learning},
    journal       = {CoRR},
    year          = {2016},
    volume        = {abs/1609.05521},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/LampleC16},
    eprint        = {1609.05521},
    timestamp     = {Mon, 13 Aug 2018 16:47:19 +0200},
    url           = {http://arxiv.org/abs/1609.05521},
}

@Article{DoubleQ,
    author        = {Hado van Hasselt and Arthur Guez and David Silver},
    title         = {Deep Reinforcement Learning with Double Q-learning},
    journal       = {CoRR},
    year          = {2015},
    volume        = {abs/1509.06461},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/HasseltGS15},
    eprint        = {1509.06461},
    timestamp     = {Mon, 13 Aug 2018 16:47:32 +0200},
    url           = {http://arxiv.org/abs/1509.06461},
}

@Article{Hausknecht2015,
    author        = {Matthew J. Hausknecht and Peter Stone},
    title         = {Deep Recurrent Q-Learning for Partially Observable MDPs},
    journal       = {CoRR},
    year          = {2015},
    volume        = {abs/1507.06527},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/HausknechtS15},
    eprint        = {1507.06527},
    timestamp     = {Mon, 13 Aug 2018 16:48:38 +0200},
    url           = {http://arxiv.org/abs/1507.06527},
}

@Article{Hu2017,
    author        = {Ronghang Hu and Piotr Doll{\'{a}}r and Kaiming He and Trevor Darrell and Ross B. Girshick},
    title         = {Learning to Segment Every Thing},
    journal       = {CoRR},
    year          = {2017},
    volume        = {abs/1711.10370},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1711-10370},
    eprint        = {1711.10370},
    timestamp     = {Mon, 13 Aug 2018 16:47:48 +0200},
    url           = {http://arxiv.org/abs/1711.10370},
}

@Article{Harwath2017,
    author        = {David F. Harwath and James R. Glass},
    title         = {Learning Word-Like Units from Joint Audio-Visual Analysis},
    journal       = {CoRR},
    year          = {2017},
    volume        = {abs/1701.07481},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/HarwathG17},
    eprint        = {1701.07481},
    timestamp     = {Mon, 13 Aug 2018 16:45:59 +0200},
    url           = {http://arxiv.org/abs/1701.07481},
}

@Article{Tai2016,
    author        = {Lei Tai and Ming Liu},
    title         = {Deep-learning in Mobile Robotics - from Perception to Control Systems: {A} Survey on Why and Why not},
    journal       = {CoRR},
    year          = {2016},
    volume        = {abs/1612.07139},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/TaiL16a},
    eprint        = {1612.07139},
    timestamp     = {Mon, 13 Aug 2018 16:46:14 +0200},
    url           = {http://arxiv.org/abs/1612.07139},
}

@Article{Denil2016,
    author        = {Misha Denil and Pulkit Agrawal and Tejas D. Kulkarni and Tom Erez and Peter Battaglia and Nando de Freitas},
    title         = {Learning to Perform Physics Experiments via Deep Reinforcement Learning},
    journal       = {CoRR},
    year          = {2016},
    volume        = {abs/1611.01843},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/DenilAKEBF16},
    eprint        = {1611.01843},
    timestamp     = {Mon, 13 Aug 2018 16:46:40 +0200},
    url           = {http://arxiv.org/abs/1611.01843},
}

@Misc{mudigonda2018investigating,
    author = {Mayur Mudigonda and Pulkit Agrawal and Michael Deweese and Jitendra Malik},
    title  = {Investigating Deep Reinforcement Learning For Grasping Objects With An Anthropomorphic Hand},
    year   = {2018},
    url    = {https://openreview.net/forum?id=BJZt4KywG},
}

@Article{Rainbow,
    author        = {Matteo Hessel and Joseph Modayil and Hado van Hasselt and Tom Schaul and Georg Ostrovski and Will Dabney and Daniel Horgan and Bilal Piot and Mohammad Gheshlaghi Azar and David Silver},
    title         = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
    journal       = {CoRR},
    year          = {2017},
    volume        = {abs/1710.02298},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1710-02298},
    eprint        = {1710.02298},
    timestamp     = {Mon, 13 Aug 2018 16:48:05 +0200},
    url           = {http://arxiv.org/abs/1710.02298},
}

@InProceedings{ResNet,
    author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    title     = {Deep Residual Learning for Image Recognition},
    booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
    year      = {2016},
    month     = {jun},
    publisher = {{IEEE}},
    doi       = {10.1109/cvpr.2016.90},
}

@Article{Eslami2018,
    author    = {S. M. Ali Eslami and Danilo Jimenez Rezende and Frederic Besse and Fabio Viola and Ari S. Morcos and Marta Garnelo and Avraham Ruderman and Andrei A. Rusu and Ivo Danihelka and Karol Gregor and David P. Reichert and Lars Buesing and Theophane Weber and Oriol Vinyals and Dan Rosenbaum and Neil Rabinowitz and Helen King and Chloe Hillier and Matt Botvinick and Daan Wierstra and Koray Kavukcuoglu and Demis Hassabis},
    title     = {Neural scene representation and rendering},
    journal   = {Science},
    year      = {2018},
    volume    = {360},
    number    = {6394},
    pages     = {1204--1210},
    month     = {jun},
    doi       = {10.1126/science.aar6170},
    publisher = {American Association for the Advancement of Science ({AAAS})},
}

@Article{Sabour2017,
    author        = {Sara Sabour and Nicholas Frosst and Geoffrey E. Hinton},
    title         = {Dynamic Routing Between Capsules},
    journal       = {CoRR},
    year          = {2017},
    volume        = {abs/1710.09829},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1710-09829},
    eprint        = {1710.09829},
    timestamp     = {Mon, 13 Aug 2018 16:47:11 +0200},
    url           = {http://arxiv.org/abs/1710.09829},
}

@InProceedings{AlexNet,
    author    = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
    title     = {Imagenet classification with deep convolutional neural networks},
    booktitle = {Advances in Neural Information Processing Systems},
    year      = {2012},
    pages     = {2012},
}

@InProceedings{GoogLeNet,
    author    = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
    title     = {Going deeper with convolutions},
    booktitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
    year      = {2015},
    month     = {jun},
    publisher = {{IEEE}},
    doi       = {10.1109/cvpr.2015.7298594},
}

@Article{Jang2018,
    author        = {Eric Jang and Coline Devin and Vincent Vanhoucke and Sergey Levine},
    title         = {Grasp2Vec: Learning Object Representations from Self-Supervised Grasping},
    journal       = {CoRR},
    year          = {2018},
    volume        = {abs/1811.06964},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1811-06964},
    eprint        = {1811.06964},
    timestamp     = {Sun, 25 Nov 2018 18:57:12 +0100},
    url           = {http://arxiv.org/abs/1811.06964},
}

@Article{LeNet,
    author    = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
    title     = {Gradient-based learning applied to document recognition},
    journal   = {Proceedings of the {IEEE}},
    year      = {1998},
    volume    = {86},
    number    = {11},
    pages     = {2278--2324},
    doi       = {10.1109/5.726791},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Gu2017,
    author    = {Shixiang Gu and Ethan Holly and Timothy Lillicrap and Sergey Levine},
    title     = {Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
    booktitle = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
    year      = {2017},
    month     = {may},
    publisher = {{IEEE}},
    doi       = {10.1109/icra.2017.7989385},
}

@Article{DDPG,
    author        = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
    title         = {Continuous control with deep reinforcement learning},
    journal       = {CoRR},
    year          = {2015},
    volume        = {abs/1509.02971},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/LillicrapHPHETS15},
    eprint        = {1509.02971},
    timestamp     = {Mon, 13 Aug 2018 16:46:11 +0200},
    url           = {http://arxiv.org/abs/1509.02971},
}

@Article{NAF,
    author        = {Shixiang Gu and Timothy P. Lillicrap and Ilya Sutskever and Sergey Levine},
    title         = {Continuous Deep Q-Learning with Model-based Acceleration},
    journal       = {CoRR},
    year          = {2016},
    volume        = {abs/1603.00748},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/GuLSL16},
    eprint        = {1603.00748},
    timestamp     = {Mon, 13 Aug 2018 16:46:07 +0200},
    url           = {http://arxiv.org/abs/1603.00748},
}

@Article{QLearning,
    author  = {Christopher J. C. H. Watkins and Peter Dayan},
    title   = {Q-Learning},
    journal = {Machine Learning},
    year    = {1992},
    volume  = {8},
    number  = {3},
}

@Article{VGGNet,
    author        = {Karen Simonyan and Andrew Zisserman},
    title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
    journal       = {CoRR},
    year          = {2014},
    volume        = {abs/1409.1556},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/SimonyanZ14a},
    eprint        = {1409.1556},
    timestamp     = {Mon, 13 Aug 2018 16:46:51 +0200},
    url           = {http://arxiv.org/abs/1409.1556},
}

@Article{DuelingNetwork,
    author        = {Ziyu Wang and Nando de Freitas and Marc Lanctot},
    title         = {Dueling Network Architectures for Deep Reinforcement Learning},
    journal       = {CoRR},
    year          = {2015},
    volume        = {abs/1511.06581},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/WangFL15},
    eprint        = {1511.06581},
    timestamp     = {Mon, 13 Aug 2018 16:48:17 +0200},
    url           = {http://arxiv.org/abs/1511.06581},
}

@Article{PrioritizedExperienceReplay,
    author        = {Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
    title         = {Prioritized Experience Replay},
    journal       = {CoRR},
    year          = {2015},
    volume        = {abs/1511.05952},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/SchaulQAS15},
    eprint        = {1511.05952},
    timestamp     = {Mon, 13 Aug 2018 16:46:28 +0200},
    url           = {http://arxiv.org/abs/1511.05952},
}

@Article{Dmitry2018,
    author        = {Dmitry Kalashnikov and Alex Irpan and Peter Pastor and Julian Ibarz and Alexander Herzog and Eric Jang and Deirdre Quillen and Ethan Holly and Mrinal Kalakrishnan and Vincent Vanhoucke and Sergey Levine},
    title         = {QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation},
    journal       = {CoRR},
    year          = {2018},
    volume        = {abs/1806.10293},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1806-10293},
    eprint        = {1806.10293},
    timestamp     = {Mon, 13 Aug 2018 16:48:22 +0200},
    url           = {http://arxiv.org/abs/1806.10293},
}

@Article{A3C,
    author        = {Volodymyr Mnih and Adri{\`{a}} Puigdom{\`{e}}nech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
    title         = {Asynchronous Methods for Deep Reinforcement Learning},
    journal       = {CoRR},
    year          = {2016},
    volume        = {abs/1602.01783},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/bib/journals/corr/MnihBMGLHSK16},
    eprint        = {1602.01783},
    timestamp     = {Mon, 13 Aug 2018 16:47:40 +0200},
    url           = {http://arxiv.org/abs/1602.01783},
}

@PhdThesis{ExperienceReplay,
    author    = {Lin, Long-Ji},
    title     = {Reinforcement Learning for Robots Using Neural Networks},
    school    = {Carnegie Mellon University Pittsburgh},
    year      = {1992},
    address   = {Pittsburgh, PA, USA},
    note      = {UMI Order No. GAX93-22750},
    publisher = {Carnegie Mellon University},
}

@InProceedings{FittedQ,
    author    = {Riedmiller, Martin},
    title     = {Neural Fitted Q Iteration -- First Experiences with a Data Efficient Neural Reinforcement Learning Method},
    booktitle = {Machine Learning: ECML 2005},
    year      = {2005},
    editor    = {Gama, Jo{\~a}o and Camacho, Rui and Brazdil, Pavel B. and Jorge, Al{\'i}pio M{\'a}rio and Torgo, Lu{\'i}s},
    pages     = {317--328},
    address   = {Berlin, Heidelberg},
    publisher = {Springer Berlin Heidelberg},
    abstract  = {This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
    isbn      = {978-3-540-31692-3},
}

@Misc{pathology,
    title        = {一般社団法人 日本病理学会，認定病理専門医一覧},
    howpublished = {http://pathology.or.jp/senmoni/board-certified.html}
}

@article{simpleRNN,
    author    = {Yann LeCun and
    Yoshua Bengio and
    Geoffrey E. Hinton},
    title     = {Deep learning},
    journal   = {Nature},
    volume    = {521},
    number    = {7553},
    pages     = {436--444},
    year      = {2015},
    url       = {https://doi.org/10.1038/nature14539},
    doi       = {10.1038/nature14539},
    timestamp = {Wed, 14 Nov 2018 00:00:00 +0100},
    biburl    = {https://dblp.org/rec/bib/journals/nature/LeCunBH15},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Dropout,
    author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
    title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
    journal = {Journal of Machine Learning Research},
    year    = {2014},
    volume  = {15},
    pages   = {1929-1958},
    url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{BatchNorm,
    author = {Ioffe, Sergey and Szegedy, Christian},
    title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
    booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
    series = {ICML'15},
    year = {2015},
    location = {Lille, France},
    pages = {448--456},
    numpages = {9},
    url = {http://dl.acm.org/citation.cfm?id=3045118.3045167},
    acmid = {3045167},
    publisher = {JMLR.org},
}

@article{Adam,
    added-at = {2018-08-13T00:00:00.000+0200},
    author = {Kingma, Diederik P. and Ba, Jimmy},
    biburl = {https://www.bibsonomy.org/bibtex/23b0328784dbfce338ba0dd2618a7a059/dblp},
    ee = {http://arxiv.org/abs/1412.6980},
    interhash = {57d2ac873f398f21bb94790081e80394},
    intrahash = {3b0328784dbfce338ba0dd2618a7a059},
    journal = {CoRR},
    keywords = {dblp},
    timestamp = {2018-08-14T14:24:27.000+0200},
    title = {Adam: A Method for Stochastic Optimization.},
    url = {http://dblp.uni-trier.de/db/journals/corr/corr1412.html#KingmaB14},
    volume = {abs/1412.6980},
    year = 2014
}

@article{SIFT,
    author = {Lowe, David G.},
    title = {Distinctive Image Features from Scale-Invariant Keypoints},
    journal = {Int. J. Comput. Vision},
    issue_date = {November 2004},
    volume = {60},
    number = {2},
    month = nov,
    year = {2004},
    issn = {0920-5691},
    pages = {91--110},
    numpages = {20},
    url = {https://doi.org/10.1023/B:VISI.0000029664.99615.94},
    doi = {10.1023/B:VISI.0000029664.99615.94},
    acmid = {996342},
    publisher = {Kluwer Academic Publishers},
    address = {Hingham, MA, USA},
    keywords = {image matching, invariant features, object recognition, scale invariance},
} 

@article{HOG,
    abstract = {We study the question of feature sets for robust visual object recognition,
    adopting linear SVM based human detection as a test case. After reviewing
    existing edge and gradient based descriptors, we show experimentally
    that grids of Histograms of Oriented Gradient (HOG) descriptors significantly
    outperform existing feature sets for human detection. We study the
    influence of each stage of the computation on performance, concluding
    that fine-scale gradients, fine orientation binning, relatively coarse
    spatial binning, and high-quality local contrast normalization in
    overlapping descriptor blocks are all important for good results.
    The new approach gives near-perfect separation on the original MIT
    pedestrian database, so we introduce a more challenging dataset containing
    over 1800 annotated human images with a large range of pose variations
    and backgrounds.},
    added-at = {2009-08-24T22:23:57.000+0200},
    author = {Dalal, N. and Triggs, B.},
    biburl = {https://www.bibsonomy.org/bibtex/23765f5c9ab9abdf0efe63d50c64147d5/sitrke},
    citeulike-article-id = {335784},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1467360},
    description = {Human Detection},
    file = {:paperpool\\hog.pdf:PDF},
    interhash = {a19602b4d8fd42e73d957ae456280c17},
    intrahash = {3765f5c9ab9abdf0efe63d50c64147d5},
    journal = {Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer
    Society Conference on},
    keywords = {detection, vision},
    pages = {886--893},
    posted-at = {2007-03-13 17:24:40},
    priority = {2},
    timestamp = {2009-08-24T22:23:57.000+0200},
    title = {Histograms of Oriented Gradients for Human Detection},
    url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1467360},
    volume = 1,
    year = 2005
}

@article{SURF,
    abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster.
    
    This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps.
    
    The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF’s application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF’s usefulness in a broad range of topics in computer vision.},
    added-at = {2012-09-30T19:58:26.000+0200},
    author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and Gool, Luc Van},
    biburl = {https://www.bibsonomy.org/bibtex/2dfea172dfaca0272dcdc66acf92ec58d/daill},
    description = {ScienceDirect.com - Computer Vision and Image Understanding - Speeded-Up Robust Features (SURF)},
    doi = {10.1016/j.cviu.2007.09.014},
    interhash = {befc128dd8bce45a9d0308f72dc9a95a},
    intrahash = {dfea172dfaca0272dcdc66acf92ec58d},
    issn = {1077-3142},
    journal = {Computer Vision and Image Understanding},
    keywords = {detector feature surf},
    note = {Similarity Matching in Computer Vision and Multimedia},
    number = 3,
    pages = {346 - 359},
    timestamp = {2012-09-30T19:58:26.000+0200},
    title = {Speeded-Up Robust Features (SURF)},
    url = {http://www.sciencedirect.com/science/article/pii/S1077314207001555},
    volume = 110,
    year = 2008
}

@article{SVM,
    added-at = {2013-09-04T15:01:39.000+0200},
    author = {Cortes, C. and Vapnik, V.},
    biburl = {https://www.bibsonomy.org/bibtex/22b1eb8bea07ae0156a53a4e9c6eac1df/thoni},
    interhash = {c223c465141618ad63aac5a6132280f7},
    intrahash = {2b1eb8bea07ae0156a53a4e9c6eac1df},
    journal = {Machine Learning},
    keywords = {machine network support vector},
    pages = {273-297},
    timestamp = {2016-09-06T08:23:07.000+0200},
    title = {Support Vector Networks},
    volume = 20,
    year = 1995
}

@article{M2model,
    author    = {Diederik P. Kingma and
    Danilo Jimenez Rezende and
    Shakir Mohamed and
    Max Welling},
    title     = {Semi-Supervised Learning with Deep Generative Models},
    journal   = {CoRR},
    volume    = {abs/1406.5298},
    year      = {2014},
    url       = {http://arxiv.org/abs/1406.5298},
    archivePrefix = {arXiv},
    eprint    = {1406.5298},
    timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
    biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaRMW14},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Backprop,
    added-at = {2018-06-03T13:17:55.000+0200},
    author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
    biburl = {https://www.bibsonomy.org/bibtex/25d95851c0f627ab11747a2e481ecbad6/achakraborty},
    description = {Learning representations by back-propagating errors | Nature},
    interhash = {c354bc293fa9aa7caffc66d40a014903},
    intrahash = {5d95851c0f627ab11747a2e481ecbad6},
    journal = {Nature},
    keywords = {deep-learning nature neural-networks paper},
    month = oct,
    pages = {533--},
    publisher = {Nature Publishing Group},
    timestamp = {2018-06-03T13:17:55.000+0200},
    title = {Learning representations by back-propagating errors},
    url = {http://dx.doi.org/10.1038/323533a0},
    volume = 323,
    year = 1986
}

@article{SSD,
    added-at = {2016-12-03T00:00:00.000+0100},
    author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott E. and Fu, Cheng-Yang and Berg, Alexander C.},
    biburl = {https://www.bibsonomy.org/bibtex/2ba015352ae6d8ef89e5885f38b30ff77/dblp},
    ee = {http://arxiv.org/abs/1512.02325},
    interhash = {3de0ab12eb5f85e1a99264c83b0d0016},
    intrahash = {ba015352ae6d8ef89e5885f38b30ff77},
    journal = {CoRR},
    keywords = {dblp},
    timestamp = {2016-12-06T11:34:26.000+0100},
    title = {SSD: Single Shot MultiBox Detector.},
    url = {http://dblp.uni-trier.de/db/journals/corr/corr1512.html#LiuAESR15},
    volume = {abs/1512.02325},
    year = 2015
}

@article{YOLOv3,
    added-at = {2018-08-13T00:00:00.000+0200},
    author = {Redmon, Joseph and Farhadi, Ali},
    biburl = {https://www.bibsonomy.org/bibtex/25c9942eff0dac7cd3f00ae3ec886a88e/dblp},
    ee = {http://arxiv.org/abs/1804.02767},
    interhash = {bbdec3df168e9809d9e61423d4b4e062},
    intrahash = {5c9942eff0dac7cd3f00ae3ec886a88e},
    journal = {CoRR},
    keywords = {dblp},
    timestamp = {2018-08-14T13:39:38.000+0200},
    title = {YOLOv3: An Incremental Improvement.},
    url = {http://dblp.uni-trier.de/db/journals/corr/corr1804.html#abs-1804-02767},
    volume = {abs/1804.02767},
    year = 2018
}

@article{Unet,
    added-at = {2018-08-13T00:00:00.000+0200},
    author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
    biburl = {https://www.bibsonomy.org/bibtex/2b99e0743410b0939acaeb871134a21d7/dblp},
    ee = {http://arxiv.org/abs/1505.04597},
    interhash = {9158de16b2caff7458df054dc6fc2748},
    intrahash = {b99e0743410b0939acaeb871134a21d7},
    journal = {CoRR},
    keywords = {dblp},
    timestamp = {2018-08-14T13:02:37.000+0200},
    title = {U-Net: Convolutional Networks for Biomedical Image Segmentation.},
    url = {http://dblp.uni-trier.de/db/journals/corr/corr1505.html#RonnebergerFB15},
    volume = {abs/1505.04597},
    year = 2015
}

@article{Hosseini-AslGE16,
    added-at = {2018-08-13T00:00:00.000+0200},
    author = {Hosseini-Asl, Ehsan and Gimel'farb, Georgy L. and El-Baz, Ayman},
    biburl = {https://www.bibsonomy.org/bibtex/24b90a9a904a43bae0bd537ba98b8eaaa/dblp},
    ee = {http://arxiv.org/abs/1607.00556},
    interhash = {3578cf6f80e08143bf808a5225b81885},
    intrahash = {4b90a9a904a43bae0bd537ba98b8eaaa},
    journal = {CoRR},
    keywords = {dblp},
    timestamp = {2018-08-14T12:14:52.000+0200},
    title = {Alzheimer's Disease Diagnostics by a Deeply Supervised Adaptable 3D Convolutional Network.},
    url = {http://dblp.uni-trier.de/db/journals/corr/corr1607.html#Hosseini-AslGE16},
    volume = {abs/1607.00556},
    year = 2016
}

@inproceedings{GAN,
    added-at = {2014-12-10T00:00:00.000+0100},
    author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron C. and Bengio, Yoshua},
    biburl = {https://www.bibsonomy.org/bibtex/2c658b5ec42920435360d333978136d50/dblp},
    booktitle = {NIPS},
    
    editor = {Ghahramani, Zoubin and Welling, Max and Cortes, Corinna and Lawrence, Neil D. and Weinberger, Kilian Q.},
    ee = {http://papers.nips.cc/paper/5423-generative-adversarial-nets},
    interhash = {5e7e73013b6ed68fa953fdb6cb55f05f},
    intrahash = {c658b5ec42920435360d333978136d50},
    keywords = {dblp},
    pages = {2672-2680},
    timestamp = {2015-06-19T08:34:13.000+0200},
    title = {Generative Adversarial Nets.},
    url = {http://dblp.uni-trier.de/db/conf/nips/nips2014.html#GoodfellowPMXWOCB14},
    year = 2014
}

@inproceedings{xingjian2015convolutional,
    added-at = {2016-04-08T00:00:00.000+0200},
    author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and chun Woo, Wang},
    biburl = {https://www.bibsonomy.org/bibtex/251bdd06cb23e4ecfcd9c444fb67eb4b7/dblp},
    booktitle = {NIPS},
    
    editor = {Cortes, Corinna and Lawrence, Neil D. and Lee, Daniel D. and Sugiyama, Masashi and Garnett, Roman},
    ee = {http://papers.nips.cc/paper/5955-convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting},
    interhash = {b6d0c65d2d62ed0824cd0bcc9ae5ddd4},
    intrahash = {51bdd06cb23e4ecfcd9c444fb67eb4b7},
    keywords = {dblp},
    pages = {802-810},
    timestamp = {2016-07-15T11:42:45.000+0200},
    title = {Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting.},
    url = {http://dblp.uni-trier.de/db/conf/nips/nips2015.html#ShiCWYWW15},
    year = 2015
}

@article{bao2017cvae,
    title={CVAE-GAN: fine-grained image generation through asymmetric training},
    author={Bao, Jianmin and Chen, Dong and Wen, Fang and Li, Houqiang and Hua, Gang},
    journal={CoRR, abs/1703.10155},
    volume={5},
    year={2017}
}

@article{dou20163d,
    author    = {Qi Dou and Hao Chen and Yueming Jin and Lequan Yu and Jing Qin and Pheng-Ann Heng},
    title     = {3D Deeply Supervised Network for Automatic Liver Segmentation from {CT} Volumes},
    booktitle = {Medical Image Computing and Computer-Assisted Intervention {\textendash} {MICCAI} 2016},
    publisher = {Springer International Publishing},
    year      = {2016},
    pages     = {149--157},
    doi       = {10.1007/978-3-319-46723-8_18}
}
    
@article{BroschTYLTT16,
    added-at = {2018-11-14T00:00:00.000+0100},
    author = {Brosch, Tom and Tang, Lisa Y. W. and Yoo, Youngjin and Li, David K. B. and Traboulsee, Anthony and Tam, Roger C.},
    biburl = {https://www.bibsonomy.org/bibtex/27c363ac59bbe0345474a863cd2aa826b/dblp},
    ee = {https://www.wikidata.org/entity/Q39990248},
    interhash = {b3ce65d9ca731fccb834d3626728f37e},
    intrahash = {7c363ac59bbe0345474a863cd2aa826b},
    journal = {IEEE Trans. Med. Imaging},
    keywords = {dblp},
    number = 5,
    pages = {1229-1239},
    timestamp = {2018-11-15T12:03:51.000+0100},
    title = {Deep 3D Convolutional Encoder Networks With Shortcuts for Multiscale Feature Integration Applied to Multiple Sclerosis Lesion Segmentation.},
    url = {http://dblp.uni-trier.de/db/journals/tmi/tmi35.html#BroschTYLTT16},
    volume = 35,
    year = 2016
}
    
@inproceedings{grad-cam,
    added-at = {2018-01-11T00:00:00.000+0100},
    author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
    biburl = {https://www.bibsonomy.org/bibtex/2f510d2fbeb8f8bc21f44ff62df3d291a/dblp},
    booktitle = {ICCV},
    crossref = {conf/iccv/2017},
    ee = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2017.74},
    interhash = {14e7991f774cb73e9dd13d23fd8898ab},
    intrahash = {f510d2fbeb8f8bc21f44ff62df3d291a},
    isbn = {978-1-5386-1032-9},
    keywords = {dblp},
    pages = {618-626},
    publisher = {IEEE Computer Society},
    timestamp = {2018-01-13T11:40:20.000+0100},
    title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.},
    url = {http://dblp.uni-trier.de/db/conf/iccv/iccv2017.html#SelvarajuCDVPB17},
    year = 2017
}
    
@article{inglese2017deep,
    title={Deep learning and 3D-DESI imaging reveal the hidden metabolic heterogeneity of cancer},
    author={Inglese, Paolo and McKenzie, James S and Mroz, Anna and Kinross, James and Veselkov, Kirill and Holmes, Elaine and Takats, Zoltan and Nicholson, Jeremy K and Glen, Robert C},
    journal={Chemical science},
    volume={8},
    number={5},
    pages={3500--3511},
    year={2017},
    publisher={Royal Society of Chemistry}
}
    
@article{hoo2016deep,
    title={Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning},
    author={Hoo-Chang, Shin and Roth, Holger R and Gao, Mingchen and Lu, Le and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M},
    journal={IEEE transactions on medical imaging},
    volume={35},
    number={5},
    pages={1285},
    year={2016},
    publisher={NIH Public Access}
}
    
@article{havaei2017brain,
    title={Brain tumor segmentation with deep neural networks},
    author={Havaei, Mohammad and Davy, Axel and Warde-Farley, David and Biard, Antoine and Courville, Aaron and Bengio, Yoshua and Pal, Chris and Jodoin, Pierre-Marc and Larochelle, Hugo},
    journal={Medical image analysis},
    volume={35},
    pages={18--31},
    year={2017},
    publisher={Elsevier}
}
    
@article{lecun2015deep,
    title={Deep learning},
    author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
    journal={nature},
    volume={521},
    number={7553},
    pages={436},
    year={2015},
    publisher={Nature Publishing Group}
}
    
@article{hoo2016deep,
    title={Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning},
    author={Hoo-Chang, Shin and Roth, Holger R and Gao, Mingchen and Lu, Le and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M},
    journal={IEEE transactions on medical imaging},
    volume={35},
    number={5},
    pages={1285},
    year={2016},
    publisher={NIH Public Access}
}
    
    
@article{RadfordMC15,
    added-at = {2018-08-13T00:00:00.000+0200},
    author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
    biburl = {https://www.bibsonomy.org/bibtex/2dd5fb901fbe12c53b342e5b77062460b/dblp},
    ee = {http://arxiv.org/abs/1511.06434},
    interhash = {ae6fc4b7593a1d0e31aeeff9fef81a36},
    intrahash = {dd5fb901fbe12c53b342e5b77062460b},
    journal = {CoRR},
    keywords = {dblp},
    timestamp = {2018-08-14T15:04:08.000+0200},
    title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.},
    url = {http://dblp.uni-trier.de/db/journals/corr/corr1511.html#RadfordMC15},
    volume = {abs/1511.06434},
    year = 2015
}

@Misc{お片づけロボット,
    title = {Autonomous Tidying-up Robot System - Preferred Networks},
    howpublished = {\url{https://projects.preferred.jp/tidying-up-robot/}}
}

@Misc{HSRサイト,
    title = {トヨタ自動車、生活支援ロボットの実用化に向けて研究機関等と技術開発を推進するコミュニティを発足 | トヨタ自動車株式会社 公式企業サイト},
    howpublished = {\url{https://global.toyota/jp/detail/8709536}}
}

@Misc{COCOサイト,
    title = {COCO - Common Objects in Context},
    howpublished = {\url{http://cocodataset.org}}
}

@inproceedings{COCO論文,
    abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
    archivePrefix = {arXiv},
    arxivId = {1405.0312},
    author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
    booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    doi = {10.1007/978-3-319-10602-1_48},
    eprint = {1405.0312},
    file = {::},
    issn = {16113349},
    mendeley-groups = {ComputerVision},
    number = {PART 5},
    pages = {740--755},
    publisher = {Springer Verlag},
    title = {{Microsoft COCO: Common objects in context}},
    volume = {8693 LNCS},
    year = {2014}
}

@Misc{pybullet,
    author =   {Erwin Coumans and Yunfei Bai},
    title =    {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
    howpublished = {\url{http://pybullet.org}},
    year = {2016--2019}
}

@inproceedings{HSR2018,
    author = {Yamamoto, Takashi and Terada, Koji and Ochiai, Akiyoshi and Saito, Fuminori and Asahara, Yoshiaki and Murase, Kazuto},
    year = {2018},
    month = {10},
    pages = {7675-7682},
    title = {Development of the Research Platform of a Domestic Mobile Manipulator Utilized for International Competition and Field Test},
    doi = {10.1109/IROS.2018.8593798}
}

@article{jamminggripper,
    abstract = {Gripping and holding of objects are key tasks for robotic manipu-lators. The development of universal grippers able to pick up unfamiliar objects of widely varying shape and surface properties remains, however, challenging. Most current designs are based on the multifingered hand, but this approach introduces hardware and software complexities. These include large numbers of control-lable joints, the need for force sensing if objects are to be handled securely without crushing them, and the computational overhead to decide how much stress each finger should apply and where. Here we demonstrate a completely different approach to a universal gripper. Individual fingers are replaced by a single mass of granular material that, when pressed onto a target object, flows around it and conforms to its shape. Upon application of a vacuum the granular material contracts and hardens quickly to pinch and hold the object without requiring sensory feedback. We find that volume changes of less than 0.5{\%} suffice to grip objects reliably and hold them with forces exceeding many times their weight. We show that the operating principle is the ability of granular materials to transition between an unjammed, deformable state and a jammed state with solid-like rigidity. We delineate three separate mechanisms, friction, suction, and interlocking, that contribute to the gripping force. Using a simple model we relate each of them to the mechanical strength of the jammed state. This advance opens up new possibilities for the design of simple, yet highly adaptive systems that excel at fast gripping of complex objects. stress-strain | packing density | friction | suction | interlocking},
    author = {Brown, Eric and Rodenberg, Nicholas and Amend, John and Mozeika, Annan and Steltz, Erik and Zakin, Mitchell R and Lipson, Hod and Jaeger, Heinrich M},
    doi = {10.1073/pnas.1003250107/-/DCSupplemental},
    file = {::},
    mendeley-groups = {ComputerVision},
    title = {{Universal robotic gripper based on the jamming of granular material}},
    url = {www.pnas.org/lookup/suppl/}
}

@book{われはロボット,
    title={I, Robot},
    author={Asimov, I.},
    lccn={68070665},
    series={Robot series},
    url={https://books.google.co.jp/books?id=MD0GAQAAIAAJ},
    year={1950},
    publisher={Bantam Books}
}

@article{pepper,
    abstract = {As robotics technology evolves, we believe that personal social robots will be one of the next big expansions in the robotics sector. Based on the accelerated advances in this multidisciplinary domain and the growing number of use cases, we can posit that robots will play key roles in everyday life and will soon coexist with us, leading all people to a smarter, safer, healthier, and happier existence.},
    author = {Pandey, Amit Kumar and Gelin, Rodolphe},
    doi = {10.1109/MRA.2018.2833157},
    issn = {1558223X},
    journal = {IEEE Robotics and Automation Magazine},
    mendeley-groups = {MobileManipulator},
    month = {sep},
    number = {3},
    pages = {40--48},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    title = {{A Mass-Produced Sociable Humanoid Robot: Pepper: the First Machine of Its Kind}},
    volume = {25},
    year = {2018}
}

@Misc{roomba,
    title = {ロボット掃除機 ルンバ | アイロボット公式サイト},
    howpublished = {\url{https://www.irobot-jp.com/roomba/}}
}

@Misc{davinci,
    title = {da Vinciiの紹介：da Vinciについて | 日本ロボット外科学会 J-robo -Japan Robotic Surgery Society-},
    howpublished = {\url{https://j-robo.or.jp/da-vinci/}}
}

@Misc{myspoon,
    title = {マイスプーン（食事支援ロボット）｜医療・介護｜防犯対策・セキュリティのセコム},
    howpublished = {\url{https://www.secom.co.jp/personal/medical/myspoon.html}}
}