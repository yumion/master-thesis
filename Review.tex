\chapter{深層学習による病理画像の診断支援}
\label{chap_review}

病理画像をデジタルで保存することが始まったのは数十年前になる．これによって遠隔地でも診断することができるようになったり，情報を共有することができるようになり，複数の医師で診断しミスを防止するセカンド・オピニオンが容易になった．計算機科学の分野の側面ではデータを収集することができるようになり，研究が盛んに行われることになった．その後は，様々な病理データでより改良されたアルゴリズムの提案が行われている．\cite{litjens2017survey}

細胞組織の形態を観察するための病理染色ではヘマトキシリン・エオジン染色(HE染色)が一般的に用いられる．細胞核を青紫色に染色し，細胞質をピンク色に染色する．正常から異常に変化していくと，細胞核が過度に増殖したり，細胞質の形が崩れたりすることで，その特徴を機械学習によって精度よく検出するための研究が行われている．\cite{wang2016deep}

これまでは，核の形やテキスチャーからパターンマッチングなどの画像処理によって腫瘍を検出する研究されてきたが，近年になって画像処理に大きなブレークスルーが起きたことをきっかけに，新しい手法で解析するようになってきた．そのブレークスルーがディープラーニングである．

\section{ニューラルネットワーク}\label{sec:NeuralNetwork}
人間の脳にはニューロンと呼ばれる神経細胞が1000億個以上あり，それぞれが複数のニューロンが電気信号によって情報を伝達している．また脳にはシナプスという場所があり，ここで電気信号を細胞体へ受け渡す．細胞体はある閾値以上の電気信号がきた場合に他のニューロンへ電気信号を伝播させる(これを発火と呼ぶ)．このようなニューロンとシナプスで行われる演算を模倣したアルゴリズムを作ることができれば，人間のような思考や認識をコンピュータを使って再現できると考えた．そのアルゴリズムがニューラルネットワーク(Neural Network: NN)である．

\subsection{多層パーセプトロン}
ニューラルネットワークは入力層，出力層，隠れ層から構成され，層と層の間にはニューロン同士のつながりの強さを示す重みがある．非線形問題を扱うために1986年Rumelhartによって考案されたのが，パーセプトロンを複数つなぎ合わせ入力と出力以外に隠れた層を持つ多層パーセプトロン(Multi-layer perceptron: MLP)である(\fig {mlp})．ニューラルネットワークで多層パーセプトロンの層を全結合(fully connected: FC)層とも呼ぶ．

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.4\columnwidth}
		\centering
		\includegraphics[width=1.2\linewidth]{fig/MLP}
		\subcaption{Muti-layer perceptron}
		\label{fig:mlp}
	\end{minipage}
	\begin{minipage}[b]{0.4\columnwidth}
		\centering
		\includegraphics[width=0.7\linewidth]{fig/simple_perceptron}
		\subcaption{Simple perceptron}
		\label{fig:perceptron}
	\end{minipage}
	\caption{Architecture of Muti-layer perceptron}
\end{figure}

\fig {mlp}における丸や矢印はそれぞれノード(またはニューロン)と重み(または結合)と呼び，ともに数値である．例えば画像を分類しようと思えば，各ピクセルの画素数を各ノードに入力する．例えば$28 \times 28$pixelのグレースケール画像であれば，784個のノードが必要となる．入力データ$\bm {x}$が入力層に入ってくると，その値に重み$\bm {w}$をかけ，活性化関数$H$と呼ばれる関数に通し，結果$\bm{y}$を出力する．ここで，入力$\bm{x}$，重み$\bm{w}$，出力$\bm{y}$を太字で表したが，これらは全てテンソルであり，1つの層にあるノード$x_1, x_2, \cdots x_n$を一括して$\bm {x}$として表記している．

ここで，中間層の1つのノードについて考える．\fig {perceptron}にMLPを構成する1ユニットである単純パーセプトロンを示した．この模式図を数式で表すと次のようになる．
\begin{align}
	y & = H(\bm{w}\bm{x} + \bm{b}) \\
	& = H\left( \sum_{i=1}^3 w_i x_i + b_i \right) 
\end{align}
ここで$\bm{b}$はバイアスと呼ばれ，発火のしやすさを表している．中間層における活性化関数は，\eq {ReLU}に示す正規化線形関数(rectified linear unit: ReLU)と呼ばれる関数)がよく用いられる．
\begin{align}\label{eq:ReLU}
	H(x) = \max \left\lbrace 0, x \right\rbrace = \begin{cases}
	x & (x > 0) \\
	0 & (x \leq 0)
	\end{cases}
\end{align}
この演算を繰り返し出力層に書き出す．ここで，各層の重みの値によって出力結果は異なってくる．

出力層では，ノードの個数は区別したいクラス数分用意する．
各ノードの出力値が各クラスに属している確率を表すように，活性化関数にはソフトマックス関数を用いる(ただし二値分類の場合はシグモイド関数を用いる)．ソフトマックス関数は\eq {softmax}で表される．
\begin{align}\label{eq:softmax}
	y_i = \dfrac{\exp(x_i)}{\displaystyle　\sum_{k=1}^n \exp(x_k)}
\end{align}
ここで$y_i$は，出力層が全部で$n$個あるとして，$i$番目の出力であることを示す．\eq {softmax}からわかるように，入力の総和に対して1つのノードがどれくらいの値を持つかという割合で表されている．これにより各ノードの出力は確率として解釈できるため，値の一番大きいノードのインデックスを予測ラベルとして見ることができる．


\subsection{畳み込みニューラルネットワーク}
従来の画像認識では，画像から特徴を抽出しそれを識別器にかける手法が主流であった．古典的手法では画像から特徴を抽出するいわゆる特徴量設計が必要で，ここをいかにうまく設計するかがポイントであった．特徴抽出の方法として，HOG\cite{HOG}やSIFT\cite{SIFT}，SURF\cite{SURF}などがあり，これらによって抽出した特徴ベクトルをSupport Vector Machine(SVM)\cite{SVM}によって識別することが多かった．

しかし，1998年にLeNetと呼ばれる畳み込みニューラルネットワーク(Convolutional Neural Network: CNN)が提案された\cite{LeNet}．CNNは畳み込み層とプーリング層からなっている．この畳み込みとプーリングの演算を通して，特徴量設計から識別までをend-to-endで行うことができる．
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{fig/LeNet}
	\caption{Architecture of convolutional neural network\cite{LeNet}}
	\label{fig:LeNet}
\end{figure}


%人間が物体を認識することをコンピュータにも計算させるには，画像の特徴的な部分を切り分けて数値化させる必要がある．例えば，カラー画像の場合，RGBの3色(3チャンネル)を組み合わせた画像で認識をしている．このようなフィルターの畳み込み計算を行うと，フィルターごとに異なった画像の特徴を抽出して数値化する．これが畳み込み(convolution)である．その後，画像のサイズを小さくしてコンピュータが計算コストを減らし，微小な変化に対してロバストになる仕組みしてプーリングという方法を用いる．

畳み込み層では，入力に対してフィルター(カーネルとも呼ばれる)を用意し，\eq {conv}に示す計算を行う．
\begin{align}\label{eq:conv}
	y_{i,j} & = (\bm{K} * \bm{x})_{i,j}\\
	& = \sum_m\sum_n x_{i+m, j+n} K_{m,n}
\end{align}
ここで，$\bm{K}$はフィルター，$\bm{x}$は入力，$y$は出力である．CNNではこの演算の後に活性化関数に通す．これを図で表すと\fig {conv}のようになる．

\begin{figure}[H]
	\centering
	\begin{minipage}{0.45\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/conv_ex1}
		\subcaption{}
		\label{fig:conv_ex1}
	\end{minipage}
	\hspace{10truemm}
	\begin{minipage}{0.45\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/conv_ex2}
		\subcaption{}
		\label{fig:conv_ex2}
	\end{minipage}
	\begin{minipage}{0.45\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/conv_ex3}
		\subcaption{}
		\label{fig:conv_ex3}
	\end{minipage}
	\hspace{10truemm}
	\begin{minipage}{0.45\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/conv_ex4}
		\subcaption{}
		\label{fig:conv_ex4}
	\end{minipage}
	\caption{Operation process of convolution}
	\label{fig:conv}
\end{figure}

\fig {conv}では，フィルターのサイズは$3\times 3$であるが，大きさは任意である($3\times 3$や$5\times 5$, $7\times 7$がよく用いられる)．また，フィルターは1マスずつ横にずらして計算を行っている．ずらし方をストライドといい，今回はストライド1である．CNNでは多くの場合，ストライドは1である．
このようなフィルターの畳み込み計算を行うと，フィルターごとに異なった画像の特徴を抽出して数値化することができる．

次にプーリングを行う．ここでは，画像認識で多く用いられる最大値プーリングについて述べる．\fig {maxpooling}に示すように，$2\times 2$のプールサイズを用意した時，その範囲内にある最大値を取る演算である．ストライドはプールサイズと合わせ，プーリングを行った領域と被らないようにすることが一般的である．\fig {maxpooling}ではストライド2である．

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.4\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/pooling_ex1}
		\subcaption{}
		\label{fig:pooling_ex1}
	\end{minipage}
	\hspace{10truemm}
	\begin{minipage}[b]{0.4\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/pooling_ex2}
		\subcaption{}
		\label{fig:pooling_ex2}
	\end{minipage}
	\begin{minipage}[b]{0.4\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/pooling_ex3}
		\subcaption{}
		\label{fig:pooling_ex3}
	\end{minipage}
	\hspace{10truemm}
	\begin{minipage}[b]{0.4\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/pooling_ex4}
		\subcaption{}
		\label{fig:pooling_ex4}
	\end{minipage}
	\caption{Operation process of max pooling}
	\label{fig:maxpooling}
\end{figure}

プーリング層では画像のサイズを小さくして(コンピュータの)計算コストを減らし，微小な変化に対してロバストになる．

この畳み込みとプーリングを繰り返して，入力からフィルターの数だけ特徴を抽出し，この抽出した特徴マップをFC層へ繋げて識別を行う手法がCNNである．


\subsection{再帰的ニューラルネットワーク}
再帰的ニューラルネットワーク(Recurrent Neural Network: RNN)は時系列解析や自然言語処理に利用されるニューラルネットワークである．\fig {simpleRNN}に示すように，内部にループを持つことで過去の情報を保持しておくことができる．

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{fig/simpleRNN}
	\caption{Architecture of RNN\cite{simpleRNN}}
	\label{fig:simpleRNN}
\end{figure}


時系列の入力$\bm{x} = (x_1, \cdots , x_T)$があった時に，出力$\bm{y} = (y_1, \cdots , y_T)$と隠れ層のベクトル$\bm{h} = (h1, \cdots ,h_T)$をそれぞれ以下の式で計算する．

\begin{align}\label{eq:RNN}
	h_t & = H(W_{ih} x_t + W_{hh} h_{t-1} + b_h) \\ 
	y_t & = W_{ho} h_t + b_o
\end{align}

ここで，$\bm{W}$は重み行列($W_{ih}$は入力と隠れ層間の重み行列)であり，$\bm{b}$はバイアス項，$H$は活性化関数である．

\subsection*{LSTM}
RNNは，過去の情報をどこまで遡って関連性を見つけるか判断することができない．そのため，時系列データが長くなるほどその長期の依存性を学習するには人が慎重にパラメータを設計する必要があり，長期的なデータは学習が難しい問題があった．この問題を解決するためにLong-short term memory(LSTM)が提案された．\fig {LSTM}に示すように，LSTMもRNNの一種であるため繰り返し構造を持ち，3つのゲートを持つ層からなっている．

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{fig/lstm.png}
	\caption{Architecture of LSTM}
	\label{fig:LSTM}
\end{figure}

\begin{align}\label{eq:LSTM}
	f_t & = \sigma(W_{xf} x_t + W_{hf} h_{t-1} + W_{cf} c_{t-1} + b_f ) \\
	i_t & = \sigma(W_{xi} x_t + W_{hi} h_{t-1} + W_{ci} c_{t-1} + b_i) \\
	c_t & = f_t c_{t-1} + i_t \tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c) \\
	o_t & = \sigma(W_{xo} x_t + W_{ho} h_{t-1} + W_{co} c_t + b_o) \\
	h_t & = o_t \tanh(c_t) 
\end{align}

ここで，活性化関数にはシグモイド関数および$\tanh$を用いる．シグモイド関数は次の式で定義される．
\begin{equation}\label{eq:sigmoid}
	\sigma (x) = \dfrac{1}{1 + e^{-x}}
\end{equation}

$f_t$で示される層は忘却ゲート層と呼ばれ，過去の情報で捨てるべき情報を判断する．これはシグモイド層によって行われ，0と1の間の値を出力し，0は完全に忘れる．1は完全に維持するという意味である．
$i_t$や$c_t$で示される層は，入力ゲート層と呼ばれ，$i_t$で新たに入力された情報から，どの情報を更新するかを判断し，$c_t$で古い情報を落とし新しい情報を加え値を更新する．
最後が$o_t$や$h_t$で示される層で，出力ゲート層と呼ばれる．まず何を出力するべきかを$o_t$のゲートで判断して，$c_t$に$\tanh$を適用して掛けることで出力が計算される．

\subsection*{GRU}
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{fig/chapter2/gru}
	\caption{GRU structure}
	\label{fig:gru}
\end{figure}

 GRUはLSTMと似た構造をもつニューラルネットワークであるが，より単純であるため計算量を少なくすることができる．\cite{cho2014learning}
\fig{gru}におけるzとrがゲートの役割でゲートzで新しく値を更新するかどうかを決定し，rで前の情報を無視するか利用するかを判断する．

\section{推論と学習}
ニューラルネットワークでは推論フェーズと学習フェーズに分かれている．\ref{sec:NeuralNetwork}節は全て推論フェーズの話であり，順伝播ニューラルネットと呼ばれる．

学習フェーズでは，逆伝播ニューラルネットを用いる．逆伝播とは誤差逆伝播法(Backpropergation)\cite{Backprop}から由来している．真値からの誤差を表す損失関数(loss function)を用いて，パラメータの微分値を更新に用いる．損失関数はコスト関数，目的関数とも呼ばれる．この微分値を効率よく計算するアルゴリズムが誤差逆伝播法である．

損失関数は解く問題の目的に合わせて選ぶ必要がある．回帰問題では平均二乗和誤差(mean squared error: MSE)が用いられる．
\begin{align}\label{eq:mse}
	L_{\mathrm{MSE}} = \dfrac{1}{2} \sum_{k} (y_k - t_k)^2
\end{align}
また，クラス分け問題では交差エントロピー誤差(cross entropy error)が用いられる．
\begin{align}\label{eq:crossentropy}
	L_{\mathrm{cross}} = - \sum_k t_k \ln {y_k}
\end{align}
ここで，$y$は予測ラベルで，$t$は教師ラベルである．

\subsection{最適化手法}
損失関数を用いてパラメータを更新するが，更新手法にはいくつか方法があるため，ここでは本研究で使用した最適化手法について述べる．

\subsection*{SGD}
SGDは日本語で確率的勾配降下法(Stochastic gradient descent)と呼ばれる手法で，最も単純な最適化手法である．画像認識の分野では多く使われている．

SGDでは以下の式\eq {SGD}でパラメータを更新する．
\begin{align}\label{eq:SGD}
	\bm{W} \leftarrow \bm{W} - \eta \pdif{L}{\bm{W}} 
\end{align}
ここで，$\bm{W}$は更新する重みパラメータ，$\pdif{L}{\bm{W}}$は$\bm{W}$に関する損失関数の勾配である．また$\eta$は学習率と呼ばれ，実際には0.01や0.001といった値を前もって決めて使用する．SGDはパラメータの勾配を利用して，勾配方向にパラメータを更新するステップを繰り返して，徐々に最適なパラメータへと近づける手法である．

\subsection*{Adam}
Adam\cite{Adam}はadaptive moment estimationの略で，勾配の値の1乗和と2乗和の両方をパラメータ更新に用いる手法である．以下の式\eq{adam}でパラメータを更新する．

\begin{align}\label{eq:adam}
	\bm{m} & \leftarrow \beta_1 \bm{m} + (1 - \beta_1) \pdif{L}{\bm{W}} \\
	\bm{v} & \leftarrow \beta_2 \bm{v} + (1 - \beta_2) \left( \pdif{L}{\bm{W}} \right) ^2 \\
	\bm{W} & \leftarrow \bm{W} - \eta \dfrac{\hat{\bm{m}}}{\sqrt{\hat{\bm{v}}} + \epsilon} 
\end{align}

$m_t$と$v_t$はそれぞれ、勾配の一次モーメント（平均値）と二次モーメント（分散した平方偏差）の概算値である．$\eta$，$\beta_1$，$\beta_2$はそれぞれ学習パラメータである．
また，$\hat{m}_t$，$\hat{v}_t$はそれぞれ移動指数平均を用いた際に生じるバイアス(大きさを変えてしまっていることなど)を打ち消すために正則化しており，\eq {adamhat}で表される．

\begin{align}\label{eq:adamhat}
	\hat{\bm{m}} & = \dfrac{\bm{m}}{1 - \beta_1} \\
	\hat{\bm{v}} & = \dfrac{\bm{v}}{1 - \beta_2}
\end{align}

学習パラメータはそれぞれ$\eta = 0.001$，$\beta_1 = 0.9$，$\beta_2 = 0.999$，$\epsilon = 10^{-8}$が最適だと言われている\cite{Adam}．

\subsection{学習のテクニック}
ディープラーニングでは過学習(overfitting)と呼ばれる問題が多く起こる．過学習とは，訓練データに対してのみ適応し過ぎてしまい，訓練データに含まれないテストデータには精度が出ない状態を指す．過学習は，大量にパラメータを持つ表現力の高いモデルであることや，訓練データが少ないことなどが原因で起こる．ここでは過学習を抑制するテクニックを述べる．
\subsection*{Dropout}
Dropout\cite{Dropout}は，ネットワークのノードをランダムに消去しながら学習する手法である．訓練時に隠れ層のノードを毎回ランダムに選択し，そのノードの出力を0にする．そしてテスト時には全てのノードを活性化させ，信号を伝達させる．

Dropoutは，学習時にノードをランダムに消去することで，毎回異なるモデルを学習していると解釈でき，アンサンブル学習と同じ効果を擬似的に1つのネットワークで実現していると考えられる．アンサンブル学習とは，弱識別器を複数合わせて1つの強力な識別器とする手法で，現在でも有効な手法として用いられている．

\subsection*{Batch Normalization}
Batch Normalization\cite{BatchNorm}は，ネットワークにおける各層での活性化後の出力(アクティベーション)の分布を，適度な広がりを持つように調整する手法である．Batch Normalizationでは学習を行う際のミニバッチを単位として，ミニバッチごとに正規化を行う(\eq {BatchNorm})．
\begin{align}\label{eq:BatchNorm}
	\mu_B & = \dfrac{1}{m}\sum_{i=1}^m x_i \\
	\sigma_B^2 & = \dfrac{1}{m}\sum_{i=1}^m (x_i - \mu_B) \\
	\hat{x_i} & \leftarrow \dfrac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
\end{align}
ミニバッチとして$B = \{x_1, x_2, \cdots, x_m\}$という$m$個の入力データの集合に対して，平均$\mu_B$，分散$\sigma_B^2$を求め，入力データを平均0分散1となるように正規化を行う．$\epsilon$は0で除算されることを防ぐもので，極小の値を用いる．

Batch Normalizationを用いると，学習を速く進行させることができる，初期値にそれほど依存しない，過学習を抑制できるといった効果が期待できる．

\subsection*{Data Augumentation}
Data Augumentation(データ拡張)は，訓練データを人工的に水増しする手法である．例えば，入力画像に対して回転や縦横の移動，上下反転，鏡面反転などの操作をして訓練データを擬似的に増やす．これは，ニューラルネットは大きな位置ずれや対象物が回転した状態にあると別のものとして認識してしまうことを逆手にとった手法である．訓練データが少ないときに大きな効果を発揮する．


\section{画像認識におけるディープラーニング}
%教師あり学習であることもここで触れておきたい
Deep LearningとはDeep Neural Network(DNN)を指すことが多い．この"Deep"とは，ニューラルネットワークの層が深いことに由来している．

\fig {ImageNet}に画像認識タスクの精度の近年の推移を示す．これはImageNet Large Scale Visual Recognition Challenge (ILSVRC)と呼ばれる世界的な画像認識のコンペティションである(2010年から始まった)．カテゴリ数は1000クラスで，画像枚数は120万枚の訓練データと15万枚のテストデータが用意されている．
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{fig/ILSVRC}
	\caption{Transition of accuracy of image recognition on ILSVRC}
	\label{fig:ImageNet}
\end{figure}
2011年と2012年は約10\%もの大差でAlexNet\cite{AlexNet}が優勝している．これがディープラーニングの始まりである．AlexNetは5つの畳み込み層と3つの全結合層を持っている．2014年にはVGGNet\cite{VGGNet}やGoogLeNet\cite{GoogLeNet}が9割の精度を超えた．VGGNetはAlexNet(8層)よりさらに深い構造(19層)であり，GoogLeNetは22層もある．そして2015年にはResNet\cite{ResNet}が人間の精度をも超える認識精度を達成した．ResNetはGoogLeNetよりもさらに深く152層もある．CNNを複数回かけて検出を行う場合，CNNの浅い側では空間分解能はあるが抽象的な情報が少ない．深い側では意味論的な情報は取得できる（ポーズ，変形など）が空間分解能が小さいため幾何学的な情報が失われる．

アーキテクチャの進化の方向は大きく3つある．1つ目は層を深くすることである．2つ目はFC層の使用を避ける，またはInceptionモジュールの使用することである．これにより学習するパラメータ数を削減することができる．3つ目はResNetなどのショートカット接続の利用や，事前学習・転移学習を行うことである．これによって学習効率を向上させ，最終的にモデルの精度向上へと繋がる．ここで，事前学習のデータセットと適用データとの間には類似性があると良い．

画像処理におけるディープラーニングでは大きく3つのタスクがあり，それぞれ，クラス分類，物体検出，セグメンテーションである．以下に詳細を述べる．

\subsection*{クラス分類}
クラス分類は画像に写っている物体が「dog」「airplane」「bird toy」など事前に定義されたラベルのどれが適切かを識別するタスクである．この識別は，事前に定義されたラベルの特徴べクトルと入力画像の特徴べクトルの距離計算を行い，距離値が小さければ同一，そうでなければ否と判定する．指紋照合，顔照合，人物照合では，本人と他人を判定するタスクとなる．深層学習では，同一人物のペア画像間の距離値が小さく，他人の画像との距離値が大きくなるような損失関数を設計し，ネットワークを構築することで人物照合問題を解いている．

\subsection*{物体検出}
物体検出とはBounding Boxで物体の位置とその物体の種類を特定する方法である．歴史的には幾何的情報，手動特徴量，そしてそのカスケードを利用していた．その後，HOGやSIFTなど局所特徴量を抽出する方法を設計するようになったが，これは深い専門知識を必要とした．また広い範囲でオブジェクトを正確に検出する方法は，メモリ容量と処理時間に課題がある．現在はDeep Neural Networkになりデータのみから抽象的な特徴量を複数得ることができる．\fig {YOLO}に物体検出で有名はアルゴリズムであるSSDとYOLOのアーキテクチャを示す．クラス分けの場合は数1000のカテゴリを学習してTop Error Rateが2\%以下と人間よりも認識精度が高いが，物体検出においては，現状ではカテゴリが数100程度くらいまででも認識精度が人間よりも低くなってしまう．また物体検出は精度を上げるために処理に時間がかかることが多いため，リアルタイムに物体検出を行う時は，速度と精度のトレードオフが生じてしまう．

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{fig/yolo_ssd.png}
	\caption{Network Architecture of SSD and YOLO}
	\label{fig:YOLO}
\end{figure}

\subsection*{セグメンテーション}
セマンティックセグメンテーションとは，画像を画素レベルで認識することである．画像内の各画素をオブジェクトクラスに割り当てる手法である．セマンティックセグメンテーションの手法についてディープラーニング以前では，Texton Forestsや，Random Forestsに基づいた分類を行っていたが，物体検出と同様にCNNが登場してからは，高精度なセグメンテーションが実現するようになった．CNNを使ったセグメンテーションの手法で一般的に使われるようになったものがUnetである(\fig {Unet})．このUnetは文字通りUの形をしたネットワークであることが特徴で，２つのアーキテクチャーからできている．1つ目がエンコーダーのアーキテクチャーでCNNとプーリングで特徴を抽出しながら次元を削減していき，2つ目のデコーダーのアーキテクチャーで画像をセグメンテーションの結果になるように復元する．ここで問題になることが，プーリングをすることで位置情報を消してしまっているので，この位置情報を利用して画像を復元するためには，エンコーダーとデコーダーで画像サイズが同じところ同士をショートカットで接続することがUnet構造の優れている点である．

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{fig/unet.png}
	\caption{Artchitecture of Unet}
	\label{fig:Unet}
\end{figure}

\section{深層学習による3次元画像解析}
3次元画像には，動画または，ボリューム画像があるが，2次元画像とその深さ方向(動画であれば，時間方向)には異方性があることから，機械学習の方法に工夫が必要になる．今まで考案されている手法として，2DCNNを拡張した3DCNN，またCNNと時系列解析でよく用いられるLSTMを組み合わせた手法と，LSTM内部にCNNを組み込んだ手法が研究されている．


\subsection{3DCNN}
2次元画像が深さ方向に連続している3次元画像の特徴を抽出するために，2次元のCNNを拡張して，3次元のカーネルを使って畳み込みを行う，3DCNNを利用した手法がある．3DCNNによって，空間的な次元と時間方向の次元を同時に畳み込み計算で特徴を抽出することができる．\cite{ji20133d}
i番面の層でj番面の特徴マップにおける(x, y, z)の値は，以下の式で与えられる．

\begin{align}\label{eq:3dcnn}
v_{ij}^{xyz} = tanh(b_{ij} + \sum_{m} \sum_{p=0}^{P_i -1}　\sum_{q=0}^{Q_i -1}　\sum_{r=0}^{R_i -1} \omega_{ijm}^{pqr} v_{(i-1)m}^{(x+p)(y+q)(z+r)})
\end{align}
ただし，$R_i$はカーネルの深さ方向の次元であり，$\omega_{ijm}^{pqr}$前層のm番面の特徴マップの(p, q, r)におけるカーネルの値である．

3DCNNでは．学習パラメータが大きすぎるため学習を安定させるには，入力画像サイズを小さくするか深さ方向を少量に限定して画像を入力しなくてはいけないという問題点がある．

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{fig/3d_cnn.png}
	\caption{Artchitecture of 3DCNN}
	\label{fig:3DCNN}
\end{figure}



\subsection{LSTMと2DCNNの組み合わせ}
時系列解析に使われるLSTMを用いて3次元の画像を解析することができる．これはよく動画の解析で行われることがある．つまりフレームごとの画像の特徴を2次元のCNNで計算してから時系列情報をLSTMで解析することで，画像の時系列解析を行うことができる．これを3次元の医療画像でCTやMRIで適用する研究も行われている．3DCNNのデメリットであったパラメータの増大を2DCNNとLSTMの組み合わせで解決することができる．\cite{Donahue_2015_CVPR}

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{fig/chapter2/cnn_lstm}
	\caption{Action recognition with CNN-LSTM}
	\label{fig:cnnlstm}
\end{figure}

また，LSTMの研究も盛んに行われているため，その改良モデルが数多く存在する．特に，LSTMの学習効率を上げたGRU(Gated Linear Unit)\cite{cho2014learning}や，順方向だけでなく逆方向の時系列も計算に入れるBiLSTM\cite{wang2016image}が時系列解析の精度向上になっている報告がある．

\section{教師なし学習}
機械学習の手法には，上記で説明したように，ラベルの貼られているデータセットを用いて学習することを教師あり学習と呼び，その反対で，データセットはあっても，そのデータセットの特性を示したラベルが与えられていない場合のデータセットを用いて学習することを教師なし学習と呼ぶ


\subsection{Autoencoder}
教師なし学習で画像の特徴を抽出する方法としてオートエンコーダがある．\cite{hinton2006reducing}画像の場合におけるオートエンコーダの手法とは，ある画像から情報を圧縮する「エンコーダ」と言われる部分と，その圧縮した情報から画像を復元する「デコーダ」の二つからなる．入力とデコーダから復元された画像が同じ画像になるようにニューラルネットワークで学習させる．この学習の結果，潜在変数は似てる画像どうしで近い値になるように変化し，この分布を見れば教師ラベルがなくても画像の分類を行うことができる．
\begin{figure}
	\centering
	\includegraphics[width=0.3\linewidth]{fig/chapter2/auto_encoder}
	\caption{Auto Encoder Network}
	\label{fig:autoencoder}
\end{figure}



\subsection{Variational Autoencoder}
本研究では，このオートエンコーダの派生である．Variational Autoencoder(VAE)\cite{kingma2013auto}を利用した．これはオートエンコーダの「エンコーダ」と「デコーダ」は同じネットワーク構造であるが，データセットの潜在変数の分布が，正規分布になるような制約を加えて学習を行う手法である．こうすることでAutoencoderの潜在変数では分布の距離に意味ないが，VAEでは正規分布に埋め込まれるため，画像の類似度を分布が表現することができるところが特徴である\cite{abbasnejad2017infinite}．

\begin{align}\label{eq:vae}
p_\theta (z) & = N (z; 0, I)  \\
p_\theta (x|z) & = N (x; \upmu(z), \sigma(z)I) \\
q_\phi (z|x) & = N (x; \upmu(x), \sigma(x)I)
\end{align}


\subsection{敵対的生成ネットワーク}
敵対的生成ネットワーク(Generative Advarsarial Network: GAN)は2014年に提案された手法である．\fig {GAN}のようにGANではGeneratorとDiscriminatorの2つのネットワークがある．Generatorは訓練データと同じような画像を生成するネットワークでDiscriminatorは，入力されたデータが訓練データから来たものかGeneratorで生成されたものかを識別するように学習する．
VAEよりもGANの方が細部まで鮮明に画像を生成することができる．しかしGANは計算時間がかかるという問題や，DiscriminatorかGeneratorのどちらかか強くなってしまうなど，学習が安定しない問題があるため，これについての多くの研究報告がされている．

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{fig/generative_adversarial_nets.png}
	\caption{Diagram of GAN}
	\label{fig:GAN}
\end{figure}

\section{半教師あり学習}
弱教師あり学習とも呼ばれる．これは，教師あり学習と教師なし学習を組み合わせて学習する方法である．こうすることでデータに教師ラベルをつけているものが少数であっても，データの特徴を学習しながら少量のラベルで識別境界を決めることができる．
VAEをベースに半教師あり学習のモデルを作ることができ，これをM2モデルと呼ばれている．\cite{M2model}，半教師あり学習の，モデルの最適化手法は，推論のモデルと，生成のモデルに分けて行われる\cite{narayanaswamy2017learning}．

\begin{figure}[H]
	\centering
	\begin{minipage}{0.4\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/chapter2/discriminative_vae}
		\subcaption{Discriminative Model}
	\end{minipage}
	\begin{minipage}{0.4\columnwidth}
		\centering
		\includegraphics[clip, width=\linewidth]{fig/chapter2/generative_vae}
		\subcaption{Generative Model}
	\end{minipage}
	\caption{VAE M2 Model}
	\label{fig:vae_m2}
\end{figure}

推論モデルは\fig{vae_m2}の(a)で以下のように定義する．
\begin{align}\label{eq:m2_discriminative}
q_\phi (y | x) & = Categorical(y | \lambda_\phi (x)) \\
q_\phi (z | x, y) & = N(x | \upmu_\phi(x, y), log(\sigma_\phi^2 (x, y)))
\end{align}

生成モデルは以下のように定義する．
\begin{align}\label{eq:m2_generative}
p(z) & = N (z| 0, 1)  \\
p(y) & = \frac{1}{N_c} \\
p_\theta (x|z, y) & = f(x; z, y, \theta) \\
p_\theta (x, z, y) & = p_\theta (x|z, y) p(z) p(y)
\end{align}
ここで$N_c$はクラス数で，関数$f(x)$は尤度関数で$\theta$はニューラルネットワークのパラメータである．