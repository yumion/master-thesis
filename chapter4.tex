\chapter{試作2号機：インスタンス認識ハンド}
\newpage

\section{要求仕様}
1号機の課題の中で特に，自由度が少ない点と物体を識別できない点は日常生活で使用する上で必須である．そこで2号機ではこれら2点の課題を克服したロボットハンドを開発する．

1号機では対象に近づきグリップすることのみを行ったが，実応用を考えると物を持って何かをする必要が出てくる．そこで2号機では手首に重点を置き，より多彩な動きを実現するためアクチュエータを増やした．スマホでは環境を認識するのに1秒程かかってしまい，より高速に演算できる必要がある．また多くのアクチュエータを制御するため，スマホではなくPCで学習・推論を行うように変更した．

物体をピックアップするために物体との距離を捉える必要がある．

自由度

点で掴む，真ん中で掴む，掴んだ後に滑りを防ぐ必要

掴む時の把持点決定及び指定した点を掴むモータ調整


\section{機構設計・機体デザイン}

対象物との距離を測るためにDepthカメラを用いる．

対象物を持ち上げるために腕に関節を1つ加える．

対象物の把持点を調整するために，腕を進行方向と垂直な方向にスライドできるよう，サーボモータとラック\&ピニオンを用いて，回転を水平移動にした．
タイヤによる把持点制御では移動方向が回転方向になるため制御が難しい．


\section{制御アルゴリズム}
2号機では強化学習ではなく比例制御の考え方を利用した手法及びルールベースによって対象物のピックアップを行う．

接近動作では2段階に分けて行う．まず対象物までのラフな接近について述べる．
対象物のマスクを取得し重心を計算する．その重心がロボットハンドの中心軸に来るように左右輪のスピードをそれぞれ独立に以下の更新式によって制御しながら接近する．
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize]
    r_motor = (1 - error_distance) / 2 * MAX_SPEED
    l_motor = (1 + error_distance) / 2 * MAX_SPEED
\end{lstlisting}
ここで\texttt{MAX\_SPEED}はモーターの最大スピードで今回は100とした．\texttt{error\_distance}はロボットハンドと対象物との画像内におけるx軸方向のズレを画像の横の長さで規格した値であり，-1から1の値をとる．そして対象物から20cmまで近づいたら停止させる．

次に，対象物の重心にハンドの把持中心が来るように横方向にラックピニオンで修正を加える．この時，画像におけるピクセル間距離を実距離に変換し重心とハンド中心の変位をなるべく小さくするようにサーボを動かす．そしてデプスカメラが対象物に覆われて見えなくなるまで直進する．
そこからさらに4cm直進しグリッパーを閉じる．その後腕を上げ物体を持ち上げ，この際のデプス画像を保持しておく．そこから1秒ごとにデプス画像の現フレームとのピクセル数比を取り，50\%以下であったら失敗とし，5秒落とさず維持したら把持成功とした．



% \subsection{色抽出}

\subsection{デプスカメラを使用したカメラ画像内における実距離推定}




\subsection{Mask R-CNNを用いた物体のマスクと測距}
Instance Segmentationの手法の1つであるMask R-CNNを用いた．MSCOCOデータセットで学習した．

デプスカメラを用いて，検出した物体の距離も同時に測定した．


まず，対象物との距離に依る識別精度を検証した．
認識精度の対象物との距離依存性を検証した．


\section{評価}
今回2号機でアップデートしたのは，対象物の識別と把持動作であったため，この2点に関して評価を行う．


MSCOCOデータセットの中で，机の上にあるものを把持できるかを検証した．
今回，対象とした物体は以下の4点である．




\section{まとめ}
物体の種類を識別し，把持し，元の位置に持ち帰ることができた．



